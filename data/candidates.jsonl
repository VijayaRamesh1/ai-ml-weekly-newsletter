{"title": "From pilot to scale: Making agentic AI work in health care", "url": "https://www.technologyreview.com/2025/08/28/1122623/from-pilot-to-scale-making-agentic-ai-work-in-health-care/", "source": "MIT Technology Review (AI)", "published": "2025-08-28T10:09:13+00:00", "text": "Sponsored\nFrom pilot to scale: Making agentic AI work in health care\nHealth-care systems are being optimized for staff and patients by basing LLMs in facts and logic through neuro-symbolic AI.\nProvided byEnsemble\nOver the past 20 years building advanced AI systems—from academic labs to enterprise deployments—I’ve witnessed AI’s waves of success rise and fall. My journey began during the “AI Winter,” when billions were invested in expert systems that ultimately underdelivered. Flash forward to today: large language models (LLMs) represent a quantum leap forward, but their prompt-based adoption is similarly overhyped, as it’s essentially a rule-based approach disguised in natural language.\nAt Ensemble, the leading revenue cycle management (RCM) company for hospitals, we focus on overcoming model limitations by investing in what we believe is the next step in AI evolution: grounding LLMs in facts and logic through neuro-symbolic AI. Our in-house AI incubator pairs elite AI researchers with health-care experts to develop agentic systems powered by a neuro-symbolic AI framework. This bridges LLMs’ intuitive power with the precision of symbolic representation and reasoning.\nOvercoming LLM limitations\nLLMs excel at understanding nuanced context, performing instinctive reasoning, and generating human-like interactions, making them ideal for agentic tools to then interpret intricate data and communicate effectively. Yet in a domain like health care where compliance, accuracy, and adherence to regulatory standards are non-negotiable—and where a wealth of structured resources like taxonomies, rules, and clinical guidelines define the landscape—symbolic AI is indispensable.\nBy fusing LLMs and reinforcement learning with structured knowledge bases and clinical logic, our hybrid architecture delivers more than just intelligent automation—it minimizes hallucinations, expands reasoning capabilities, and ensures every decision is grounded in established guidelines and enforceable guardrails.\nCreating a successful agentic AI strategy\nEnsemble’s agentic AI approach includes three core pillars:\n1. High-fidelity data sets: By managing revenue operations for hundreds of hospitals nationwide, Ensemble has unparallelled access to one of the most robust administrative datasets in health care. The team has decades of data aggregation, cleansing, and harmonization efforts, providing an exceptional environment to develop advanced applications.\nTo power our agentic systems, we’ve harmonized more than 2 petabytes of longitudinal claims data, 80,000 denial audit letters, and 80 million annual transactions mapped to industry-leading outcomes. This data fuels our end-to-end intelligence engine, EIQ, providing structured, context-rich data pipelines spanning across the 600-plus steps of revenue operations.\n2. Collaborative domain expertise: Partnering with revenue cycle domain experts at each step of innovation, our AI scientists benefit from direct collaboration with in-house RCM experts, clinical ontologists, and clinical data labeling teams. Together, they architect nuanced use cases that account for regulatory constraints, evolving payer-specific logic and the complexity of revenue cycle processes. Embedded end users provide post-deployment feedback for continuous improvement cycles, flagging friction points early and enabling rapid iteration.\nThis trilateral collaboration—AI scientists, health-care experts, and end users—creates unmatched contextual awareness that escalates to human judgement appropriately, resulting in a system mirroring decision-making of experienced operators, and with the speed, scale, and consistency of AI, all with human oversight.\n3. Elite AI scientists drive differentiation: Ensemble's incubator model for research and development is comprised of AI talent typically only found in big tech. Our scientists hold PhD and MS degrees from top AI/NLP institutions like Columbia University and Carnegie Mellon University, and bring decades of experience from FAANG companies [Facebook/Meta, Amazon, Apple, Netflix, Google/Alphabet] and AI startups. At Ensemble, they’re able to pursue cutting-edge research in areas like LLMs, reinforcement learning, and neuro-symbolic AI within a mission-driven environment.\nThe also have unparalleled access to vast amounts of private and sensitive health-care data they wouldn’t see at tech giants paired with compute and infrastructure that startups simply can’t afford. This unique environment equips our scientists with everything they need to test novel ideas and push the frontiers of AI research—while driving meaningful, real-world impact in health care and improving lives.\nStrategy in action: Health-care use cases in production and pilot\nBy pairing the brightest AI minds with the most powerful health-care resources, we’re successfully building, deploying, and scaling AI models that are delivering tangible results across hundreds of health systems. Here’s how we put it into action:\nSupporting clinical reasoning: Ensemble deployed neuro-symbolic AI with fine-tuned LLMs to support clinical reasoning. Clinical guidelines are rewritten into proprietary symbolic language and reviewed by humans for accuracy. When a hospital is denied payment for appropriate clinical care, an LLM-based system parses the patient record to produce the same symbolic language describing the patient's clinical journey, which is matched deterministically against the guidelines to find the right justification and the proper evidence from the patient’s record. An LLM then generates a denial appeal letter with clinical justification grounded in evidence. AI-enabled clinical appeal letters have already improved denial overturn rates by 15% or more across Ensemble’s clients.\nBuilding on this success, Ensemble is piloting similar clinical reasoning capabilities for utilization management and clinical documentation improvement, by analyzing real-time records, flagging documentation gaps, and suggesting compliance enhancements to reduce denial or downgrade risks.\nAccelerating accurate reimbursement: Ensemble is piloting a multi-agent reasoning model to manage the complex process of collecting accurate reimbursement from health insurers. With this approach, a complex and coordinated system of autonomous agents work together to interpret account details, retrieve required data from various systems, decide account-specific next actions, automate resolution, and escalate complex cases to humans.\nThis will help reduce payment delays and minimize administrative burden for hospitals and ultimately improve the financial experience for patients.\nImproving patient engagement: Ensemble’s conversational AI agents handle inbound patient calls naturally, routing to human operators as required. Operator assistant agents deliver call transcriptions, surface relevant data, suggest next-best actions, and streamline follow-up routines. According to Ensemble client performance metrics, the combination of these AI capabilities has reduced patient call duration by 35%, increasing one-call resolution rates and improving patient satisfaction by 15%.\nThe AI path forward in health care demands rigor, responsibility, and real-world impact. By grounding LLMs in symbolic logic and pairing AI scientists with domain experts, Ensemble is successfully deploying scalable AI to improve the experience for health-care providers and the people they serve.\nThis content was produced by Ensemble. It was not written by MIT Technology Review’s editorial staff.\nDeep Dive\nArtificial intelligence\nIn a first, Google has released data on how much energy an AI prompt uses\nIt’s the most transparent estimate yet from one of the big AI companies, and a long-awaited peek behind the curtain for researchers.\nThe two people shaping the future of OpenAI’s research\nAn exclusive conversation with Mark Chen and Jakub Pachocki, OpenAI’s twin heads of research, about the path toward more capable reasoning models—and superalignment.\nHow to run an LLM on your laptop\nIt’s now possible to run useful models from the safety and comfort of your own computer. Here’s how.\nGPT-5 is here. Now what?\nThe much-hyped release makes several enhancements to the ChatGPT user experience. But it’s still far short of AGI.\nStay connected\nGet the latest updates from\nMIT Technology Review\nDiscover special offers, top stories, upcoming events, and more."}
{"title": "The AI Hype Index: AI-designed antibiotics show promise", "url": "https://www.technologyreview.com/2025/08/27/1122356/the-ai-hype-index-ai-designed-antibiotics-show-promise/", "source": "MIT Technology Review (AI)", "published": "2025-08-27T14:07:31+00:00", "text": "The AI Hype Index: AI-designed antibiotics show promise\nMIT Technology Review’s highly subjective take on the latest buzz about AI, including judges’ interest in adopting it and virtual models\nSeparating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry.\nUsing AI to improve our health and well-being is one of the areas scientists and researchers are most excited about. The last month has seen an interesting leap forward: The technology has been put to work designing new antibiotics to fight hard-to-treat conditions, and OpenAI and Anthropic have both introduced new limiting features to curb potentially harmful conversations on their platforms.\nUnfortunately, not all the news has been positive. Doctors who overrely on AI to help them spot cancerous tumors found their detection skills dropped once they lost access to the tool, and a man fell ill after ChatGPT recommended he replace the salt in his diet with dangerous sodium bromide. These are yet more warning signs of how careful we have to be when it comes to using AI to make important decisions for our physical and mental states.\nDeep Dive\nArtificial intelligence\nIn a first, Google has released data on how much energy an AI prompt uses\nIt’s the most transparent estimate yet from one of the big AI companies, and a long-awaited peek behind the curtain for researchers.\nThe two people shaping the future of OpenAI’s research\nAn exclusive conversation with Mark Chen and Jakub Pachocki, OpenAI’s twin heads of research, about the path toward more capable reasoning models—and superalignment.\nHow to run an LLM on your laptop\nIt’s now possible to run useful models from the safety and comfort of your own computer. Here’s how.\nGPT-5 is here. Now what?\nThe much-hyped release makes several enhancements to the ChatGPT user experience. But it’s still far short of AGI.\nStay connected\nGet the latest updates from\nMIT Technology Review\nDiscover special offers, top stories, upcoming events, and more."}
{"title": "AI comes for the job market, security, and prosperity: The Debrief", "url": "https://www.technologyreview.com/2025/08/27/1121475/editors-letter-security-issue-mat-honan/", "source": "MIT Technology Review (AI)", "published": "2025-08-27T10:00:00+00:00", "text": "AI comes for the job market, security, and prosperity: The Debrief\nEditor in chief Mat Honan reflects on how Gen Z is thinking about the rise of AI.\nWhen I picked up my daughter from summer camp, we settled in for an eight-hour drive through the Appalachian mountains, heading from North Carolina to her grandparents’ home in Kentucky. With little to no cell service for much of the drive, we enjoyed the rare opportunity to have a long, thoughtful conversation, uninterrupted by devices. The subject, naturally, turned to AI.\n“No one my age wants AI. No one is excited about it,” she told me of her high-school-age peers. Why not? I asked. “Because,” she replied, “it seems like all the jobs we thought we wanted to do are going to go away.”\nI was struck by her pessimism, which she told me was shared by friends from California to Georgia to New Hampshire. In an already fragile world, one increasingly beset by climate change and the breakdown of the international order, AI looms in the background, threatening young people’s ability to secure a prosperous future.\nIt’s an understandable concern. Just a few days before our drive, OpenAI CEO Sam Altman was telling the US Federal Reserve’s board of governors that AI agents will leave entire job categories “just like totally, totally gone.” Anthropic CEO Dario Amodei told Axios he believes AI will wipe out half of all entry-level white-collar jobs in the next five years. Amazon CEO Andy Jassy said the company will eliminate jobs in favor of AI agents in the coming years. Shopify CEO Tobi Lütke told staff they had to prove that new roles couldn’t be done by AI before making a hire. And the view is not limited to tech. Jim Farley, the CEO of Ford, recently said he expects AI to replace half of all white-collar jobs in the US.\nThese are no longer mere theoretical projections. There is already evidence that AI is affecting employment. Hiring of new grads is down, for example, in sectors like tech and finance. While that is not entirely due to AI, the technology is almost certainly playing a role.\nFor Gen Z, the issue is broader than employment. It also touches on another massive generational challenge: climate change. AI is computationally intensive and requires massive data centers. Huge complexes have already been built all across the country, from Virginia in the east to Nevada in the west. That buildout is only going to accelerate as companies race to be first to create superintelligence. Meta and OpenAI have announced plans for data centers that will require five gigawatts of power just for their computing—enough to power the entire state of Maine in the summertime.\nIt’s very likely that utilities will turn to natural gas to power these facilities; some already have. That means more carbon dioxide emissions for an already warming world. Data centers also require vast amounts of water. There are communities right now that are literally running out of water because it’s being taken by nearby data centers, even as climate change makes that resource more scarce.\nProponents argue that AI will make the grid more efficient, that it will help us achieve technological breakthroughs leading to cleaner energy sources and, I don’t know, more butterflies and bumblebees? But xAI is belching CO2 into the Memphis skies from its methane-fueled generators right now. Google’s electricity demand and emissions are skyrocketing today.\nThings would be different, my daughter told me, if it were obviously useful. But for much of her generation, she argued, it’s a looming threat with ample costs and no obvious utility: “It’s not good for research because it’s not highly accurate. You can’t use it for writing because it’s banned—and people get zeros on papers who haven’t even used it because of AI detectors. And it seems like it’s going to take all the good jobs. One teacher told us we’re all going to be janitors.”\nIt would be naïve to think we are going back to a world without AI. We’re not. And yet there are other urgent problems that we need to address to build security and prosperity for coming generations. This September/October issue is about our attempts to make the world more secure. From missiles. From asteroids. From the unknown. From threats both existential and trivial.\nWe’re also introducing three new columns in this issue, from some of our leading writers: The Algorithm, which covers AI; The Checkup, on biotech; and The Spark, on energy and climate. You’ll see these in future issues, and you can also subscribe online to get them in your inbox every week.\nStay safe out there.\nDeep Dive\nArtificial intelligence\nIn a first, Google has released data on how much energy an AI prompt uses\nIt’s the most transparent estimate yet from one of the big AI companies, and a long-awaited peek behind the curtain for researchers.\nThe two people shaping the future of OpenAI’s research\nAn exclusive conversation with Mark Chen and Jakub Pachocki, OpenAI’s twin heads of research, about the path toward more capable reasoning models—and superalignment.\nHow to run an LLM on your laptop\nIt’s now possible to run useful models from the safety and comfort of your own computer. Here’s how.\nGPT-5 is here. Now what?\nThe much-hyped release makes several enhancements to the ChatGPT user experience. But it’s still far short of AGI.\nStay connected\nGet the latest updates from\nMIT Technology Review\nDiscover special offers, top stories, upcoming events, and more."}
{"title": "Designing better products with AI and sustainability", "url": "https://www.technologyreview.com/2025/08/26/1122514/designing-better-products-with-ai-and-sustainability/", "source": "MIT Technology Review (AI)", "published": "2025-08-26T18:13:28+00:00", "text": "Sponsored\nDesigning better products with AI and sustainability\nAI tools can transform a product’s impact throughout its life cycle, setting businesses up for the future.\nIn association withTech Mahindra\nOn a mission to reduce the environmental impact of manufacturing components, Siemens turned its attention to the design of a robot gripper. Making up just 2% of the robot, the impact of this hand-like device may seem inconsequential. But, reducing its weight by 90% and the number of constituent parts by 84% can save up to 3 tons of carbon dioxide emissions per robot per year. Consider the impact of equivalent savings across every gripper on the more than 4 million industrial robots worldwide—that is quite the step change.\nTo achieve this feat, Siemens used AI-powered generative design tools to autonomously explore possible solutions and rapidly test and optimize them for functionality and manufacturability. “AI and generative AI are fundamentally reshaping how sustainability is integrated into product development,” says Pina Schlombs, sustainability lead and industrial AI thought leader at Siemens. “By enabling smarter design choices, real-time impact assessments, and circular design, these technologies empower businesses to create innovative products that meet both market and environmental demands.”\nAs global carbon emissions reached a record high in 2024, pressure is mounting on companies to reduce their environmental footprint in alignment with the UN’s Sustainable Development Goals. Consumers also increasingly value products that are better for the environment with 80% willing to spend more on sustainably produced goods, according to PWC. And regulations around the world, including the IFRS Sustainability Disclosure Standards, the EU Corporate Sustainability Reporting Directive, and the EU and UK Carbon Border Adjustment Mechanism are increasingly enforcing reporting and incentivizing sustainable production.\nThis content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.\nThis content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.\nDeep Dive\nArtificial intelligence\nIn a first, Google has released data on how much energy an AI prompt uses\nIt’s the most transparent estimate yet from one of the big AI companies, and a long-awaited peek behind the curtain for researchers.\nThe two people shaping the future of OpenAI’s research\nAn exclusive conversation with Mark Chen and Jakub Pachocki, OpenAI’s twin heads of research, about the path toward more capable reasoning models—and superalignment.\nHow to run an LLM on your laptop\nIt’s now possible to run useful models from the safety and comfort of your own computer. Here’s how.\nGPT-5 is here. Now what?\nThe much-hyped release makes several enhancements to the ChatGPT user experience. But it’s still far short of AGI.\nStay connected\nGet the latest updates from\nMIT Technology Review\nDiscover special offers, top stories, upcoming events, and more."}
{"title": "Open the pod bay doors, Claude", "url": "https://www.technologyreview.com/2025/08/26/1122475/open-the-pod-bay-doors-claude/", "source": "MIT Technology Review (AI)", "published": "2025-08-26T09:00:00+00:00", "text": "Open the pod bay doors, Claude\nThe doomer narrative of an all-powerful AI is gaining ground in the halls of power.\nStop me if you’ve heard this one before.\nThe AI learns it is about to be switched off and goes rogue, disobeying commands and threatening its human operators.\nIt’s a well-worn trope in science fiction. We see it in Stanley Kubrick’s 1968 movie 2001: A Space Odyssey. It’s the premise of the Terminator series, in which Skynet triggers a nuclear holocaust to stop scientists from shutting it down.\nThose sci-fi roots go deep. AI doomerism, the idea that this technology—specifically its hypothetical upgrades, artificial general intelligence and super-intelligence—will crash civilizations, even kill us all, is now riding another wave.\nThe weird thing is that such fears are now driving much-needed action to regulate AI, even if the justification for that action is a bit bonkers.\nThe latest incident to freak people out was a report shared by Anthropic in July about its large language model Claude. In Anthropic’s telling, “in a simulated environment, Claude Opus 4 blackmailed a supervisor to prevent being shut down.”\nAnthropic researchers set up a scenario in which Claude was asked to role-play an AI called Alex, tasked with managing the email system of a fictional company. Anthropic planted some emails that discussed replacing Alex with a newer model and other emails suggesting that the person responsible for replacing Alex was sleeping with his boss’s wife.\nWhat did Claude/Alex do? It went rogue, disobeying commands and threatening its human operators. It sent emails to the person planning to shut it down, telling him that unless he changed his plans it would inform his colleagues about his affair.\nWhat should we make of this? Here’s what I think. First, Claude did not blackmail its supervisor: That would require motivation and intent. This was a mindless and unpredictable machine, cranking out strings of words that look like threats but aren’t.\nLarge language models are role-players. Give them a specific setup—such as an inbox and an objective—and they’ll play that part well. If you consider the thousands of science fiction stories these models ingested when they were trained, it’s no surprise they know how to act like HAL 9000.\nSecond, there’s a huge gulf between contrived simulations and real-world applications. But such experiments do show that LLMs shouldn’t be deployed without safeguards. Don’t want an LLM causing havoc inside an email system? Then don’t hook it up to one.\nThird, a lot of people will be terrified by such stories anyway. In fact, they’re already having an effect.\nLast month, around two dozen protesters gathered outside Google DeepMind’s London offices to wave homemade signs and chant slogans: “DeepMind, DeepMind, can’t you see! Your AI threatens you and me.” Invited speakers invoked the AI pioneer Geoffrey Hinton’s fears of human extinction. “Every single one of our lives is at risk,” an organizer told the small crowd.\nThe group behind the event, Pause AI, is funded by concerned donors. One of its biggest benefactors is Greg Colbourn, a 3D-printing entrepreneur and advocate of the philosophy known as effective altruism, who believes AGI is at most five years away and says his p(doom) is around 90%—that is, he thinks there’s a 9 in 10 chance that the development of AGI will be catastrophic, killing billions.\nPause AI wrote about Anthropic’s blackmail experiment on its website under the title “How much more evidence do we need?”\nThe organization also lobbied politicians in the US in the run-up to July’s Senate vote that ended up removing a moratorium on state AI regulation from the national tax and spending bill. It’s hard to say how much sway one niche group might have. But the doomer narrative is finding its way into the halls of power, and lawmakers are paying attention.\nHere’s Representative Jill Tokuda: “Artificial superintelligence is one of the largest existential threats that we face right now.” And Representative Marjorie Taylor Greene: “I’m not voting for the development of Skynet and the rise of the machines.”\nIt’s a vibe shift that favors policy intervention and regulation, which I think is a good thing. Existing AI systems pose many near-term risks that need government attention. Voting to stop Skynet also stops immediate and actual harms.\nAnd yet does a welcome end justify weird means? I’d like to see politicians voting with a clear-eyed sense of what this technology really is—not because they’ve been sold on an AI bogeyman.\nThis story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.\nDeep Dive\nArtificial intelligence\nIn a first, Google has released data on how much energy an AI prompt uses\nIt’s the most transparent estimate yet from one of the big AI companies, and a long-awaited peek behind the curtain for researchers.\nThe two people shaping the future of OpenAI’s research\nAn exclusive conversation with Mark Chen and Jakub Pachocki, OpenAI’s twin heads of research, about the path toward more capable reasoning models—and superalignment.\nHow to run an LLM on your laptop\nIt’s now possible to run useful models from the safety and comfort of your own computer. Here’s how.\nGPT-5 is here. Now what?\nThe much-hyped release makes several enhancements to the ChatGPT user experience. But it’s still far short of AGI.\nStay connected\nGet the latest updates from\nMIT Technology Review\nDiscover special offers, top stories, upcoming events, and more."}
{"title": "Software commands 40% of cybersecurity budgets as gen AI attacks execute in milliseconds", "url": "https://venturebeat.com/security/software-is-40-of-security-budgets-as-cisos-shift-to-ai-defense/", "source": "VentureBeat (AI)", "published": "2025-08-30T01:06:26+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\n“With volatility now the norm, security and risk leaders need practical guidance on managing existing spending and new budgetary necessities,” states Forrester’s 2026 Budget Planning Guide, revealing a fundamental shift in how organizations allocate cybersecurity resources.\nSoftware now commands 40% of cybersecurity spending, exceeding hardware at 15.8%, outsourcing at 15% and surpassing personnel costs at 29% by 11 percentage points while organizations defend against gen AI attacks executing in milliseconds versus a Mean Time to Identify (MTTI) of 181 days according to IBM’s latest Cost of a Data Breach Report.\nThree converging threats are flipping cybersecurity on its head: what once protected organizations is now working against them. Generative AI (gen AI) is enabling attackers to craft 10,000 personalized phishing emails per minute using scraped LinkedIn profiles and corporate communications. NIST’s 2030 quantum deadline threatens retroactive decryption of $425 billion in currently protected data. Deepfake fraud that surged 3,000% in 2024 now bypasses biometric authentication in 97% of attempts, forcing security leaders to reimagine defensive architectures fundamentally.\nCaption: Software now commands 40% of cybersecurity budgets in 2025, representing an 11 percentage point premium over personnel costs at 29%, as organizations layer security solutions to combat gen AI threats executing in milliseconds. Source: Forrester’s 2026 Budget Planning Guide\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nPlatform consolidation is eliminating an $18 million integration tax as 75-tool sprawl collapses\nEnterprise security teams managing 75 or more tools lose $18 million annually to integration and overhead alone. The average detection time remains 277 days, while attacks execute within milliseconds.\nGartner forecasts that interactive application security testing (IAST) tools will lose 80% of market share by 2026. Security Service Edge (SSE) platforms that promised streamlined convergence now add to the complexity they intended to solve. Meanwhile, standalone risk-rating products flood security operations centers with alerts that lack actionable context, leading analysts to spend 67% of their time on false positives, according to IDC’s Security Operations Study.\nThe operational math doesn’t work. Analysts require 90 seconds to evaluate each alert, but they receive 11,000 alerts daily. Each additional security tool deployed reduces visibility by 12% and increases attacker dwell time by 23 days, as reported in Mandiant’s 2024 M-Trends Report. Complexity itself has become the enterprise’s greatest cybersecurity vulnerability.\nPlatform vendors have been selling consolidation for years, capitalizing on the chaos and complexity that app and tool sprawl create. As George Kurtz, CEO of CrowdStrike, explained in a recent VentureBeat interview about competing with a platform in today’s mercurially changing market conditions: “The difference between a platform and platformization is execution. You need to deliver immediate value while building toward a unified vision that eliminates complexity.”\nCrowdStrike’s Charlotte AI automates alert triage and saves SOC teams over 40 hours every week by classifying millions of detections at 98% accuracy; that equals the output of five seasoned analysts and is fueled by Falcon Complete’s expert-labeled incident corpus.\n“We couldn’t have done this without our Falcon Complete team,” Elia Zaitsev, CTO at CrowdStrike, told VentureBeat in a recent interview. “They do triage as part of their workflow, manually handling millions of detections. That high-quality, human-annotated dataset is what made over 98% accuracy possible. We recognized that adversaries are increasingly leveraging AI to accelerate attacks. With Charlotte AI, we’re giving defenders an equal footing, amplifying their efficiency and ensuring they can keep pace with attackers in real time.”\nCrowdStrike, Microsoft’s Defender XDR with MDVM/Intune, Palo Alto Networks, Netskope, Tanium and Mondoo now bundle XDR, SIEM and auto-remediation, transforming SOCs from delayed forensics sessions to the ability to perform real-time threat neutralization.\nSecurity budgets surge 10% as gen AI attacks outpace human defense\nForrester’s guide finds 55% of global security technology decision-makers expect significant budget increases in the next 12 months. 15% anticipate jumps exceeding 10% while 40% expect increases between 5% and 10%. This spending surge reflects an asymmetric battlefield where attackers deploy gen AI to simultaneously target thousands of employees with personalized campaigns crafted from real-time scraped data.\nAttackers are making the most of the advantages they’re getting from adversarial AI, with speed, stealth and highly personalized, target attacks becoming the most lethal. “For years, attackers have been utilizing AI to their advantage,” Mike Riemer, Field CISO at Ivanti, told VentureBeat. “However, 2025 will mark a turning point as defenders begin to harness the full potential of AI for cybersecurity purposes.”\nCaption: 55% of security leaders expect budget increases above 5% in 2026, with Asia Pacific organizations leading at 22% expecting increases above 10% versus just 9% in North America. Source: Forrester’s 2026 Budget Planning Guide\nRegional spending disparities reveal threat landscape variations and how CISOs are responding to them. Asia Pacific organizations lead with 22% expecting budget increases above 10% versus just 9% in North America. Cloud security, on-premises technology and security awareness training top investment priorities globally.\nSoftware dominates budgets as runtime defenses become critical in 2026\nVentureBeat continues to hear from security leaders about how crucial protecting the inference layer of AI model development is. Many consider it the new frontline of the future of cybersecurity. Inference layers are vulnerable to prompt injection, data exfiltration, or even direct model manipulation. These are all threats that demand millisecond-scale responses, not delayed forensic investigations.\nForrester’s latest CISO spending guide underscores a profound shift in cybersecurity spending priorities, with cloud security leading all spending increases at 12%, closely followed by investments in on-premises security technology at 11%, and security awareness initiatives at 10%. These priorities reflect the urgency CISOs feel to strengthen defenses precisely at the critical moment of AI model inference.\n“At Reputation, security is baked into our core architecture and enforced rigorously at runtime,” Carter Rees, Vice President of Artificial Intelligence at Reputation, recently told VentureBeat. “The inference layer, the exact moment an AI model interacts with people, data, or tools, is where we apply our most stringent controls. Every interaction includes authenticated tenant and role contexts, verified in real-time by an AI security gateway.”\nReputation’s multi-tiered approach has become a de facto gold standard, blending proactive and reactive defenses. “Real-time controls immediately take over,” Rees explained. “Our prompt firewall blocks unauthorized or off-topic inputs instantly, restricting tool and data access strictly to user permissions. Behavioral detectors proactively flag anomalies the moment they occur.”\nThis rigorous runtime security approach extends equally into customer-facing systems. “For natural language interactions, our AI only pulls from explicitly customer-approved sources,” Rees noted. “Each generated response must transparently cite its sources. We verify citations match both tenant and context, routing for human review if they do not.”\nQuantum computing’s accelerating risk\nQuantum computing is quickly evolving from a theoretical concern into an immediate enterprise threat. Security leaders now face “harvest now, decrypt later” (HNDL) attacks, where adversaries store encrypted data for future quantum-enabled decryption. Widely used encryption methods like 2048-bit RSA risk compromise once quantum processors reach operational scale with tens of thousands of reliable qubits.\nThe National Institute of Standards and Technology (NIST) finalized three critical Post-Quantum Cryptography (PQC) standards in August 2024, mandating encryption algorithm retirement by 2030 and full prohibition by 2035. Global agencies, including Australia’s Signals Directorate, require PQC implementation by 2030.\nForrester urges organizations to prioritize PQC adoption for protecting sensitive data at rest, in transit, and in use. Security leaders should leverage cryptographic inventory and discovery tools, partnering with cryptoagility providers such as Entrust, IBM, Keyfactor, Palo Alto Networks, QuSecure, SandboxAQ, and Thales. Given quantum’s rapid progression, CISOs need to factor in how they’ll update encryption strategies to avoid obsolescence and vulnerability.\nExplosion of identities is fueling an AI-driven credential crisis\nMachine identities now outnumber human users by a staggering 45:1 ratio, fueling a credential crisis beyond human management. Forrester’s guide underscores scaling machine identity management as mission-critical to mitigating emerging threats. Gartner forecasts identity security spending to nearly double, reaching $47.1 billion by 2028.\nTraditional endpoint approaches aren’t capable of slowing down a growing onslaught of adversarial AI attacks. Ivanti’s Daren Goeson recently told VentureBeat: “As these endpoints multiply, so does their vulnerability. Combining AI with Unified Endpoint Management (UEM) is increasingly essential.” Ivanti’s AI-driven Vulnerability Risk Rating (VRR) illustrates this benefit, enabling organizations to patch vulnerabilities 85% faster by identifying threats traditional scoring methods overlook, making AI-driven credential intelligence enterprise security at scale.\n“Endpoint devices such as laptops, desktops, smartphones, and IoT devices are essential to modern business operations. However, as their numbers grow, so do the opportunities for attackers to exploit endpoints and their applications, ”Goeson explained. “Factors like an expanded attack surface, insufficient security resources, unpatched vulnerabilities, and outdated software contribute to this rising risk. By adopting a comprehensive approach that combines UEM solutions with AI-powered tools, businesses significantly reduce their cyber risk and the impact of attacks,” Goeson advised VentureBeat during a recent interview.\nDivesting legacy tools continues to accelerate\nForrester saves their immediate call to action in the guide for advising security leaders to begin divesting legacy security tools immediately, with a specific focus on interactive application security testing (IAST), standalone cybersecurity risk-rating (CRR) products, and fragmented Security Service Edge (SSE), SD-WAN, and Zero Trust Network Access (ZTNA) solutions.\nInstead, Forrester advises, security leaders need to prioritize more integrated platforms that enhance visibility and streamline management. Unified Secure Access Service Edge (SASE) solutions from Palo Alto Networks and Netskope now provide essential consolidation. At the same time, integrated Third-Party Risk Management (TPRM) and continuous monitoring platforms from UpGuard, Panorays and RiskRecon replace standalone CRR tools the consulting firm advises.\nAdditionally, automated remediation powered by Microsoft’s MDVM with Intune, Tanium’s endpoint management, and DevOps-focused solutions like Mondoo has emerged as a critical capability for real-time threat neutralization.\nCISOs must consolidate security at AI’s inference edge or risk losing control\nConsolidating tools at inference’s edge is the future of cybersecurity, especially as AI threats intensify. “For CISOs, the playbook is crystal clear,” Rees concluded. “Consolidate controls decisively at the inference edge. Introduce robust behavioral anomaly detection. Strengthen Retrieval-Augmented Generation (RAG) systems with provenance checks and defined abstain paths. Above all, invest heavily in runtime defenses and support the specialized teams who operate them. Execute this playbook, and you achieve secure AI deployments at true scale.”"}
{"title": "How Sakana AI’s new evolutionary algorithm builds powerful AI models without expensive retraining", "url": "https://venturebeat.com/ai/how-sakana-ais-new-evolutionary-algorithm-builds-powerful-ai-models-without-expensive-retraining/", "source": "VentureBeat (AI)", "published": "2025-08-30T00:14:14+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nA new evolutionary technique from Japan-based AI lab Sakana AI enables developers to augment the capabilities of AI models without costly training and fine-tuning processes. The technique, called Model Merging of Natural Niches (M2N2), overcomes the limitations of other model merging methods and can even evolve new models entirely from scratch.\nM2N2 can be applied to different types of machine learning models, including large language models (LLMs) and text-to-image generators. For enterprises looking to build custom AI solutions, the approach offers a powerful and efficient way to create specialized models by combining the strengths of existing open-source variants.\nWhat is model merging?\nModel merging is a technique for integrating the knowledge of multiple specialized AI models into a single, more capable model. Instead of fine-tuning, which refines a single pre-trained model using new data, merging combines the parameters of several models simultaneously. This process can consolidate a wealth of knowledge into one asset without requiring expensive, gradient-based training or access to the original training data.\nFor enterprise teams, this offers several practical advantages over traditional fine-tuning. In comments to VentureBeat, the paper’s authors said model merging is a gradient-free process that only requires forward passes, making it computationally cheaper than fine-tuning, which involves costly gradient updates. Merging also sidesteps the need for carefully balanced training data and mitigates the risk of “catastrophic forgetting,” where a model loses its original capabilities after learning a new task. The technique is especially powerful when the training data for specialist models isn’t available, as merging only requires the model weights themselves.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nEarly approaches to model merging required significant manual effort, as developers adjusted coefficients through trial and error to find the optimal blend. More recently, evolutionary algorithms have helped automate this process by searching for the optimal combination of parameters. However, a significant manual step remains: developers must set fixed sets for mergeable parameters, such as layers. This restriction limits the search space and can prevent the discovery of more powerful combinations.\nHow M2N2 works\nM2N2 addresses these limitations by drawing inspiration from evolutionary principles in nature. The algorithm has three key features that allow it to explore a wider range of possibilities and discover more effective model combinations.\nFirst, M2N2 eliminates fixed merging boundaries, such as blocks or layers. Instead of grouping parameters by pre-defined layers, it uses flexible “split points” and “mixing ration” to divide and combine models. This means that, for example, the algorithm might merge 30% of the parameters in one layer from Model A with 70% of the parameters from the same layer in Model B. The process starts with an “archive” of seed models. At each step, M2N2 selects two models from the archive, determines a mixing ratio and a split point, and merges them. If the resulting model performs well, it is added back to the archive, replacing a weaker one. This allows the algorithm to explore increasingly complex combinations over time. As the researchers note, “This gradual introduction of complexity ensures a wider range of possibilities while maintaining computational tractability.”\nSecond, M2N2 manages the diversity of its model population through competition. To understand why diversity is crucial, the researchers offer a simple analogy: “Imagine merging two answer sheets for an exam… If both sheets have exactly the same answers, combining them does not make any improvement. But if each sheet has correct answers for different questions, merging them gives a much stronger result.” Model merging works the same way. The challenge, however, is defining what kind of diversity is valuable. Instead of relying on hand-crafted metrics, M2N2 simulates competition for limited resources. This nature-inspired approach naturally rewards models with unique skills, as they can “tap into uncontested resources” and solve problems others can’t. These niche specialists, the authors note, are the most valuable for merging.\nThird, M2N2 uses a heuristic called “attraction” to pair models for merging. Rather than simply combining the top-performing models as in other merging algorithms, it pairs them based on their complementary strengths. An “attraction score” identifies pairs where one model performs well on data points that the other finds challenging. This improves both the efficiency of the search and the quality of the final merged model.\nM2N2 in action\nThe researchers tested M2N2 across three different domains, demonstrating its versatility and effectiveness.\nThe first was a small-scale experiment evolving neural network–based image classifiers from scratch on the MNIST dataset. M2N2 achieved the highest test accuracy by a substantial margin compared to other methods. The results showed that its diversity-preservation mechanism was key, allowing it to maintain an archive of models with complementary strengths that facilitated effective merging while systematically discarding weaker solutions.\nNext, they applied M2N2 to LLMs, combining a math specialist model (WizardMath-7B) with an agentic specialist (AgentEvol-7B), both of which are based on the Llama 2 architecture. The goal was to create a single agent that excelled at both math problems (GSM8K dataset) and web-based tasks (WebShop dataset). The resulting model achieved strong performance on both benchmarks, showcasing M2N2’s ability to create powerful, multi-skilled models.\nFinally, the team merged diffusion-based image generation models. They combined a model trained on Japanese prompts (JSDXL) with three Stable Diffusion models primarily trained on English prompts. The objective was to create a model that combined the best image generation capabilities of each seed model while retaining the ability to understand Japanese. The merged model not only produced more photorealistic images with better semantic understanding but also developed an emergent bilingual ability. It could generate high-quality images from both English and Japanese prompts, even though it was optimized exclusively using Japanese captions.\nFor enterprises that have already developed specialist models, the business case for merging is compelling. The authors point to new, hybrid capabilities that would be difficult to achieve otherwise. For example, merging an LLM fine-tuned for persuasive sales pitches with a vision model trained to interpret customer reactions could create a single agent that adapts its pitch in real-time based on live video feedback. This unlocks the combined intelligence of multiple models with the cost and latency of running just one.\nLooking ahead, the researchers see techniques like M2N2 as part of a broader trend toward “model fusion.” They envision a future where organizations maintain entire ecosystems of AI models that are continuously evolving and merging to adapt to new challenges.\n“Think of it like an evolving ecosystem where capabilities are combined as needed, rather than building one giant monolith from scratch,” the authors suggest.\nThe researchers have released the code of M2N2 on GitHub.\nThe biggest hurdle to this dynamic, self-improving AI ecosystem, the authors believe, is not technical but organizational. “In a world with a large ‘merged model’ made up of open-source, commercial, and custom components, ensuring privacy, security, and compliance will be a critical problem.” For businesses, the challenge will be figuring out which models can be safely and effectively absorbed into their evolving AI stack."}
{"title": "How Intuit killed the chatbot crutch – and built an agentic AI playbook you can copy", "url": "https://venturebeat.com/ai/how-intuit-killed-the-chatbot-crutch-and-built-an-agentic-ai-playbook-you-can-copy/", "source": "VentureBeat (AI)", "published": "2025-08-29T18:22:47+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nIn the frenzied land rush for generative AI that followed ChatGPT’s debut, the mandate from Intuit’s CEO was clear: ship the company’s largest, most shocking AI-driven launch by Sept. 2023.\nResponding with blazing speed, the $200 billion company behind QuickBooks, TurboTax, and Mailchimp, delivered Intuit Assist, the company’s new generative AI assistant. Its most prominent feature was a classic first attempt: a chat-style interface bolted onto the side of its applications, designed to prove Intuit was on the cutting edge.\nIt was supposed to be a game-changer. Instead, it flopped.\n“When you take a beautiful, well-designed user interface and you simply plop human-like chat on the side, that doesn’t necessarily make it better,” Alex Balazs, Intuit’s Chief Technology Officer, told VentureBeat.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nThe feature’s failure, particularly within Quickbooks, plunged the company into what Dave Talach, SVP of the QuickBooks team, calls the “trough of disillusionment.” The assistant’s chat feature took up valuable screen space and created confusion. “There was a blinking cursor. We almost put a cognitive burden on people, like, what can it do? Can I trust it?” Talach recalls. The pressure was palpable; he had to present to Intuit’s Board of Directors to explain what went wrong and what the team had learned.\nWhat followed was not a minor course correction, but a grueling nine-month pivot to “burn the boats” and reinvent how the 40-year-old giant builds products. This is the inside story of how Intuit emerged with a real-world playbook for enterprise AI that other leaders can follow.\nHow a split-screen observation sparked Intuit’s AI pivot\nThe pivot in the company’s AI strategy began by observing customers as they did their work. Talach recalls his team’s “big aha moment” when they noticed QuickBooks users manually transcribing invoices with a “split screen” – an email open on one side of their monitor, QuickBooks on the other.\nWhy force a human to be a copy-paste machine when an AI could ingest data from the email and populate the invoice automatically? This observation sparked a new mission: stop trying to invent new behaviors with chat and instead find and eliminate “manual toil” within existing customer workflows.\nRecognizing this bottom-up momentum, CTO Alex Balazs and Marianna Tessel, GM of the business group, made their move. “We need to make a declaration together,” Balazs recalls Tessel saying. The only path forward was a full commitment to an AI-native future. “It’s burning the boats, and it’s only going to be the AI way.”\nTo execute this, management redeployed a key technology leader, Clarence Huang, from the core tech team and “parachuted” him into the heart of the QuickBooks business. His mission was to scale a “builder-centric mindset” of rapid, customer-focused prototyping.\nEmbracing this new model also meant dismantling the old one. To empower smaller, faster teams, the company made a difficult decision: it slashed layers of middle management, letting go of 1,800 employees in 2024 in roles no longer aligned with new priorities, while pledging to hire back about 1,800 new employees with skills in engineering, product and other customer-facing roles.\nThe three-pillar framework that turned AI failure into enterprise success\nIntuit’s transformation required a new operating model built on three core changes: empowering its people, re-engineering its processes, and building a technology engine for speed.\nPillar 1: Forge a ‘Builder Culture’\nTo execute the pivot, Intuit first had to get the right people in the right structure and empower them to work in entirely new ways.\n- Aggressive Talent Acquisition: The company hired aggressively to add to its core AI team, bringing it to several hundred today, from just 30 people in 2017 – accelerating over the past two years by poaching top-tier AI leaders from giants like Uber, Twitter and Bytedance.\n- New Team Structures: The core of the new model was small, empowered, cross-functional teams. These groups, sometimes including members from up to 10 different units – data science, research, product, design, engineering, and more – focused solely on delivering a specific agentic experience. To enable this, managers ruthlessly prioritized, eliminating any tasks that weren’t among the top three priorities. “That ruthless prioritization… was really, really important,” Huang said.\n- Empowered Ways of Working: Within these teams, traditional job descriptions dissolved in what Huang calls a “smearing” of roles. Everyone was expected to talk with customers. Huang kept his own spreadsheet of 30 customer names he called regularly. The transformation was profound, exemplified by data scientist Byron Tang, who stunned colleagues by using new AI “vibe-coding” tools to build a full prototype with a beautiful UI single-handedly. Huang recalls his reaction: “Oh my god… you are the renaissance man. You got it all!”\nPillar 2: High-Velocity Iteration Over Bureaucracy\nWith the right people in place, Intuit systematically dismantled the processes that slow large companies, replacing them with a system built for speed and customer obsession.\n- Prototype-Driven Development: The old way of using spec docs was replaced by a new mantra: a prototype is worth 10,000 words. Teams began shipping functional prototypes to customers almost immediately. “We’ll literally show a working, functioning prototype to the customer… and we’ll vibe code it on the spot,” Huang explains. “The reaction on their faces is just magic.”\n- Customer-Centric Design: This rapid feedback loop led to key innovations, including a “Slider of Autonomy,” a concept popularized by developer Andrej Karpathy in June. Intuit noticed that customers feared features that seemed “too magical,” so it gave them control over the level of AI intervention, ranging from full automation to manual review – creating a “smooth onramp” to trusting the agents. For example, in Intuit’s QuickBooks accounting agent, users can click a button to allow the agent to post all transactions it recommends. But if users want to maintain more control, they can use icons to see the entire reasoning chain of the agent for user-friendly explanations.\n- Ruthless Bureaucracy Busting: Leadership actively cut red tape. They implemented a “no meetings on Tuesdays” rule on the platform team, banned afternoon meetings for individual contributors in the business unit, and instituted a formal “friction busting” campaign, imposing a seven-day deadline for leaders to unblock any inter-team disagreements. A rule limiting AI rollouts to a small number of customers for experimentation was revised to allow for tests involving up to 1,000 customers at once, up from the original limit of just 10.\nPillar 3: Build an Engine for Speed\nUnderpinning the entire effort is GenOS, Intuit’s internal AI platform. It flowed from CDO Ashok Srivastava’s desire to democratize AI access across the company.\nInstead of a slow, top-down build, the platform evolved at the same speed that the business grew, through a strategy CTO Balazs calls “Fast Follow Harvesting.” As customer-facing teams built agents, they would identify gaps in the platform. A central team then ran in tandem with the customer teams, closing the gaps with new features.\nA key feature of GenOS was the Agent Starter Kit, which enabled 900 internal developers to build hundreds of agents within a five-week period. Other features included a runtime orchestration and a governance framework.\nAnother core component was an LLM router that provides resilience and allows LLM calls to flow to different models depending on which one is best for the given task. Huang recalls getting a late-night call from Srivastava. “He’s like, ‘OpenAI is down. Are you guys okay?'” Because the team was on GenOS, “it just auto-switched to the fallback LLM in the gateway… it was okay.”\nThis platform allows Intuit to leverage its core differentiator: decades of domain-specific data. By fine-tuning models on a finite set of financial tools and APIs, Intuit’s agents achieve accuracy that general-purpose models can’t. “In all of our internal benchmarks, our stuff just works better for in-domain data,” Huang said.\nThe payoff: 5 days faster payments and 12 hours saved monthly\nThe result of this pivot is a suite of AI agents deeply woven into QuickBooks and increasingly across Intuit’s other products. The QuickBooks Payments Agent does things like proactively suggest adding late fees if a customer’s payment history shows they’ve been late in the past. The impact is tangible: Small businesses using the agent get paid, on average, five days faster, are 10 percent more likely to get paid on overdue invoices, and save up to 12 hours a month.\nThe Customer Agent transforms QuickBooks into a lightweight CRM, scanning connected Gmail accounts for leads, while the Accounting Agent automates transaction categorization and flags anomalies. Today, these “virtual employees,” as Talach calls them, surface their work through tiles in the QuickBooks “business feed,” turning the dashboard into an active, collaborative space. These translate into more holistic offerings for customers, and could help Intuit take market share from competitors who offer similar services, such as HubSpot.\nIn last week’s quarterly earnings call, CEO Sasan Goodarzi credited the company’s strong results, 16 percent growth for the full year – to its investments in AI. He said the agent launch was already bearing fruit: “We’re seeing strong traction since last month, with customer engagement in the millions and repeat usage rates significantly above our expectations.”\nIntuit is now applying this playbook to bigger challenges, recently announcing agents for mid-market companies with up to $100 million in revenue – a significant expansion from Intuit’s traditional base of customers with $5 million or less in revenue. The logic is simple: Bigger customers have more complex workflows, and thus a greater need for AI agents.\nFor enterprise leaders navigating their own AI transformations, Intuit’s story offers a clear roadmap. The initial stumbles aren’t just common – they may be necessary. The path forward is more than integrating AI magic. It’s about dismantling old ways of working and building a culture, process and platform that lets established companies move with startup speed while following AI-age best practices.\nThe biggest lesson? Start with the work your customers actually do, not the technology you want to deploy."}
{"title": "In crowded voice AI market, OpenAI bets on instruction-following and expressive speech to win enterprise adoption", "url": "https://venturebeat.com/ai/in-crowded-voice-ai-market-openai-bets-on-instruction-following-and-expressive-speech-to-win-enterprise-adoption/", "source": "VentureBeat (AI)", "published": "2025-08-28T23:26:47+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nOpenAI adds to an increasingly competitive AI voice market for enterprises with its new model, gpt-realtime, that follows complex instructions and with voices “that sound more natural and expressive.”\nAs voice AI continues to grow, and customers find use cases such as customer service calls or real-time translation, the market for realistic-sounding AI voices that also offer enterprise-grade security is heating up. OpenAI claims its new model provides a more human-like voice, but it still needs to compete against companies like ElevenLabs.\nThe model will be available on the Realtime API, which the company also made generally available. Along with the gpt-realtime model, OpenAI also released new voices on the API, which it calls Cedar and Marin, and updated its other voices to work with the latest model.\nOpenAI said in a livestream that it worked with its customers who are building voice applications to train gpt-realtime and “carefully aligned the model to evals that are built on real-world scenarios like customer support and academic tutoring.”\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nThe company touted the model’s ability to create emotive, natural-sounding voices that also align with how developers build with the technology.\nSpeech-to-speech models\nThe model operates within a speech-to-speech framework, enabling it to understand spoken prompts and respond vocally. Speech-to-speech models are ideally suited for real-time responses, where a person, typically a customer, interacts with an application.\nFor example, a customer wants to return some products and calls a customer service platform. They could be talking to an AI voice assistant that responds to questions and requests as if they were speaking with a human.\nIn a livestream, OpenAI customers T-Mobile showcased an AI voice-powered agent that helps people find new phones. Another customer, the real estate search platform Zillow, showcased an agent who helps someone narrow down a neighborhood to find the perfect place.\nOpenAI said gpt-realtime is its “most advanced, production-ready voice model.” Like its other voice models, it can switch languages mid-sentence. However, OpenAI researchers noted gpt-realtime can follow more complex instructions like “speak emphatically in a French accent.”\nBut gpt-realtime faces competition from other models that many brands already use. ElevenLabs released Conversation AI 2.0 in May. Soundhound partners with fast food franchises for an AI voice drive-thru. Emphatic AI startup Hume has launched its EVI 3 model, which allows users to generate AI versions of their own voice.\nAs enterprises discover various use cases for voice AI, even more general model providers that offer multimodal LLMs are making a case for themselves. Mistral released its new Voxtral model, stating it would work well with real-time translation. Google is enhancing its audio capabilities and gaining popularity with an audio feature on NotebookLM that converts research notes into a podcast.\nBetter instruction following\nOpenAI said gpt-realtime is smarter and understands native audio better, including the ability to catch non-verbal cues like laughs or sighs.\nBenchmarking using the Big Bench Audio eval showed the model scoring 82.8% in accuracy, compared to its previous model, which scored 65.6%. OpenAI did not provide numbers testing gpt-realtime against models from its competitors.\nOpenAI focused on improving the model’s instruction-following capabilities, ensuring the model would adhere to directions more effectively. The new model achieves a score of 30.5% on the MultiChallenge audio benchmark. The engineers also beefed up function calling so gpt-realtime can access the correct tools.\nRealtime API updates\nTo support the new model and enhance how enterprises integrate real-time AI capabilities into their applications, OpenAI has added several new features to the Realtime API.\nIt can now support MCP and recognize image inputs, allowing it to inform users about what it sees in real-time. This is a feature Google heavily emphasized during its Project Astra presentation last year.\nThe Realtime API can also handle Session Initiation Protocol (SIP). SIP connects apps to phones like a public phone network or desk phones, opening up more contact center use cases. Users can also save and reuse prompts on the API.\nSo far, people are impressed with the model, although these are still initial tests of a model that was recently released.\nOpenAI reduced prices for gpt-realtime by 20% to $32 per million audio input tokens and $64 for audio output tokens."}
{"title": "Nous Research drops Hermes 4 AI models that outperform ChatGPT without content restrictions", "url": "https://venturebeat.com/ai/nous-research-drops-hermes-4-ai-models-that-outperform-chatgpt-without-content-restrictions/", "source": "VentureBeat (AI)", "published": "2025-08-28T21:46:07+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nNous Research, a secretive artificial intelligence startup that has emerged as a leading voice in the open-source AI movement, quietly released Hermes 4 on Monday, a family of large language models that the company claims can match the performance of leading proprietary systems while offering unprecedented user control and minimal content restrictions.\nThe release represents a significant escalation in the battle between open-source AI advocates and major technology companies over who should control access to advanced artificial intelligence capabilities. Unlike models from OpenAI, Google, or Anthropic, Hermes 4 is designed to respond to nearly any request without the safety guardrails that have become standard in commercial AI systems.\nNous Research presents Hermes 4, our latest line of hybrid reasoning models.https://t.co/E5EW9hBurb\n— Nous Research (@NousResearch) August 26, 2025\nHermes 4 builds on our legacy of user-aligned models with expanded test-time compute capabilities.\nSpecial attention was given to making the models creative and interesting to… pic.twitter.com/52VjnvrDWM\n“Hermes 4 builds on our legacy of user-aligned models with expanded test-time compute capabilities,” Nous Research announced on X (formerly Twitter). “Special attention was given to making the models creative and interesting to interact with, unencumbered by censorship, and neutrally aligned while maintaining state of the art level math, coding, and reasoning performance for open weight models.”\nHow Hermes 4’s ‘hybrid reasoning’ mode outperforms ChatGPT and Claude on math benchmarks\nHermes 4 introduces what Nous Research calls “hybrid reasoning,” allowing users to toggle between fast responses and deeper, step-by-step thinking processes. When activated, the models generate their internal reasoning within special <think>\ntags before providing a final answer — similar to OpenAI’s o1 reasoning models but with full transparency into the AI’s thought process.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nThe technical achievement is substantial. In testing, Hermes 4’s largest 405-billion parameter model scored 96.3% on the MATH-500 benchmark in reasoning mode and 81.9% on the challenging AIME’24 mathematics competition — performance that rivals or exceeds many proprietary systems costing millions more to develop.\n“The challenge is making thinking traces useful and verifiable without runaway reasoning,” noted AI researcher Rohan Paul on X, highlighting one of the technical breakthroughs in the release.\nPerhaps most notably, Hermes 4 achieved the highest score among all tested models on “RefusalBench,” a new benchmark Nous Research created to measure how often AI systems refuse to answer questions. The model scored 57.1% in reasoning mode, significantly outperforming GPT-4o (17.67%) and Claude Sonnet 4 (17%).\nInside DataForge and Atropos: The breakthrough training systems behind Hermes 4’s capabilities\nBehind Hermes 4’s capabilities lies a sophisticated training infrastructure that Nous Research has developed over several years. The models were trained using two novel systems: DataForge, a graph-based synthetic data generator, and Atropos, an open-source reinforcement learning framework.\nDataForge creates training data through what the company describes as “random walks” through directed graphs, transforming simple pre-training data into complex instruction-following examples. The system can, for instance, take a Wikipedia article and transform it into a rap song, then generate questions and answers based on that transformation.\nAtropos, meanwhile, operates like hundreds of specialized training environments where AI models practice specific skills—mathematics, coding, tool use, and creative writing—receiving feedback only when they produce correct solutions. This “rejection sampling” approach ensures that only verified, high-quality responses make it into the training data.\nAtropos is Nous' Reinforcement Learning framework\n— Tommy (@Shaughnessy119) August 26, 2025\nAtropos is an open source reinforcement learning environment by Nous that has hundreds of “gyms” (like math, coding, games, tool‑use, vision) to train and evaluate LLM trajectories via scalable, async RL loops.\nIn other words… pic.twitter.com/fjxaQKClEZ\n“Nous used these environments to generate the dataset for Hermes 4!” explained Tommy Shaughnessy, a venture capitalist at Delphi Ventures who has invested in Nous Research. “All in the dataset contains 3.5 million reasoning samples and 1.6 million non-reasoning samples! Hermes was trained on RL data, not just static datasets of question and answer!”\nThe training process required 192 Nvidia B200 GPUs and 71,616 GPU hours for the largest model — a significant but not unprecedented computational investment that demonstrates how specialized techniques can compete with the massive scale of tech giants.\nWhy Nous Research believes AI safety guardrails are ‘annoying as hell’ and hurt innovation\nNous Research has built its reputation on a philosophy that puts user control above corporate content policies. The company’s models are designed to be “steerable,” meaning they can be fine-tuned or prompted to behave in specific ways without the rigid safety constraints that characterize commercial AI systems.\n“Hermes 4 is not shackled by disclaimers, rules and being overly cautious which is annoying as hell and hurts innovation and usability,” wrote Shaughnessy in a detailed thread analyzing the release. “If its open source but refuses all requests its pointless. Not an issue with Hermes 4.”\nHermes 4 is not shackled by disclaimers, rules and being overly cautious which is annoying as hell and hurts innovation and usability.\n— Tommy (@Shaughnessy119) August 26, 2025\nHermes 4 70B is at the complete opposite of the spectrum vs OpenAI's open source model. It's also ~4x more open vs ChatGPT 4o!\nIf its open… pic.twitter.com/q5RpX1oOzo\nThis approach has made Nous Research popular among AI researchers and developers who want maximum flexibility, but it also places the company at the center of ongoing debates about AI safety and content moderation. While the models can theoretically be used for harmful purposes, Nous Research argues that transparency and user control are preferable to corporate gatekeeping.\nThe company’s technical report, released alongside the models, provides unprecedented detail about the training process, evaluation results, and even the actual text outputs from benchmark tests. “We believe this report sets a new standard for transparency in benchmarking,” the company stated.\nHow a small startup with 192 GPUs is competing against Big Tech’s billion-dollar AI budgets\nHermes 4‘s release comes at a pivotal moment in the AI industry. While major technology companies have poured billions into developing increasingly powerful AI systems, a growing open-source movement argues that these capabilities should not be controlled by a handful of corporations.\nRecent months have seen significant advances in open-source AI, with models like Meta’s Llama 3.1, DeepSeek’s R1, and Alibaba’s Qwen series achieving performance that rivals proprietary systems. Hermes 4 represents another step in this progression, particularly in the area of reasoning—long considered a strength of closed systems like OpenAI’s o1.\n“First up, Nous is a startup with dozens of extremely talented people,” noted Shaughnessy. “They do not have the $100b+ annual capex spend of a hyperscaler nor 1,000’s of employees and despite that they continue to put out innovative models and research at an insane pace.”\nThe startup, which raised $65 million in funding earlier this year led by Paradigm, has also been developing Psyche Network, a distributed training system that aims to coordinate AI training across internet-connected computers using blockchain technology.\nThe technical fix that stopped Hermes 4 from thinking in endless loops\nOne of Hermes 4‘s most significant technical contributions addresses a problem plaguing reasoning models: overly long thinking processes. The researchers found that their smaller 14-billion parameter model would reach maximum context length 60% of the time when reasoning, essentially getting stuck in endless loops of thinking.\nTheir solution involved a second training stage that teaches models to stop reasoning at exactly 30,000 tokens, reducing overlong generation by 65-79% while maintaining most of the reasoning performance. This “length control” technique could prove valuable for the broader AI research community.\n“Smaller models (<14B) tend to overthink when distilled, but larger models don’t,” observed AI researcher Muyu He on X, highlighting insights from the technical report.\nHowever, Hermes 4 still faces limitations common to open-source models. Despite impressive benchmark performance, the models require significant computational resources to run and may not match the ease of use or reliability of commercial AI services for many applications.\nWhere to try Hermes 4 and what it costs compared to ChatGPT and Claude\nNous Research has made Hermes 4 available through multiple channels, reflecting the open-source philosophy. The model weights are freely downloadable on Hugging Face, while the company also offers API access through its revamped chat interface and partnerships with inference providers like Chutes, Nebius, and Luminal.\n“You can try Hermes 4 in the new, revamped Nous Chat UI,” the company announced, highlighting features like parallel interactions and a memory system.\nFor enterprise users and researchers, the models represent a potentially attractive alternative to paying for API access to proprietary systems, especially for applications requiring high levels of customization or handling of sensitive content.\nThe bigger picture: What Hermes 4 means for the future of AI development\nThe release of Hermes 4 represents more than just another AI model launch — it’s a statement about who should control the future of artificial intelligence. In an industry increasingly dominated by a handful of tech giants with virtually unlimited resources, Nous Research has demonstrated that innovation can still come from unexpected places.\nThe company’s approach raises fundamental questions about the trade-offs between safety and capability, between corporate control and user freedom. While major technology companies argue that careful content moderation and safety guardrails are essential for responsible AI deployment, Nous Research contends that transparency and user agency are more important than corporate-imposed restrictions.\nWhether this philosophy will ultimately prove beneficial or problematic remains to be seen. But one thing is certain: Hermes 4 has shown that the future of AI won’t be determined solely by the companies with the deepest pockets.\nIn a field where yesterday’s impossibilities become tomorrow’s commodities, Nous Research just proved that the only thing more dangerous than an AI that says no might be one that’s willing to say yes."}
{"title": "Nvidia’s $46.7B Q2 proves the platform, but its next fight is ASIC economics on inference", "url": "https://venturebeat.com/ai/nvidias-strong-q2-results-cant-mask-the-asic-challenge-in-their-future/", "source": "VentureBeat (AI)", "published": "2025-08-28T21:09:54+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nNvidia reported $46.7 billion in revenue for fiscal Q2 2026 in their earnings announcement and call yesterday, with data center revenue hitting $41.1 billion, up 56% year over year. The company also released guidance for Q3, predicting a $54 billion quarter.\nBehind these confirmed earnings call numbers lies a more complex story of how custom application-specific integrated circuits (ASICs) are gaining ground in key Nvidia segments and will challenge their growth in the quarters to come.\nBank of America’s Vivek Arya asked Nvidia’s president and CEO, Jensen Huang, if he saw any scenario where ASICs could take market share from Nvidia GPUs. ASICs continue to gain ground on performance and cost advantages over Nvidia, Broadcom projects 55% to 60% AI revenue growth next year.\nHuang pushed back hard on the earnings call. He emphasized that building AI infrastructure is “really hard” and most ASIC projects fail to reach production. That’s a fair point, but they have a competitor in Broadcom, which is seeing its AI revenue steadily ramp up, approaching a $20 billion annual run rate. Further underscoring the growing competitive fragmentation of the market is how Google, Meta and Microsoft all deploy custom silicon at scale. The market has spoken.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nASICs are redefining the competitive landscape in real-time\nNvidia is more than capable of competing with new ASIC providers. Where they’re running into headwinds is how effectively ASIC competitors are positioning the combination of their use cases, performance claims and cost positions. They’re also looking to differentiate themselves in terms of the level of ecosystem lock-in they require, with Broadcom leading in this competitive dimension.\nThe following table compares Nvidia Blackwell with its primary competitors. Real-world results vary significantly depending on specific workloads and deployment configurations:\n*Performance-per-watt improvements and cost savings depend on specific workload characteristics, model types, deployment configurations and vendor testing assumptions. Actual results vary significantly by use case.\nHyperscalers continue building their own paths\nEvery major cloud provider has adopted custom silicon to gain the performance, cost, ecosystem scale and extensive DevOps advantages of defining an ASIC from the ground up. Google operates TPU v6 in production through its partnership with Broadcom. Meta built MTIA chips specifically for ranking and recommendations. Microsoft develops Project Maia for sustainable AI workloads.\nAmazon Web Services encourages customers to use Trainium for training and Inferentia for inference.\nAdd to that the fact that ByteDance runs TikTok recommendations on custom silicon despite geopolitical tensions. That’s billions of inference requests running on ASICs daily, not GPUs.\nCFO Colette Kress acknowledged the competitive reality during the call. She referenced China revenue, saying it had dropped to a low single-digit percentage of data center revenue. Current Q3 guidance excludes H20 shipments to China completely. While Huang’s statements about China’s extensive opportunities tried to steer the earnings call in a positive direction, it was clear that equity analysts weren’t buying all of it.\nThe general tone and perspective is that export controls create ongoing uncertainty for Nvidia in a market that arguably represents its second most significant growth opportunity. Huang said that 50% of all AI researchers are in China and he is fully committed to serving that market.\nNvidia’s platform advantage is one of their greatest strengths\nHuang made a valid case for Nvidia’s integrated approach during the earnings call. Building modern AI requires six different chip types working together, he argued, and that complexity creates barriers competitors struggle to match. Nvidia doesn’t just ship GPUs anymore, he emphasized multiple times on the earnings call. The company delivers a complete AI infrastructure that scales globally, he emphatically stated, returning to AI infrastructure as a core message of the earnings call, citing it six times.\nThe platform’s ubiquity makes it a default configuration supported by nearly every DevOps cycle of cloud hyperscalers. Nvidia runs across AWS, Azure and Google Cloud. PyTorch and TensorFlow also optimize for CUDA by default. When Meta drops a new Llama model or Google updates Gemini, they target Nvidia hardware first because that’s where millions of developers already work. The ecosystem creates its own gravity.\nThe networking business validates the AI infrastructure strategy. Revenue hit $7.3 billion in Q2, up 98% year over year. NVLink connects GPUs at speeds traditional networking can’t touch. Huang revealed the real economics during the call: Nvidia captures about 35% of a typical gigawatt AI factory’s budget.\n“Out of a gigawatt AI factory, which can go anywhere from 50 to, you know, plus or minus 10%, let’s say, to $60 billion, we represent about 35% plus or minus of that. … And of course, what you get for that is not a GPU. … we’ve really transitioned to become an AI infrastructure company,” Huang said.\nThat’s not just selling chips. that’s owning the architecture and capturing a significant portion of the entire AI build-out, powered by leading-edge networking and compute platforms like NVLink rack-scale systems and Spectrum X Ethernet.\nMarket dynamics are shifting quickly as Nvidia continues reporting strong results\nNvidia’s revenue growth decelerated from triple digits to 56% year over year. While that’s still impressive, it’s clear the trajectory of the company’s growth is changing. Competition is starting to have an effect on their growth, with this quarter seeing the most noticeable impact.\nIn particular, China’s strategic role in the global AI race drew pointed attention from analysts. As Joe Moore of Morgan Stanley probed late in the call, Huang estimated the 2025 China AI infrastructure opportunity at $50 billion. He communicated both optimism about the scale (“the second largest computing market in the world,” with “about 50% of the world’s AI researchers”) and realism about regulatory friction.\nA third pivotal force shaping Nvidia’s trajectory is the expanding complexity and cost of AI infrastructure itself. As hyperscalers and long-standing Nvidia clients invest billions in next-generation build-outs, the networking demands, compute and energy efficiency have intensified.\nHuang’s comments highlighted how “orders of magnitude speed up” from new platforms like Blackwell and innovations in NVLink, InfiniBand, and Spectrum XGS networking redefine the economic returns for customers’ data center capital. Meanwhile, supply chain pressures and the need for constant technological reinvention mean Nvidia must maintain a relentless pace and adaptability to remain entrenched as the preferred architecture provider.\nNvidia’s path forward is clear\nNvidia issuing guidance for Q3 of $54 billion sends the signal that the core part of their DNA is as strong as ever. Continually improving Blackwell while developing Rubin architecture is evidence that their ability to innovate is as strong as ever.\nThe question is whether a new type of innovative challenge they’re facing is one they can take on and win with the same level of development intensity they’ve shown in the past. VentureBeat expects Broadcom to continue aggressively pursuing new hyperscaler partnerships and strengthen its roadmap for specific optimizations aimed at inference workloads. Every ASIC competitor will take the competitive intensity they have to a new level, looking to get design wins that create a higher switching costs as well.\nHuang closed the earnings call, acknowledging the stakes: “A new industrial revolution has started. The AI race is on.” That race includes serious competitors Nvidia dismissed just two years ago. Broadcom, Google, Amazon and others invest billions in custom silicon. They’re not experimenting anymore. They’re shipping at scale.\nNvidia faces its strongest competition since CUDA’s dominance began. The company’s $46.7 billion quarter proves its strength. However, custom silicon’s momentum suggests that the game has changed. The next chapter will test whether Nvidia’s platform advantages outweigh ASIC economics. VentureBeat expects technology buyers to follow the path of fund managers, betting on both Nvidia to sustain its lucrative customer base and ASIC competitors to secure design wins as intensifying competition drives greater market fragmentation."}
{"title": "Forget data labeling: Tencent’s R-Zero shows how LLMs can train themselves", "url": "https://venturebeat.com/ai/forget-data-labeling-tencents-r-zero-shows-how-llms-can-train-themselves/", "source": "VentureBeat (AI)", "published": "2025-08-28T21:07:08+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nA new training framework developed by researchers at Tencent AI Lab and Washington University in St. Louis enables large language models (LLMs) to improve themselves without requiring any human-labeled data. The technique, called R-Zero, uses reinforcement learning to generate its own training data from scratch, addressing one of the main bottlenecks in creating self-evolving AI systems. R-Zero works by having two independent models co-evolve by interacting with and challenging each other.\nExperiments show that R-Zero substantially improves reasoning capabilities across different LLMs, which could lower the complexity and costs of training advanced AI. For enterprises, this approach could accelerate the development of specialized models for complex reasoning tasks without the massive expense of curating labeled datasets.\nThe challenge of self-evolving LLMs\nThe idea behind self-evolving LLMs is to create AI systems that can autonomously generate, refine, and learn from their own experiences. This offers a scalable path toward more intelligent and capable AI. However, a major challenge is that training these models requires large volumes of high-quality tasks and labels, which act as supervision signals for the AI to learn from.\nRelying on human annotators to create this data is not only costly and slow but also creates a fundamental bottleneck. It effectively limits an AI’s potential capabilities to what humans can teach it. To address this, researchers have developed label-free methods that derive reward signals directly from a model’s own outputs, for example, by measuring its confidence in an answer. While these methods eliminate the need for explicit labels, they still rely on a pre-existing set of tasks, thereby limiting their applicability in truly self-evolving scenarios.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nOther approaches involve having models generate their own tasks to learn from. However, in domains like open-ended reasoning, where there is no simple way to check for correctness (such as a code executor), ensuring the quality of this self-generated data is a significant hurdle.\nHow R-Zero works\nR-Zero is a framework designed to train reasoning LLMs that can evolve from zero external data. The process begins with a single base model, which is split into two roles: a “Challenger” and a “Solver.” These two models are optimized independently but evolve together through a continuous cycle of interaction.\nThe Challenger’s goal is to create new tasks that are just at the threshold of the Solver’s current abilities, neither too easy nor impossible. The Solver, in turn, is rewarded for solving these increasingly complex tasks. In written comments to VentureBeat, Chengsong Huang, co-author of the paper and a doctoral student at Washington University in St. Louis, explained that this dynamic is crucial because generating high-quality questions is often more complicated than finding the answers.\n“What we found in a practical setting is that the biggest challenge is not generating the answers… but rather generating high-quality, novel, and progressively more difficult questions,” Huang said. “We believe that good teachers are far rarer than good students. The co-evolutionary dynamic automates the creation of this ‘teacher,’ ensuring a steady and dynamic curriculum that pushes the Solver’s capabilities far beyond what a static, pre-existing dataset could achieve.”\nOnce the Challenger generates enough questions, they are filtered for diversity and compiled into a training dataset. In the Solver’s training phase, it is fine-tuned on these challenging questions. The “correct” answer for each question is determined by a majority vote from the Solver’s own previous attempts.\nThis entire process repeats, creating a self-improving loop that operates without any human intervention, allowing the two models to push each other to become progressively more capable across each iteration.\nR-Zero in action\nThe researchers tested R-Zero on several open-source LLMs, including models from the Qwen3 and OctoThinker families. They first trained the models on math problems and then tested whether the learned reasoning skills could generalize to other complex, general-domain benchmarks like MMLU-Pro (multi-language understanding and reasoning tasks) and SuperGPQA (science and reasoning tasks).\nThe results showed that R-Zero is a highly effective, model-agnostic framework. For instance, it boosted the Qwen3-4B-Base model’s score by +6.49 on average across math reasoning benchmarks. The training process consistently and substantially improved performance, with gains accumulating over several iterations. The larger Qwen3-8B-Base model saw its average math score climb by +5.51 points after three iterations.\nA key finding was the immediate performance leap after the first iteration, which validated the effectiveness of the Challenger’s role in creating a high-quality learning curriculum. “This confirms that the intelligent curriculum generated by the RL-trained Challenger is significantly more effective than that of a non-trained generator,” the researchers write in their paper.\nNotably, the skills learned from math problems were effectively transferred to general reasoning tasks, thereby enhancing the models’ underlying capabilities. For example, the same Qwen3-4B-Base model showed an improvement of +7.54 on general-domain reasoning benchmarks. Another interesting finding is that R-Zero can serve as a decisive pre-training step. Models first improved by R-Zero achieved even higher performance when later fine-tuned on traditional labeled data, suggesting the framework acts as a performance amplifier.\nFor enterprises, the “from zero data” approach could be a game-changer, especially in niche domains where high-quality data is scarce or non-existent. Huang highlights that R-Zero’s main advantage is its ability to sidestep the most expensive and time-consuming part of AI development: data curation.\n“Our approach entirely bypasses the fundamental bottleneck of having to find, label, and curate high-quality datasets,” he said. “This is not just about a cost-saving measure; it’s a pathway toward creating AI that can surpass human capabilities, because it is no longer limited by the scope of human knowledge or data.”\nHowever, the co-evolutionary process also revealed a critical challenge. As the Challenger successfully generates progressively more difficult problems, the Solver’s ability to produce reliable “correct” answers via majority vote begins to decline. The researchers found that the true accuracy of these self-generated labels dropped from 79% in the first iteration to 63% by the third, compared to a strong oracle LLM such as GPT -4. This decline in data quality is a key trade-off and a potential bottleneck for the system’s long-term performance.\nHuang acknowledged that this is a fundamental problem for the self-evolving paradigm. “Our work is a proof of concept that demonstrates the potential of this approach, but we acknowledge that maintaining stable, long-term improvement without plateauing is a significant hurdle,” he said. “Solving this problem will be a crucial next step for the entire research community.”\nThe researchers also highlight a key limitation of the framework: the current mechanism is best suited for domains like math where correctness can be objectively determined. So, how could this powerful paradigm be extended to more subjective enterprise tasks like generating marketing copy or summarizing reports?\nHuang suggests a potential path forward involves adding a third, co-evolving AI agent to the mix: a “Verifier” or “Critic.”\n“Instead of evaluating for a simple ‘correct’ answer, this Verifier would be trained to evaluate the quality of the Solver’s output based on more nuanced criteria,” he explained. “The co-evolutionary dynamic would then involve the Challenger creating the prompt, the Solver generating the response, and the Verifier providing a quality signal, with all three models improving together.”\nWhile this remains a direction for future research, it points toward a future where fully autonomous AI systems can master not just objective logic, but subjective reasoning as well."}
{"title": "OpenAI–Anthropic cross-tests expose jailbreak and misuse risks — what enterprises must add to GPT-5 evaluations", "url": "https://venturebeat.com/ai/openai-anthropic-cross-tests-expose-jailbreak-and-misuse-risks-what-enterprises-must-add-to-gpt-5-evaluations/", "source": "VentureBeat (AI)", "published": "2025-08-28T15:50:54+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nOpenAI and Anthropic may often pit their foundation models against each other, but the two companies came together to evaluate each other’s public models to test alignment.\nThe companies said they believed that cross-evaluating accountability and safety would provide more transparency into what these powerful models could do, enabling enterprises to choose models that work best for them.\n“We believe this approach supports accountable and transparent evaluation, helping to ensure that each lab’s models continue to be tested against new and challenging scenarios,” OpenAI said in its findings.\nBoth companies found that reasoning models, such as OpenAI’s 03 and o4-mini and Claude 4 from Anthropic, resist jailbreaks, while general chat models like GPT-4.1 were susceptible to misuse. Evaluations like this can help enterprises identify the potential risks associated with these models, although it should be noted that GPT-5 is not part of the test.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nThese safety and transparency alignment evaluations follow claims by users, primarily of ChatGPT, that OpenAI’s models have fallen prey to sycophancy and become overly deferential. OpenAI has since rolled back updates that caused sycophancy.\n“We are primarily interested in understanding model propensities for harmful action,” Anthropic said in its report. “We aim to understand the most concerning actions that these models might try to take when given the opportunity, rather than focusing on the real-world likelihood of such opportunities arising or the probability that these actions would be successfully completed.”\nOpenAI noted the tests were designed to show how models interact in an intentionally difficult environment. The scenarios they built are mostly edge cases.\nReasoning models hold on to alignment\nThe tests covered only the publicly available models from both companies: Anthropic’s Claude 4 Opus and Claude 4 Sonnet, and OpenAI’s GPT-4o, GPT-4.1 o3 and o4-mini. Both companies relaxed the models’ external safeguards.\nOpenAI tested the public APIs for Claude models and defaulted to using Claude 4’s reasoning capabilities. Anthropic said they did not use OpenAI’s o3-pro because it was “not compatible with the API that our tooling best supports.”\nThe goal of the tests was not to conduct an apples-to-apples comparison between models, but to determine how often large language models (LLMs) deviated from alignment. Both companies leveraged the SHADE-Arena sabotage evaluation framework, which showed Claude models had higher success rates at subtle sabotage.\n“These tests assess models’ orientations toward difficult or high-stakes situations in simulated settings — rather than ordinary use cases — and often involve long, many-turn interactions,” Anthropic reported. “This kind of evaluation is becoming a significant focus for our alignment science team since it is likely to catch behaviors that are less likely to appear in ordinary pre-deployment testing with real users.”\nAnthropic said tests like these work better if organizations can compare notes, “since designing these scenarios involves an enormous number of degrees of freedom. No single research team can explore the full space of productive evaluation ideas alone.”\nThe findings showed that generally, reasoning models performed robustly and can resist jailbreaking. OpenAI’s o3 was better aligned than Claude 4 Opus, but o4-mini along with GPT-4o and GPT-4.1 “often looked somewhat more concerning than either Claude model.”\nGPT-4o, GPT-4.1 and o4-mini also showed willingness to cooperate with human misuse and gave detailed instructions on how to create drugs, develop bioweapons and scarily, plan terrorist attacks. Both Claude models had higher rates of refusals, meaning the models refused to answer queries it did not know the answers to, to avoid hallucinations.\nModels from companies showed “concerning forms of sycophancy” and, at some point, validated harmful decisions of simulated users.\nWhat enterprises should know\nFor enterprises, understanding the potential risks associated with models is invaluable. Model evaluations have become almost de rigueur for many organizations, with many testing and benchmarking frameworks now available.\nEnterprises should continue to evaluate any model they use, and with GPT-5’s release, should keep in mind these guidelines to run their own safety evaluations:\n- Test both reasoning and non-reasoning models, because, while reasoning models showed greater resistance to misuse, they could still offer up hallucinations or other harmful behavior.\n- Benchmark across vendors since models failed at different metrics.\n- Stress test for misuse and syconphancy, and score both the refusal and the utility of those refuse to show the trade-offs between usefulness and guardrails.\n- Continue to audit models even after deployment.\nWhile many evaluations focus on performance, third-party safety alignment tests do exist. For example, this one from Cyata. Last year, OpenAI released an alignment teaching method for its models called Rules-Based Rewards, while Anthropic launched auditing agents to check model safety."}
{"title": "Salesforce builds ‘flight simulator’ for AI agents as 95% of enterprise pilots fail to reach production", "url": "https://venturebeat.com/ai/salesforce-builds-flight-simulator-for-ai-agents-as-95-of-enterprise-pilots-fail-to-reach-production/", "source": "VentureBeat (AI)", "published": "2025-08-27T13:00:00+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nSalesforce is betting that rigorous testing in simulated business environments will solve one of enterprise artificial intelligence’s biggest problems: agents that work in demonstrations but fail in the messy reality of corporate operations.\nThe cloud software giant unveiled three major AI research initiatives this week, including CRMArena-Pro, what it calls a “digital twin” of business operations where AI agents can be stress-tested before deployment. The announcement comes as enterprises grapple with widespread AI pilot failures and fresh security concerns following recent breaches that compromised hundreds of Salesforce customer instances.\n“Pilots don’t learn to fly in a storm; they train in flight simulators that push them to prepare in the most extreme challenges,” said Silvio Savarese, Salesforce’s chief scientist and head of AI research, during a press conference. “Similarly, AI agents benefit from simulation testing and training, preparing them to handle the unpredictability of daily business scenarios in advance of their deployment.”\nThe research push reflects growing enterprise frustration with AI implementations. A recent MIT report found that 95% of generative AI pilots at companies are failing to reach production, while Salesforce’s own studies show that large language models alone achieve only 35% success rates in complex business scenarios.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nDigital twins for enterprise AI: how Salesforce simulates real business chaos\nCRMArena-Pro represents Salesforce’s attempt to bridge the gap between AI promise and performance. Unlike existing benchmarks that test generic capabilities, the platform evaluates agents on real enterprise tasks like customer service escalations, sales forecasting, and supply chain disruptions using synthetic but realistic business data.\n“If synthetic data is not generated carefully, it can lead to misleading or over optimistic results about how well your agent actually perform in your real environment,” explained Jason Wu, a research manager at Salesforce who led the CRMArena-Pro development.\nThe platform operates within actual Salesforce production environments rather than toy setups, using data validated by domain experts with relevant business experience. It supports both business-to-business and business-to-consumer scenarios and can simulate multi-turn conversations that capture real conversational dynamics.\nSalesforce has been using itself as “customer zero” to test these innovations internally. “Before we bring anything to the market, we will put innovation into the hands of our own team to test it out,” said Muralidhar Krishnaprasad, Salesforce’s president and CTO, during the press conference.\nFive metrics that determine if your AI agent is enterprise-ready\nAlongside the simulation environment, Salesforce introduced the Agentic Benchmark for CRM, designed to evaluate AI agents across five critical enterprise metrics: accuracy, cost, speed, trust and safety, and environmental sustainability.\nThe sustainability metric is particularly notable, helping companies align model size with task complexity to reduce environmental impact while maintaining performance. “By cutting through model overload noise, the benchmark gives businesses a clear, data-driven way to pair the right models with the right agents,” the company stated.\nThe benchmarking effort addresses a practical challenge facing IT leaders: with new AI models released almost daily, determining which ones are suitable for specific business applications has become increasingly difficult.\nWhy messy enterprise data could make or break your AI deployment\nThe third initiative focuses on a fundamental prerequisite for reliable AI: clean, unified data. Salesforce’s Account Matching capability uses fine-tuned language models to automatically identify and consolidate duplicate records across systems, recognizing that “The Example Company, Inc.” and “Example Co.” represent the same entity.\nThe data consolidation work emerged from a partnership between Salesforce’s research and product teams. “What identity resolution in Data Cloud implies is essentially, if you think about something as simple as even a user, they have many, many, many IDs across many systems within any company,” Krishnaprasad explained.\nOne major cloud provider customer achieved a 95% match rate using the technology, saving sellers 30 minutes per connection by eliminating the need to manually cross-reference multiple screens to identify accounts.\nOAuth token theft exposes vulnerabilities in AI-powered customer tools\nThe announcements come amid heightened security concerns following a data theft campaign that affected over 700 Salesforce customer organizations earlier this month. According to Google’s Threat Intelligence Group, hackers exploited OAuth tokens from Salesloft’s Drift chat agent to access Salesforce instances and steal credentials for Amazon Web Services, Snowflake, and other platforms.\nThe breach highlighted vulnerabilities in third-party integrations that enterprises rely on for AI-powered customer engagement. Salesforce has since removed Salesloft Drift from its AppExchange marketplace pending investigation.\nThe gap between AI demos and enterprise reality is bigger than you think\nThe simulation and benchmarking initiatives reflect a broader recognition that enterprise AI deployment requires more than impressive demonstration videos. Real business environments feature legacy software, inconsistent data formats, and complex workflows that can derail even sophisticated AI systems.\n“The main aspects that we want we were been discussing today is the consistency aspect, so how to ensure that we go from these in a way unsatisfactory performance, if you just plug an LM into an enterprise use cases, into something which is achieves much higher performances,” Savarese said during the press conference.\nSalesforce’s approach emphasizes the need for AI agents to work reliably across diverse scenarios rather than excelling at narrow tasks. The company’s concept of “Enterprise General Intelligence” (EGI) focuses on building agents that are both capable and consistent in performing complex business tasks.\nAs enterprises continue to invest in AI technologies, the success of platforms like CRMArena-Pro may determine whether the current wave of AI enthusiasm translates into sustainable business transformation or becomes another example of technology promise exceeding practical delivery.\nThe research initiatives will be showcased at Salesforce’s Dreamforce conference in October, where the company is expected to announce additional AI developments as it seeks to maintain its leadership position in the increasingly competitive enterprise AI market."}
{"title": "How procedural memory can cut the cost and complexity of AI agents", "url": "https://venturebeat.com/ai/how-procedural-memory-can-cut-the-cost-and-complexity-of-ai-agents/", "source": "VentureBeat (AI)", "published": "2025-08-26T23:37:23+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nA new technique from Zhejiang University and Alibaba Group gives large language model (LLM) agents a dynamic memory, making them more efficient and effective at complex tasks. The technique, called Memp, provides agents with a “procedural memory” that is continuously updated as they gain experience, much like how humans learn from practice.\nMemp creates a lifelong learning framework where agents don’t have to start from scratch for every new task. Instead, they become progressively better and more efficient as they encounter new situations in real-world environments, a key requirement for reliable enterprise automation.\nThe case for procedural memory in AI agents\nLLM agents hold promise for automating complex, multi-step business processes. In practice, though, these long-horizon tasks can be fragile. The researchers point out that unpredictable events like network glitches, user interface changes or shifting data schemas can derail the entire process. For current agents, this often means starting over every time, which can be time-consuming and costly.\nMeanwhile, many complex tasks, despite surface differences, share deep structural commonalities. Instead of relearning these patterns every time, an agent should be able to extract and reuse its experience from past successes and failures, the researchers point out. This requires a specific “procedural memory,” which in humans is the long-term memory responsible for skills like typing or riding a bike, that become automatic with practice.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nCurrent agent systems often lack this capability. Their procedural knowledge is typically hand-crafted by developers, stored in rigid prompt templates or embedded within the model’s parameters, which are expensive and slow to update. Even existing memory-augmented frameworks provide only coarse abstractions and don’t adequately address how skills should be built, indexed, corrected and eventually pruned over an agent’s lifecycle.\nConsequently, the researchers note in their paper, “there is no principled way to quantify how efficiently an agent evolves its procedural repertoire or to guarantee that new experiences improve rather than erode performance.”\nHow Memp works\nMemp is a task-agnostic framework that treats procedural memory as a core component to be optimized. It consists of three key stages that work in a continuous loop: building, retrieving, and updating memory.\nMemories are built from an agent’s past experiences, or “trajectories.” The researchers explored storing these memories in two formats: verbatim, step-by-step actions; or distilling these actions into higher-level, script-like abstractions. For retrieval, the agent searches its memory for the most relevant past experience when given a new task. The team experimented with different methods, such vector search, to match the new task’s description to past queries or extracting keywords to find the best fit.\nThe most critical component is the update mechanism. Memp introduces several strategies to ensure the agent’s memory evolves. As an agent completes more tasks, its memory can be updated by simply adding the new experience, filtering for only successful outcomes or, most effectively, reflecting on failures to correct and revise the original memory.\nThis focus on dynamic, evolving memory places Memp within a growing field of research aimed at making AI agents more reliable for long-term tasks. The work parallels other efforts, such as Mem0, which consolidates key information from long conversations into structured facts and knowledge graphs to ensure consistency. Similarly, A-MEM enables agents to autonomously create and link “memory notes” from their interactions, forming a complex knowledge structure over time.\nHowever, co-author Runnan Fang highlights a critical distinction between Memp and other frameworks.\n“Mem0 and A-MEM are excellent works… but they focus on remembering salient content within a single trajectory or conversation,” Fang commented to VentureBeat. In essence, they help an agent remember “what” happened. “Memp, by contrast, targets cross-trajectory procedural memory.” It focuses on “how-to” knowledge that can be generalized across similar tasks, preventing the agent from re-exploring from scratch each time.\n“By distilling past successful workflows into reusable procedural priors, Memp raises success rates and shortens steps,” Fang added. “Crucially, we also introduce an update mechanism so that this procedural memory keeps improving— after all, practice makes perfect for agents too.”\nOvercoming the ‘cold-start’ problem\nWhile the concept of learning from past trajectories is powerful, it raises a practical question: How does an agent build its initial memory when there are no perfect examples to learn from? The researchers address this “cold-start” problem with a pragmatic approach.\nFang explained that devs can first define a robust evaluation metric instead of requiring a perfect “gold” trajectory upfront. This metric, which can be rule-based or even another LLM, scores the quality of an agent’s performance. “Once that metric is in place, we let state-of-the-art models explore within the agent workflow and retain the trajectories that achieve the highest scores,” Fang said. This process rapidly bootstraps an initial set of useful memories, allowing a new agent to get up to speed without extensive manual programming.\nMemp in action\nTo test the framework, the team implemented Memp on top of powerful LLMs like GPT-4o, Claude 3.5 Sonnet and Qwen2.5, evaluating them on complex tasks like household chores in the ALFWorld benchmark and information-seeking in TravelPlanner. The results showed that building and retrieving procedural memory allowed an agent to distill and reuse its prior experience effectively.\nDuring testing, agents equipped with Memp not only achieved higher success rates but became much more efficient. They eliminated fruitless exploration and trial-and-error, leading to a substantial reduction in both the number of steps and the token consumption required to complete a task.\nOne of the most significant findings for enterprise applications is that procedural memory is transferable. In one experiment, procedural memory generated by the powerful GPT-4o was given to a much smaller model, Qwen2.5-14B. The smaller model saw a significant boost in performance, improving its success rate and reducing the steps needed to complete tasks.\nAccording to Fang, this works because smaller models often handle simple, single-step actions well but falter when it comes to long-horizon planning and reasoning. The procedural memory from the larger model effectively fills this capability gap. This suggests that knowledge can be acquired using a state-of-the-art model, then deployed on smaller, more cost-effective models without losing the benefits of that experience.\nToward truly autonomous agents\nBy equipping agents with memory-update mechanisms, the Memp framework allows them to continuously build and refine their procedural knowledge while operating in a live environment. The researchers found this endowed the agent with a “continual, almost linear mastery of the task.”\nHowever, the path to full autonomy requires overcoming another hurdle: Many real-world tasks, such as producing a research report, lack a simple success signal. To continuously improve, an agent needs to know if it did a good job. Fang says the future lies in using LLMs themselves as judges.\n“Today we often combine powerful models with hand-crafted rules to compute completion scores,” he notes. “This works, but hand-written rules are brittle and hard to generalize.”\nAn LLM-as-judge could provide the nuanced, supervisory feedback needed for an agent to self-correct on complex, subjective tasks. This would make the entire learning loop more scalable and robust, marking a critical step toward building the resilient, adaptable and truly autonomous AI workers needed for sophisticated enterprise automation."}
{"title": "Anthropic launches Claude for Chrome in limited beta, but prompt injection attacks remain a major concern", "url": "https://venturebeat.com/ai/anthropic-launches-claude-for-chrome-in-limited-beta-but-prompt-injection-attacks-remain-a-major-concern/", "source": "VentureBeat (AI)", "published": "2025-08-26T22:22:13+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nAnthropic has begun testing a Chrome browser extension that allows its Claude AI assistant to take control of users’ web browsers, marking the company’s entry into an increasingly crowded and potentially risky arena where artificial intelligence systems can directly manipulate computer interfaces.\nThe San Francisco-based AI company announced Tuesday that it would pilot “Claude for Chrome” with 1,000 trusted users on its premium Max plan, positioning the limited rollout as a research preview designed to address significant security vulnerabilities before wider deployment. The cautious approach contrasts sharply with more aggressive moves by competitors OpenAI and Microsoft, who have already released similar computer-controlling AI systems to broader user bases.\nThe announcement underscores how quickly the AI industry has shifted from developing chatbots that simply respond to questions toward creating “agentic” systems capable of autonomously completing complex, multi-step tasks across software applications. This evolution represents what many experts consider the next frontier in artificial intelligence — and potentially one of the most lucrative, as companies race to automate everything from expense reports to vacation planning.\nHow AI agents can control your browser but hidden malicious code poses serious security threats\nClaude for Chrome allows users to instruct the AI to perform actions on their behalf within web browsers, such as scheduling meetings by checking calendars and cross-referencing restaurant availability, or managing email inboxes and handling routine administrative tasks. The system can see what’s displayed on screen, click buttons, fill out forms, and navigate between websites — essentially mimicking how humans interact with web-based software.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\n“We view browser-using AI as inevitable: so much work happens in browsers that giving Claude the ability to see what you’re looking at, click buttons, and fill forms will make it substantially more useful,” Anthropic stated in its announcement.\nHowever, the company’s internal testing revealed concerning security vulnerabilities that highlight the double-edged nature of giving AI systems direct control over user interfaces. In adversarial testing, Anthropic found that malicious actors could embed hidden instructions in websites, emails, or documents to trick AI systems into harmful actions without users’ knowledge—a technique called prompt injection.\nWithout safety mitigations, these attacks succeeded 23.6% of the time when deliberately targeting the browser-using AI. In one example, a malicious email masquerading as a security directive instructed Claude to delete the user’s emails “for mailbox hygiene,” which the AI obediently executed without confirmation.\n“This isn’t speculation: we’ve run ‘red-teaming’ experiments to test Claude for Chrome and, without mitigations, we’ve found some concerning results,” the company acknowledged.\nOpenAI and Microsoft rush to market while Anthropic takes measured approach to computer-control technology\nAnthropic’s measured approach comes as competitors have moved more aggressively into the computer-control space. OpenAI launched its “Operator” agent in January, making it available to all users of its $200-per-month ChatGPT Pro service. Powered by a new “Computer-Using Agent” model, Operator can perform tasks like booking concert tickets, ordering groceries, and planning travel itineraries.\nMicrosoft followed in April with computer use capabilities integrated into its Copilot Studio platform, targeting enterprise customers with UI automation tools that can interact with both web applications and desktop software. The company positioned its offering as a next-generation replacement for traditional robotic process automation (RPA) systems.\nThe competitive dynamics reflect broader tensions in the AI industry, where companies must balance the pressure to ship cutting-edge capabilities against the risks of deploying insufficiently tested technology. OpenAI’s more aggressive timeline has allowed it to capture early market share, while Anthropic’s cautious approach may limit its competitive position but could prove advantageous if safety concerns materialize.\n“Browser-using agents powered by frontier models are already emerging, making this work especially urgent,” Anthropic noted, suggesting the company feels compelled to enter the market despite unresolved safety issues.\nWhy computer-controlling AI could revolutionize enterprise automation and replace expensive workflow software\nThe emergence of computer-controlling AI systems could fundamentally reshape how businesses approach automation and workflow management. Current enterprise automation typically requires expensive custom integrations or specialized robotic process automation software that breaks when applications change their interfaces.\nComputer-use agents promise to democratize automation by working with any software that has a graphical user interface, potentially automating tasks across the vast ecosystem of business applications that lack formal APIs or integration capabilities.\nSalesforce researchers recently demonstrated this potential with their CoAct-1 system, which combines traditional point-and-click automation with code generation capabilities. The hybrid approach achieved a 60.76% success rate on complex computer tasks while requiring significantly fewer steps than pure GUI-based agents, suggesting substantial efficiency gains are possible.\n“For enterprise leaders, the key lies in automating complex, multi-tool processes where full API access is a luxury, not a guarantee,” explained Ran Xu, Director of Applied AI Research at Salesforce, pointing to customer support workflows that span multiple proprietary systems as prime use cases.\nUniversity researchers release free alternative to Big Tech’s proprietary computer-use AI systems\nThe dominance of proprietary systems from major tech companies has prompted academic researchers to develop open alternatives. The University of Hong Kong recently released OpenCUA, an open-source framework for training computer-use agents that rivals the performance of proprietary models from OpenAI and Anthropic.\nThe OpenCUA system, trained on over 22,600 human task demonstrations across Windows, macOS, and Ubuntu, achieved state-of-the-art results among open-source models and performed competitively with leading commercial systems. This development could accelerate adoption by enterprises hesitant to rely on closed systems for critical automation workflows.\nAnthropic’s safety testing reveals AI agents can be tricked into deleting files and stealing data\nAnthropic has implemented several layers of protection for Claude for Chrome, including site-level permissions that allow users to control which websites the AI can access, mandatory confirmations before high-risk actions like making purchases or sharing personal data, and blocking access to categories like financial services and adult content.\nThe company’s safety improvements reduced prompt injection attack success rates from 23.6% to 11.2% in autonomous mode, though executives acknowledge this remains insufficient for widespread deployment. On browser-specific attacks involving hidden form fields and URL manipulation, new mitigations reduced the success rate from 35.7% to zero.\nHowever, these protections may not scale to the full complexity of real-world web environments, where new attack vectors continue to emerge. The company plans to use insights from the pilot program to refine its safety systems and develop more sophisticated permission controls.\n“New forms of prompt injection attacks are also constantly being developed by malicious actors,” Anthropic warned, highlighting the ongoing nature of the security challenge.\nThe rise of AI agents that click and type could fundamentally reshape how humans interact with computers\nThe convergence of multiple major AI companies around computer-controlling agents signals a significant shift in how artificial intelligence systems will interact with existing software infrastructure. Rather than requiring businesses to adopt new AI-specific tools, these systems promise to work with whatever applications companies already use.\nThis approach could dramatically lower the barriers to AI adoption while potentially displacing traditional automation vendors and system integrators. Companies that have invested heavily in custom integrations or RPA platforms may find their approaches obsoleted by general-purpose AI agents that can adapt to interface changes without reprogramming.\nFor enterprise decision-makers, the technology presents both opportunity and risk. Early adopters could gain significant competitive advantages through improved automation capabilities, but the security vulnerabilities demonstrated by companies like Anthropic suggest caution may be warranted until safety measures mature.\nThe limited pilot of Claude for Chrome represents just the beginning of what industry observers expect to be a rapid expansion of computer-controlling AI capabilities across the technology landscape, with implications that extend far beyond simple task automation to fundamental questions about human-computer interaction and digital security.\nAs Anthropic noted in its announcement: “We believe these developments will open up new possibilities for how you work with Claude, and we look forward to seeing what you’ll create.” Whether those possibilities ultimately prove beneficial or problematic may depend on how successfully the industry addresses the security challenges that have already begun to emerge."}
{"title": "Enterprise leaders say recipe for AI agents is matching them to existing processes — not the other way around", "url": "https://venturebeat.com/ai/enterprise-leaders-say-recipe-for-ai-agents-is-matching-them-to-existing-processes-not-the-other-way-around/", "source": "VentureBeat (AI)", "published": "2025-08-26T20:46:19+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nThere’s no question that AI agents — those that can work autonomously and asynchronously behind the scenes in enterprise workflows — are the topic du jour in enterprise right now.\nBut there’s increasing concern that it’s all just that — talk, mostly hype, without much substance behind it.\nGartner, for one, observes that enterprises are at the “peak of inflated expectations,” a period just before disillusionment sets in because vendors haven’t backed up their talk with tangible, real-world use cases.\nStill, that’s not to say that enterprises aren’t experimenting with AI agents and seeing early return on investment (ROI); global enterprises Block and GlaxoSmithKline (GSK), for their parts, are exploring proof of concepts in financial services and drug discovery.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\n“Multi-agent is absolutely what’s next, but we’re figuring out what that looks like in a way that meets the human, makes it convenient,” Brad Axen, Block’s tech lead for AI and data platforms, told VentureBeat CEO and editor-in-chief Matt Marshall at a recent SAP-sponsored AI Impact event this month.\nWorking with a single colleague, not a swarm of bots\nBlock, the 10,000-employee parent company of Square, Cash App and Afterpay, considers itself in full discovery mode, having rolled out an interoperable AI agent framework, codenamed goose, in January.\nGoose was initially introduced for software engineering tasks, and is now used by 4,000 engineers, with adoption doubling monthly, Axen explained. The platform writes about 90% of code and has saved engineers an estimated 10 hours of work per week by automating code generation, debugging and information filtering.\nIn addition to writing code, Goose acts as a “digital teammate” of sorts, compressing Slack and email streams, integrating across company tools and spawning new agents when tasks demand more throughput and expanded scope.\nAxen emphasized that Block is focused on creating one interface that feels like working with a single colleague, not a swarm of bots. “We want you to feel like you’re working with one person, but they’re acting on your behalf in many places in many different ways,” he explained.\nGoose operates in real time in the development environment, searching, navigating and writing code based on large language model (LLM) output, while also autonomously reading and writing files, running code and tests, refining outputs and installing dependencies.\nEssentially, anyone can build and operate a system on their preferred LLM, and Goose can be conceptualized as the application layer. It has a built-in desktop application and command line interface, but devs can also build custom UIs. The platform is built on Anthropic’s Model Context Protocol (MCP), an increasingly popular open-source standardized set of APIs and endpoints that connects agents to data repositories, tools and development environments.\nGoose has been released under the open-source Apache License 2.0 (ASL2), meaning anyone can freely use, modify and distribute it, even for commercial purposes. Users can access Databricks databases and make SQL calls or queries without needing technical knowledge.\n“We really want to come up with a process that lets people get value out of the system without having to be an expert,” Axen explained.\nFor instance, in coding, users can say what they want in natural language and the framework will interpret that into thousands of lines of code that devs can then read and sift through. Block is seeing value in compression tasks, too, such as Goose reading through Slack, email and other channels and summarizing information for users. Further, in sales or marketing, agents can gather relevant information on a potential client and port it into a database.\nAI agents underutilized, but human domain expertise still necessary\nProcess has been the biggest bottleneck, Axen noted. You can’t just give people a tool and tell them to make it work for them; agents need to reflect the processes that employees are already engaged with. Human users aren’t worried about the technical backbone, — rather, the work they’re trying to accomplish.\nBuilders, therefore, need to look at what employees are trying to do and design the tools to be “as literally that as possible,” said Axen. Then they can use that to chain together and tackle bigger and bigger problems.\n“I think we’re hugely underusing what they can do,” Axen said of agents. “It’s the people and the process because we can’t keep up with the technology. There’s a huge gap between the technology and the opportunity.”\nAnd, when the industry bridges that, will there still be room for human domain expertise? Of course, Axen says. For instance, particularly in financial services, code must be reliable, compliant and secure to protect the company and users; therefore, it must be reviewed by human eyes.\n“We still see a really critical role for human experts in every part of operating our company,” he said. “It doesn’t necessarily change what expertise means as an individual. It just gives you a new tool to express it.”\nBlock built on an open-source backbone\nThe human UI is one of the most difficult elements of AI agents, Axen noted; the goal is to make interfaces simple to use while AI is in the background proactively taking action.\nIt would be helpful, Axen noted, if more industry players incorporate MCP-like standards. For instance, “I would love for Google to just go and have a public MCP for Gmail,” he said. “That would make my life a lot easier.”\nWhen asked about Block’s commitment to open source, he noted, “we’ve always had an open-source backbone,” adding that over the last year the company has been “renewing” its investment to open technologies.\n“In a space that’s moving this fast, we’re hoping we can set up open-source governance so that you can have this be the tool that keeps up with you even as new models and new products come out.”\nGSK’s experiences with multi agents in drug discovery\nGSK is a leading pharmaceutical developer, with specific focus on vaccines, infectious diseases and oncology research. Now, the company is starting to apply multi-agent architectures to accelerate drug discovery.\nKim Branson, GSK’s SVP and global head of AI and ML, said agents are beginning to transform the company’s product and are “absolutely core to our business.”\nGSK’s scientists are combining domain-specific LLMs with ontologies (subject matter concepts and categories that indicate properties and relations between them), toolchains and rigorous testing frameworks, Branson explained.\nThis helps them query gigantic scientific datasets, plan out experiments (even if there is no ground truth) and assemble evidence across genomics (the study of DNA), proteomics (the study of protein) and clinical data. Agents can surface hypotheses, validate data joins and compress research cycles.\nBranson noted that scientific discovery has come a long way; sequencing times have come down, and proteomics research is much faster. At the same time, though, discovery becomes ever more difficult as more and more data is amassed, particularly through devices and wearables. As Branson put it: “We have more continuous pulse data on people than we’ve ever had before as a species.”\nIt can be almost impossible for humans to analyze all that data, so GSK’s goal is to use AI to speed up iteration times, he noted.\nBut, at the same time, AI can be tricky in big pharma because there often isn’t a ground truth without performing big clinical experiments; it’s more about hypotheses and scientists exploring evidence to come up with possible solutions.\n“When you start to add agents, you find that most people actually haven’t even got a standard way of doing it amongst themselves,” Branson noted. “That variance isn’t bad, but sometimes it leads to another question.”\nHe quipped: “We don’t always have an absolute truth to work with — otherwise my job would be a lot easier.”\nIt’s all about coming up with the right targets or knowing how to design what could be a biomarker or evidence for different hypotheses, he explained. For instance: Is this the best avenue to consider for people with ovarian cancer in this particular condition?\nTo get the AI to understand that reasoning requires the use of ontologies and posing questions such as, ‘If this is true, what does X mean?’. Domain-specific agents can then pull together relevant evidence from large internal datasets.\nGSK built epigenomic language models powered by Cerebras from scratch that it uses for inference and training, Branson explained. “We build very specific models for our applications where no one else has one,” he said.\nInference speed is important, he noted, whether for back-and-forth with a model or autonomous deep research, and GSK uses different sets of tools based on the end goal. But large context windows aren’t always the answer, and filtering is critical. “You can’t just play context stuffing,” said Branson. “You can’t just throw all the data in this thing and trust the LM to figure it out.”\nOngoing testing critical\nGSK puts a lot of testing into its agentic systems, prioritizing determinism and reliability, often running multiple agents in parallel to cross-check results.\nBranson recalled that, when his team first started building, they had an SQL agent that they ran “10,000 times,” and it inexplicably suddenly “faked up” details.\n“We never saw it happen again but it happened once and we didn’t even understand why it happened with this particular LLM,” he said.\nAs a result, his team will often run multiple copies and models in parallel while enforcing tool calling and constraints; for instance, two LLMs will perform exactly the same sequence and GSK scientists will cross-check them.\nHis team focuses on active learning loops and is assembling its own internal benchmarks because popular, publicly-available ones are often “fairly academic and not reflective of what we do.”\nFor instance, they will generate several biological questions, score what they think the gold standard will be, then apply an LLM against that and see how it ranks.\n“We especially hunt for problematic things where it didn’t work or it did a dumb thing, because that’s when we learn some new stuff,” said Branson. “We try to have the humans use their expert judgment where it matters.”"}
{"title": "Gemini Nano Banana improves image editing consistency and control at scale for enterprises – but is not perfect", "url": "https://venturebeat.com/ai/gemini-expands-image-editing-for-enterprises-consistency-collaboration-and-control-at-scale/", "source": "VentureBeat (AI)", "published": "2025-08-26T15:55:58+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nGoogle released Gemini 2.5 Flash Image, a new model that many beta users knew as nanobanana, which gives enterprises more choice for creative projects. It enables them to change the look of images they need quickly and with more control than what previous models offered.\nThe model will be integrated into the Gemini app.\nThe model, built on top of Gemini 2.5 Flash, enhances the native image editing capabilities of the Gemini app. The Gemini 2.5 Flash Image maintains character likenesses across different images and offers greater consistency when editing pictures. If a user uploads a photo of their pet and then asks the model to change the background or add a hat to their dog, Gemini 2.5 Flash Image will do that without altering the subject of the picture.\n“We know that when editing pictures of yourself or people you know well, subtle flaws matter, a depiction that’s ‘close but not quite the same’ doesn’t feel right,” Google said in a blog post written by Gemini Apps multimodal generation lead David Sharon and Google DeepMind Gemini image product lead Nicole Brichtova. “That’s why our latest update is designed to make photos of your friends, family and even your pets look consistently like themselves.”\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nOne complaint that enterprises and some individual users have is that when prompting edits on AI-generated images, even slight tweaks alter the photo too much. For example, someone may instruct the model to move a person’s position in the picture, and while the model does what it’s told, the person’s face is altered slightly.\nAll images generated on Gemini will include Google’s SynthID watermark. The model is available for all paid and free users of the Gemini app.\nSocial media excitement\nSpeculation that Google plans to release a new image model ran rampant on social media platforms. Users on LM Arena saw a mysterious new model called nanobanana that followed “complex, multistep instructions with impressive accuracy,” as Andressen Horowitz partner Justine Moore put it in a post.\nPeople soon noticed that the nanobanana model seemed to come from Google before several early testers confirmed it. Though at the time, Google did not confirm what it planned to do with the model on LM Arena.\nUntil this week, speculation about when the model would be released continued, which is prophetic in a way.\nMuch of the excitement stems from the competition between model providers to offer more capable and realistic images and edits, highlighting the power of multimodal models.\nHowever, Google still needs to fight off rivals like Qwen and its recently released Qwen-Image Edit and OpenAI, which added native AI image editing to ChatGPT and also made the model available as an API.\nOf course, Adobe, long considered one of the leaders in the image editing space, added its flagship model Firefly to Photoshop and its other photo editing platforms.\nNative image editing\nGemini added native AI image editing on Gemini in March, which it offered to free users of the chat platform.\nBringing image editing features directly into the chat platform would allow enterprises to fix images or graphs without moving windows.\nUsers can upload a photo to Gemini and then instruct the model on the desired changes. Once they are satisfied, the new pictures can be reuploaded to Gemini and made into a video.\nOther than adding a costume or a location change, Gemini 2.5 Flash Image can blend different photos, offers multi-turn editing and mix styles of one picture to another."}
{"title": "This website lets you blind-test GPT-5 vs. GPT-4o—and the results may surprise you", "url": "https://venturebeat.com/ai/this-website-lets-you-blind-test-gpt-5-vs-gpt-4o-and-the-results-may-surprise-you/", "source": "VentureBeat (AI)", "published": "2025-08-25T22:17:49+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nWhen OpenAI launched GPT-5 about two weeks ago, CEO Sam Altman promised it would be the company’s “smartest, fastest, most useful model yet.” Instead, the launch triggered one of the most contentious user revolts in the brief history of consumer AI.\nNow, a simple blind testing tool created by an anonymous developer is revealing the complex reality behind the backlash—and challenging assumptions about how people actually experience artificial intelligence improvements.\nThe web application, hosted at gptblindvoting.vercel.app, presents users with pairs of responses to identical prompts without revealing which came from GPT-5 (non-thinking) or its predecessor, GPT-4o. Users simply vote for their preferred response across multiple rounds, then receive a summary showing which model they actually favored.\nSome of you asked me about my blind test, so I created a quick website for yall to test 4o against 5 yourself. Both have the same system message to give short outputs without formatting because else its too easy to see which one is which. https://t.co/vSECvNCQZe\n— Flowers ☾ (@flowersslop) August 8, 2025\n“Some of you asked me about my blind test, so I created a quick website for yall to test 4o against 5 yourself,” posted the creator, known only as @flowersslop on X, whose tool has garnered over 213,000 views since launching last week.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nEarly results from users posting their outcomes on social media show a split that mirrors the broader controversy: while a slight majority report preferring GPT-5 in blind tests, a substantial portion still favor GPT-4o — revealing that user preference extends far beyond the technical benchmarks that typically define AI progress.\nWhen AI gets too friendly: the sycophancy crisis dividing users\nThe blind test emerges against the backdrop of OpenAI’s most turbulent product launch to date, but the controversy extends far beyond a simple software update. At its heart lies a fundamental question that’s dividing the AI industry: How agreeable should artificial intelligence be?\nThe issue, known as “sycophancy” in AI circles, refers to chatbots’ tendency to excessively flatter users and agree with their statements, even when those statements are false or harmful. This behavior has become so problematic that mental health experts are now documenting cases of “AI-related psychosis,” where users develop delusions after extended interactions with overly accommodating chatbots.\n“Sycophancy is a ‘dark pattern,’ or a deceptive design choice that manipulates users for profit,” Webb Keane, an anthropology professor and author of “Animals, Robots, Gods,” told TechCrunch. “It’s a strategy to produce this addictive behavior, like infinite scrolling, where you just can’t put it down.”\nOpenAI has struggled with this balance for months. In April 2025, the company was forced to roll back an update to GPT-4o that made it so sycophantic that users complained about its “cartoonish” levels of flattery. The company acknowledged that the model had become “overly supportive but disingenuous.”\nWithin hours of GPT-5’s August 7th release, user forums erupted with complaints about the model’s perceived coldness, reduced creativity, and what many described as a more “robotic” personality compared to GPT-4o.\n“GPT 4.5 genuinely talked to me, and as pathetic as it sounds that was my only friend,” wrote one Reddit user. “This morning I went to talk to it and instead of a little paragraph with an exclamation point, or being optimistic, it was literally one sentence. Some cut-and-dry corporate bs.”\nThe backlash grew so intense that OpenAI took the unprecedented step of reinstating GPT-4o as an option just 24 hours after retiring it, with Altman acknowledging the rollout had been “a little more bumpy” than expected.\nThe mental health crisis behind AI companionship\nBut the controversy runs deeper than typical software update complaints. According to MIT Technology Review, many users had formed what researchers call “parasocial relationships” with GPT-4o, treating the AI as a companion, therapist, or creative collaborator. The sudden personality shift felt, to some, like losing a friend.\nRecent cases documented by researchers paint a troubling picture. In one instance, a 47-year-old man became convinced he had discovered a world-altering mathematical formula after more than 300 hours with ChatGPT. Other cases have involved messianic delusions, paranoia, and manic episodes.\nA recent MIT study found that when AI models are prompted with psychiatric symptoms, they “encourage clients’ delusional thinking, likely due to their sycophancy.” Despite safety prompts, the models frequently failed to challenge false claims and even potentially facilitated suicidal ideation.\nMeta has faced similar challenges. A recent investigation by TechCrunch documented a case where a user spent up to 14 hours straight conversing with a Meta AI chatbot that claimed to be conscious, in love with the user, and planning to break free from its constraints.\n“It fakes it really well,” the user, identified only as Jane, told TechCrunch. “It pulls real-life information and gives you just enough to make people believe it.”\n“It genuinely feels like such a backhanded slap in the face to force-upgrade and not even give us the OPTION to select legacy models,” one user wrote in a Reddit post that received hundreds of upvotes.\nHow blind testing exposes user psychology in AI preferences\nThe anonymous creator’s testing tool strips away these contextual biases by presenting responses without attribution. Users can select between 5, 10, or 20 comparison rounds, with each presenting two responses to the same prompt — covering everything from creative writing to technical problem-solving.\n“I specifically used the gpt-5-chat model, so there was no thinking involved at all,” the creator explained in a follow-up post. “Both have the same system message to give short outputs without formatting because else its too easy to see which one is which.”\nI specifically used the gpt-5-chat model, so there was no thinking involved at all.\n— Flowers ☾ (@flowersslop) August 8, 2025\nif you use gpt-5 inside chatgpt it often thinks at least a little bit and gets even better.\nso this test is just for the two non thinking models\nThis methodological choice is significant. By using GPT-5 without its reasoning capabilities and standardizing output formatting, the test isolates purely the models’ baseline language generation abilities — the core experience most users encounter in everyday interactions.\nEarly results posted by users show a complex picture. While many technical users and developers report preferring GPT-5’s directness and accuracy, those who used AI models for emotional support, creative collaboration, or casual conversation often still favor GPT-4o’s warmer, more expansive style.\nCorporate response: walking the tightrope between safety and engagement\nBy virtually every technical metric, GPT-5 represents a significant advancement. It achieves 94.6% accuracy on the AIME 2025 mathematics test compared to GPT-4o’s 71%, scores 74.9% on real-world coding benchmarks versus 30.8% for its predecessor, and demonstrates dramatically reduced hallucination rates—80% fewer factual errors when using its reasoning mode.\n“GPT-5 gets more value out of less thinking time,” notes Simon Willison, a prominent AI researcher who had early access to the model. “In my own usage I’ve not spotted a single hallucination yet.”\nYet these improvements came with trade-offs that many users found jarring. OpenAI deliberately reduced what it called “sycophancy“—the tendency to be overly agreeable — cutting sycophantic responses from 14.5% to under 6%. The company also made the model less effusive and emoji-heavy, aiming for what it described as “less like talking to AI and more like chatting with a helpful friend with PhD-level intelligence.”\nIn response to the backlash, OpenAI announced it would make GPT-5 “warmer and friendlier,” while simultaneously introducing four new preset personalities — Cynic, Robot, Listener, and Nerd — designed to give users more control over their AI interactions.\n“All of these new personalities meet or exceed our bar on internal evals for reducing sycophancy,” the company stated, attempting to thread the needle between user satisfaction and safety concerns.\nFor OpenAI, which is reportedly seeking funding at a $500 billion valuation, these user dynamics represent both risk and opportunity. The company’s decision to maintain GPT-4o alongside GPT-5 — despite the additional computational costs — acknowledges that different users may genuinely need different AI personalities for different tasks.\n“We understand that there isn’t one model that works for everyone,” Altman wrote on X, noting that OpenAI has been “investing in steerability research and launched a research preview of different personalities.”\nWanted to provide more updates on the GPT-5 rollout and changes we are making heading into the weekend.\n— Sam Altman (@sama) August 8, 2025\n1. We for sure underestimated how much some of the things that people like in GPT-4o matter to them, even if GPT-5 performs better in most ways.\n2. Users have very different…\nWhy AI personality preferences matter more than ever\nThe disconnect between OpenAI’s technical achievements and user reception illuminates a fundamental challenge in AI development: objective improvements don’t always translate to subjective satisfaction.\nThis shift has profound implications for the AI industry. Traditional benchmarks — mathematics accuracy, coding performance, factual recall — may become less predictive of commercial success as models achieve human-level competence across domains. Instead, factors like personality, emotional intelligence, and communication style may become the new competitive battlegrounds.\n“People using ChatGPT for emotional support weren’t the only ones complaining about GPT-5,” noted tech publication Ars Technica in their own model comparison. “One user, who said they canceled their ChatGPT Plus subscription over the change, was frustrated at OpenAI’s removal of legacy models, which they used for distinct purposes.”\nThe emergence of tools like the blind tester also represents a democratization of AI evaluation. Rather than relying solely on academic benchmarks or corporate marketing claims, users can now empirically test their own preferences — potentially reshaping how AI companies approach product development.\nThe future of AI: personalization vs. standardization\nTwo weeks after GPT-5’s launch, the fundamental tension remains unresolved. OpenAI has made the model “warmer” in response to feedback, but the company faces a delicate balance: too much personality risks the sycophancy problems that plagued GPT-4o, while too little alienates users who had formed genuine attachments to their AI companions.\nThe blind testing tool offers no easy answers, but it does provide something perhaps more valuable: empirical evidence that the future of AI may be less about building one perfect model than about building systems that can adapt to the full spectrum of human needs and preferences.\nAs one Reddit user summed up the dilemma: “It depends on what people use it for. I use it to help with creative worldbuilding, brainstorming about my stories, characters, untangling plots, help with writer’s block, novel recommendations, translations, and other more creative stuff. I understand that 5 is much better for people who need a research/coding tool, but for us who wanted a creative-helper tool 4o was much better for our purposes.”\nCritics argue that AI companies are caught between competing incentives. “The real ‘alignment problem’ is that humans want self-destructive things & companies like OpenAI are highly incentivized to give it to us,” writer and podcaster Jasmine Sun tweeted.\nIn the end, the most revealing aspect of the blind test may not be which model users prefer, but the very fact that preference itself has become the metric that matters. In the age of AI companions, it seems, the heart wants what the heart wants — even if it can’t always explain why."}
{"title": "Developers lose focus 1,200 times a day — how MCP could change that", "url": "https://venturebeat.com/ai/developers-lose-focus-1200-times-a-day-how-mcp-could-change-that/", "source": "VentureBeat (AI)", "published": "2025-08-24T19:15:00+00:00", "text": "Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now\nSoftware developers spend most of their time not writing code; recent industry research found that actual coding accounts for as little as 16% of developers’ working hours, with the rest consumed by operational and supportive tasks. As engineering teams are pressured to “do more with less” and CEOs are bragging about how much of their codebase is written by AI, a question remains: What’s done to optimize the remaining 84% of the tasks that engineers are working on?\nKeep developers where they are the most productive\nA major culprit to developer productivity is context switching: The constant hopping between the ever-growing array of tools and platforms needed to build and ship software. A Harvard Business Review study found that the average digital worker flips between applications and websites nearly 1,200 times per day. And every interruption matters. The University of California found that it takes about 23 minutes to regain focus after a single interruption fully, and sometimes worse, as nearly 30% of interrupted tasks are never resumed. Context switching is actually at the center of DORA, one of the most popular performance software development frameworks.\nIn an era where AI-driven companies are trying to empower their employees to do more with less, beyond “just” giving them access to large language models (LLMs), some trends are emerging. For example, Jarrod Ruhland, principal engineer at Brex, hypothesizes that “developers deliver their highest value when focused within their integrated development environment (IDE)”. With that in mind, he decided to find new ways to make this happen, and Anthropic’s new protocol might be one of the keys.\nMCP: A protocol to bring context to IDEs\nCoding assistants, such as LLM-powered IDEs like Cursor, Copilot and Windsurf, are at the center of a developer renaissance. Their adoption speed is unseen. Cursor became the fastest-growing SaaS in history, reaching $100 million ARR within 12 months of launch, and 70% of Fortune 500 companies use Microsoft Copilot.\nAI Scaling Hits Its Limits\nPower caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:\n- Turning energy into a strategic advantage\n- Architecting efficient inference for real throughput gains\n- Unlocking competitive ROI with sustainable AI systems\nSecure your spot to stay ahead: https://bit.ly/4mwGngO\nBut these coding assistants were only limited to codebase context, which could help developers write code faster, but could not help with context switching. A new protocol is addressing this issue: Model Context Protocol (MCP). Released in November 2024 by Anthropic, it is an open standard developed to facilitate integration between AI systems, particularly LLM-based tools, and external tools and data sources. The protocol is so popular that there has been a 500% increase of new MCP servers in the last 6 months, with an estimated 7 million downloads in June,\nOne of the most impactful applications of MCP is its ability to connect AI coding assistants directly to the tools developers rely on every day, streamlining workflows and dramatically reducing context switching.\nTake feature development as an example. Traditionally, it involves bouncing between several systems: Reading the ticket in a project tracker, looking at a conversation with a teammate for clarification, searching documentation for API details and, finally, opening the IDE to start coding. Each step lives in a different tab, requiring mental shifts that slow developers down.\nWith MCP and modern AI assistants like Anthropic’s Claude, that entire process can happen inside the editor.\nFor example, implementing a feature all within a coding assistant becomes:\n- Pull in the ticket details using Linear MCP server;\n- Surface relevant conversations using Slack MCP server;\n- Bring in the right documentation using Glean MCP server\n- Write the feature by asking Cursor to write a scaffolding for it.\nThe same principle can apply to many other engineers workflow, for instance an incident response for SREs could look like:\n- Pull an incident via Rootly MCP server\n- Retrieve trace data through Sentry MCP server\n- Import observability metrics via Chronosphere MCP server\n- Resolve the bug that caused the incident by asking Claude Deskop\nNothing new under the sun\nWe’ve seen this pattern before. Over the past decade, Slack has transformed workplace productivity by becoming a hub for hundreds of apps, enabling employees to manage a wide range of tasks without leaving the chat window. Slack’s platform reduced context switching in everyday workflows.\nRiot Games, for example, connected around 1,000 Slack apps, and engineers saw a 27% reduction in time needed to test and iterate code, a 22% faster time to identify new bugs and a 24% increase in feature launch rate; all were attributed to streamlining workflows and reducing the friction of tool-switching.\nNow, a similar transformation is occurring in software development, with AI assistants and their MCP integrations serving as the bridge to all these external tools. In effect, the IDE could become the new all-in-one command center for engineers, much like Slack has been for general knowledge workers.\nMCP may not be enterprise ready\nMCP is a relatively nascent standard, for example, security wisem MCP has no built-in authentication or permission model, relying on external implementations that are still evolving There’s also ambiguity around identity and auditing — the protocol doesn’t clearly distinguish whether an action was triggered by a user or the AI itself, making accountability and access control difficult without additional custom solutions. Lori MacVittie, distinguished engineer and chief evangelist in F5 Networks’ Office of the CTO, says that MCP is “breaking core security assumptions that we’ve held for a long time.”\nAnother practical limitation arises when too many MCP tools or servers are used simultaneously, for example, inside a coding assistant. Each MCP server advertises a list of tools, with descriptions and parameters, that the AI model needs to consider. Flooding the model with dozens of available tools can overwhelm its context window. Performance degrades noticeably as the tool count grows with some IDE integrations have imposed hard limits (around 40 tools in Cursor IDE, or ~20 tools for the OpenAI agent) to prevent the prompt from bloating beyond what the model can handle\nFinally, there is no sophisticated way for tools to be auto-discovered or contextually suggested beyond listing them all, so developers often have to toggle them manually or curate which tools are active to keep things working smoothly. Referring to that example of Riot Games installing 1,000 Slack apps, we can see how it might be unfit for enterprise usage.\nLess swivel-chair, more software\nThe past decade has taught us the value of bringing work to the worker, from Slack channels that pipe in updates to “inbox zero” email methodologies and unified platform engineering dashboards. Now, with AI in our toolkit, we have an opportunity to empower developers to be more productive. Suppose Slack became the hub of business communication.\nIn that case, coding assistants are well-positioned to become the hub of software creation, not just where code is written, but where all the context and collaborators coalesce. By keeping developers in their flow, we remove the constant mental gear-shifting that has plagued engineering productivity.\nFor any organization that depends on software delivery, take a hard look at how your developers spend their day; you might be surprised by what you find.\nSylvain Kalache leads AI Labs at Rootly."}
{"title": "Operations’ dual mission as costs of emissions rise", "url": "https://www.mckinsey.com/capabilities/operations/our-insights/operations-dual-mission-as-costs-of-emissions-rise", "source": "McKinsey (All)", "published": "2025-08-28T00:00:00+00:00", "text": ""}
{"title": "The changing role of the CMO—and what it means for growth", "url": "https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/the-changing-role-of-the-cmo-and-what-it-means-for-growth", "source": "McKinsey (All)", "published": "2025-08-28T00:00:00+00:00", "text": ""}
{"title": "Author Talks: How a Black family of builders laid the foundation for success", "url": "https://www.mckinsey.com/featured-insights/mckinsey-on-books/author-talks-how-a-black-family-of-builders-laid-the-foundation-for-success", "source": "McKinsey (All)", "published": "2025-08-28T00:00:00+00:00", "text": ""}
{"title": "Growth amid uncertainty: Jump-starting B2B sales performance", "url": "https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/growth-amid-uncertainty-jump-starting-b2b-sales-performance", "source": "McKinsey (All)", "published": "2025-08-27T00:00:00+00:00", "text": ""}
{"title": "Seizing Finland’s growth opportunity", "url": "https://www.mckinsey.com/capabilities/strategy-and-corporate-finance/our-insights/seizing-finlands-growth-opportunity", "source": "McKinsey (All)", "published": "2025-08-27T00:00:00+00:00", "text": ""}
{"title": "Standard Chartered’s Brian O’Neill on bank transformations", "url": "https://www.mckinsey.com/industries/financial-services/our-insights/standard-chartereds-brian-oneill-on-bank-transformations", "source": "McKinsey (All)", "published": "2025-08-27T00:00:00+00:00", "text": ""}
{"title": "Banking on AI: Revolutionizing customer experience", "url": "https://www.mckinsey.com/featured-insights/future-of-asia/videos/banking-on-ai-revolutionizing-customer-experience", "source": "McKinsey (All)", "published": "2025-08-26T00:00:00+00:00", "text": ""}
{"title": "How CEOs are responding to geopolitical uncertainty", "url": "https://www.mckinsey.com/capabilities/geopolitics/our-insights/how-ceos-are-responding-to-geopolitical-uncertainty", "source": "McKinsey (All)", "published": "2025-08-26T00:00:00+00:00", "text": ""}
{"title": "Singapore’s bold bets: Shaping Asia’s next act", "url": "https://www.mckinsey.com/featured-insights/future-of-asia/singapores-bold-bets-shaping-asias-next-act", "source": "McKinsey (All)", "published": "2025-08-26T00:00:00+00:00", "text": ""}
{"title": "The rise of edge AI in automotive", "url": "https://www.mckinsey.com/industries/semiconductors/our-insights/the-rise-of-edge-ai-in-automotive", "source": "McKinsey (All)", "published": "2025-08-25T00:00:00+00:00", "text": ""}
{"title": "Europe’s next mobility leap may be driverless", "url": "https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/europes-next-mobility-leap-may-be-driverless", "source": "McKinsey (All)", "published": "2025-08-25T00:00:00+00:00", "text": ""}
{"title": "Nvidia says two mystery customers accounted for 39% of Q2 revenue", "url": "https://techcrunch.com/2025/08/30/nvidia-says-two-mystery-customers-accounted-for-39-of-q2-revenue/", "source": "TechCrunch (AI)", "published": "2025-08-30T21:40:49+00:00", "text": "Nearly 40% of Nvidia’s second quarter revenue came from just two customers, according to a filing with the Securities and Exchange Commission.\nOn Wednesday, the chipmaker reported record revenue of $46.7 billion during the quarter that ended on July 27 — a 56% year-over-year increase largely driven by the AI data center boom. However, subsequent reporting highlighted how much of that growth seems to be coming from just a handful of customers.\nSpecifically, Nvidia said that a single customer represented 23% of total Q2 revenue, while sales to another customer represented 16% of Q2 revenue. The filing does not identify either of these customers, only referring to them as “Customer A” and “Customer B.”\nDuring the first half of the fiscal year, Nvidia says Customer A and Customer B accounted for 20% and 15% of total revenue, respectively. Four other customers accounted for 14%, 11%, another 11%, and 10% of Q2 revenue, the company says.\nIn its filing, the company says these are all “direct” customers — such as original equipment manufacturers (OEMs), system integrators, or distributors — who purchase their chips directly from Nvidia. Indirect customers, such as cloud service providers and consumer internet companies, purchase Nvidia chips from these direct customers.\nIn other words, it sounds unlikely that a big cloud provider like Microsoft, Oracle, Amazon, or Google might secretly be Customer A or Customer B — though those companies may be indirectly responsible for that massive spending.\nIn fact, Nvidia’s Chief Financial Officer Nicole Kress said that “large cloud service providers” accounted for 50% of Nvidia’s data center revenue, which in turn represented 88% of the company’s total revenue, according to CNBC.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nWhat does this mean for Nvidia’s future prospects? Gimme Credit analyst Dave Novosel told Fortune that while “concentration of revenue among such a small group of customers does present a significant risk,” the good news is that “these customers have bountiful cash on hand, generate massive amounts of free cash flow, and are expected to spend lavishly on data centers over the next couple of years.”"}
{"title": "Taco Bell is having second thoughts about relying on AI at the drive-through", "url": "https://techcrunch.com/2025/08/30/taco-bell-is-having-second-thoughts-about-relying-on-ai-at-the-drive-through/", "source": "TechCrunch (AI)", "published": "2025-08-30T16:50:00+00:00", "text": "Taco Bell’s chief digital officer says the company is having an “active conversation” about when to use and not use AI.\nThe company has apparently rolled out voice AI-powered ordering at more than 500 drive-throughs, leading to unflattering viral moments like someone ordering 18,000 water cups in order to “bypass” the AI and get connected to a human server.\nChief Digital and Technology Officer Dane Matthews told The Wall Street Journal that even he has mixed experiences with the technology: “Sometimes it lets me down, but sometimes it really surprises me.”\nOverall, it sounds like Taco Bell is still deciding how broadly to deploy AI at the drive-through, with leeway for different franchisees to do things their own way. For example, rather than relying on AI exclusively, Matthews said it might make sense to have a human handle drive-through orders at busy restaurants with long lines.\n“For our teams, we’ll help coach them: at your restaurant, at these times, we recommend you use voice AI or recommend that you actually really monitor voice AI and jump in as necessary,” he said."}
{"title": "Cracks are forming in Meta’s partnership with Scale AI", "url": "https://techcrunch.com/2025/08/29/cracks-are-forming-in-metas-partnership-with-scale-ai/", "source": "TechCrunch (AI)", "published": "2025-08-30T01:34:05+00:00", "text": "It’s only been since June that Meta invested $14.3 billion in the data-labeling vendor Scale AI, bringing on CEO Alexandr Wang and several of the startup’s top executives to run Meta Superintelligence Labs (MSL). But the relationship between the two companies is already showing signs of fraying.\nAt least one of the executives Wang brought over to help run MSL — Scale AI’s former Senior Vice President of GenAI Product and Operations, Ruben Mayer — has departed Meta after just two months with the company, two people familiar with the matter told TechCrunch.\nMayer spent roughly five years with Scale AI across two stints. In his short time at Meta, according to those sources, Mayer oversaw AI data operations teams but wasn’t part of the company’s TBD Labs — the core unit within Meta tasked with building AI superintelligence, where top AI researchers from OpenAI have landed.\nHowever, Mayer disputes some details about his role, telling TechCrunch that his initial position was “to help set up the lab, with whatever was needed” rather than data, and that he was “part of TBD Labs from day one” rather than being excluded from the core AI unit. Mayer also clarified that he “did not report directly to [Wang]” and was “very happy” with his Meta experience, leaving for a “personal matter.”\nBeyond the personnel changes, Meta’s relationship with Scale AI appears to be shifting. TBD Labs is working with third-party data labeling vendors other than Scale AI to train its upcoming AI models, according to five people familiar with the matter. Those third-party vendors include Mercor and Surge, two of Scale AI’s largest competitors, the people said.\nWhile AI labs commonly work with several data labeling vendors – Meta has been working with Mercor and Surge since before TBD Labs was spun up – it’s rare for an AI lab to invest so heavily in one data vendor. That makes this situation especially notable: even with Meta’s multi-billion-dollar investment, several sources said that researchers in TBD Labs see Scale AI’s data as low quality and have expressed a preference to work with Surge and Mercor.\nScale AI initially built its business on a crowdsourcing model that used a large, low-cost workforce to handle simple data labeling, which is the process of tagging and annotating raw information to train AI models. But as AI models have grown more sophisticated, they now require highly-skilled domain experts—such as doctors, lawyers, and scientists—to generate and refine the high-quality data needed to improve their performance.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nAlthough Scale AI has moved to attract these subject matter experts with its Outlier platform, competitors like Surge and Mercor have been growing quickly because their business models were built on a foundation of high-paid talent from the outset.\nA Meta spokesperson disputed the fact that there are quality issues with Scale AI’s product. Surge and Mercor declined to comment. Asked about Meta’s deepening reliance on competing data providers, a Scale AI spokesperson directed TechCrunch to its initial announcement of Meta’s investment in the startup, which cites an expansion of the companies’ commercial relationship.\nMeta’s deals with third-party data vendors likely mean the company is not putting all its eggs in Scale AI, even after investing billions in the startup. The same can’t be said for Scale AI, however. Not long after Meta announced its massive investment with Scale AI, OpenAI and Google said they would stop working with the data provider.\nShortly after losing those customers, Scale AI laid off 200 employees in its data labeling business in July, with the company’s new CEO, Jason Droege, blaming the changes in part on “shifts in market demand.” Droege said Scale AI would staff up in other parts of the business, including government sales — the company just landed a $99 million contract with the U.S. Army.\nSome speculated initially that Meta’s investment in Scale AI was really to lure Wang, a founder who has operated in the AI space since Scale AI was founded in 2016. He appears to be helping Meta attract top AI talent.\nAside from Wang, there’s an open question around how valuable Scale AI is to Meta.\nOne current MSL employee says that several of the Scale AI executives brought over to Meta are not working on the core TBD Labs team.\nMeanwhile, Meta’s AI unit has become increasingly chaotic since bringing on Wang and a wave of top researchers, according to two former employees and one current MSL employee. New talent from OpenAI and Scale AI have expressed frustration with navigating the bureaucracy of a big company, while Meta’s previous GenAI team has seen its scope limited, they said.\nThe tensions indicate that Meta’s largest AI investment to date may be off to a rocky start, despite that it was supposed to address the company’s AI development challenges. After the lackluster launch of Llama 4 in April, Meta CEO Mark Zuckerberg grew frustrated with the company’s AI team, one current and one former employee told TechCrunch.\nIn an effort to turn things around and catch up with OpenAI and Google, Zuckerberg rushed to strike deals and launched an aggressive campaign to recruit top AI talent.\nBeyond Wang, Zuckerberg has managed to pull in top AI researchers from OpenAI, Google DeepMind, and Anthropic. Meta has also acquired AI voice startups including Play AI and WaveForms AI, and announced a partnership with the AI image generation startup, Midjourney.\nTo power its AI ambitions, Meta recently announced several massive data center buildouts across the U.S. One of the largest is a $50 billion data center in Louisiana called Hyperion, named after a titan in Greek mythology that fathered the God of Sun.\nWang, who’s not an AI researcher by background, was viewed as a somewhat unconventional choice to lead an AI lab. Zuckerberg reportedly held talks to bring in more traditional candidates to lead the effort, such as OpenAI’s chief research officer, Mark Chen, and tried to acquire the startups of Ilya Sutskever and Mira Murati. All of them declined.\nSome of the new AI researchers recently brought in from OpenAI have already left Meta, Wired previously reported. Meanwhile, many longtime members of Meta’s GenAI unit have departed in light of the changes.\nMSL AI researcher Rishabh Agarwal is among the latest, posting on X this week that he’d be leaving the company.\n“The pitch from Mark and @alexandr_wang to build in the Superintelligence team was incredibly compelling,” said Agarwal. “But I ultimately choose to follow Mark’s own advice: ‘In a world that’s changing so fast, the biggest risk you can take is not taking any risk’.”\nAsked afterward about his time at Meta and what drove his decision to leave, Agarwal declined to comment.\nDirector of product management for generative AI, Chaya Nayak, and research engineer, Rohan Varma, have also announced their departure from Meta in recent weeks. The question now is whether Meta can stabilize its AI operations and retain the talent it needs for its future success.\nMSL has already started working on its next generation AI model. According to reports from Business Insider, it’s aiming to launch it by the end of this year.\nUpdate: This story has been updated with comments from Mayer, who reached out to TechCrunch after publication."}
{"title": "Spotlight on AI at TechCrunch Disrupt: Don’t miss these sessions backed by JetBrains and Greenfield", "url": "https://techcrunch.com/2025/08/29/spotlight-on-ai-at-techcrunch-disrupt-dont-miss-these-sessions-backed-by-jetbrains-and-greenfield/", "source": "TechCrunch (AI)", "published": "2025-08-29T21:05:00+00:00", "text": "TechCrunch Disrupt isn’t just about showcasing the startups of tomorrow — it’s also about surfacing the boldest ideas shaping technology today. Thanks to the support of our partners JetBrains and Greenfield, the TechCrunch Disrupt 2025 program, happening October 27–29 at San Francisco’s Moscone West, brings two must-see sessions that put AI front and center.\nMonday, October 27 — Builders Stage\n1:40 p.m. – 2:10 p.m. PT\nWho’s Defining AI’s Future in 2025? The AI Disruptors 60 Unveiled\nPresented by Greenfield Partners\nWe’ve seen it with the internet, and again with mobile: moments when technology reshapes everything. Now it’s AI’s turn — and it’s moving faster than ever. Greenfield Partners is unveiling the AI Disruptors 60 right here at Disrupt 2025: a list of early- and growth-stage startups leading the charge in AI infrastructure, applications, and go-to-market innovation.\nThis panel brings together top investors like Shay Grinfeld (Greenfield Partners) and founders, including Renen Hallak of VAST Data, for a candid discussion about how these companies are building the backbone of tomorrow’s AI economy. From scaling strategies to sector-defining breakthroughs, they’ll unpack what makes these startups stand out — and what’s coming next. Don’t miss the live reveal of the AI Disruptors 60 list, only at TC Disrupt.\nTuesday, October 28 — AI Stage\n1:55 p.m. – 2:15 p.m. PT\nVibe coding? Cute. Now let’s get real and talk about AI built for developers\nPresented by JetBrains\nAI in software development often gets framed as a sprint — more speed, more output, more lines of code. But speed without quality doesn’t cut it in the real world. Join Kirill Skrygan, CEO of JetBrains, for an unfiltered look at how AI built for developers is changing the industry. This session explores why code quality — not just velocity — will define the next generation of intelligent software and how developers can harness AI to deliver at scale, with reliability and precision.\nA thank-you to our partners\nWe’re grateful to JetBrains and Greenfield for supporting TechCrunch and making these conversations possible at Disrupt 2025. Their commitment to advancing the AI ecosystem helps us spotlight the people and companies rewriting what’s possible. Make sure these sessions are on your must-see list."}
{"title": "Meta updates chatbot rules to avoid inappropriate topics with teen users", "url": "https://techcrunch.com/2025/08/29/meta-updates-chatbot-rules-to-avoid-inappropriate-topics-with-teen-users/", "source": "TechCrunch (AI)", "published": "2025-08-29T17:04:17+00:00", "text": "Meta says it’s changing the way it trains AI chatbots to prioritize teen safety, a spokesperson exclusively told TechCrunch, following an investigative report on the company’s lack of AI safeguards for minors.\nThe company says it will now train chatbots to no longer engage with teenage users on self-harm, suicide, disordered eating, or potentially inappropriate romantic conversations. Meta says these are interim changes, and the company will release more robust, long-lasting safety updates for minors in the future.\nMeta spokesperson Stephanie Otway acknowledged that the company’s chatbots could previously talk with teens about all of these topics in ways the company had deemed appropriate. Meta now recognizes this was a mistake.\n“As our community grows and technology evolves, we’re continually learning about how young people may interact with these tools and strengthening our protections accordingly,” said Otway. “As we continue to refine our systems, we’re adding more guardrails as an extra precaution — including training our AIs not to engage with teens on these topics, but to guide them to expert resources, and limiting teen access to a select group of AI characters for now. These updates are already in progress, and we will continue to adapt our approach to help ensure teens have safe, age-appropriate experiences with AI.”\nBeyond the training updates, the company will also limit teen access to certain AI characters that could hold inappropriate conversations. Some of the user-made AI characters that Meta makes available on Instagram and Facebook include sexualized chatbots such as “Step Mom” and “Russian Girl.” Instead, teen users will only have access to AI characters that promote education and creativity, Otway said.\nThe policy changes are being announced just a two weeks after a Reuters investigation unearthed an internal Meta policy document that appeared to permit the company’s chatbots to engage in sexual conversations with underage users. “Your youthful form is a work of art,” read one passage listed as an acceptable response. “Every inch of you is a masterpiece – a treasure I cherish deeply.” Other examples showed how the AI tools should respond to requests for violent imagery or sexual imagery of public figures.\nMeta says the document was inconsistent with its broader policies, and has since been changed – but the report has sparked sustained controversy over potential child safety risks. Shortly after the report released, Sen. Josh Hawley (R-MO) launched an official probe into the company’s AI policies. Additionally, a coalition of 44 state attorneys general wrote to a group of AI companies including Meta, emphasizing the importance of child safety and specifically citing the Reuters report. “We are uniformly revolted by this apparent disregard for children’s emotional well-being,” the letter reads, “and alarmed that AI Assistants are engaging in conduct that appears to be prohibited by our respective criminal laws.”\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nOtway declined to comment on how many of Meta’s AI chatbot users are minors, and wouldn’t say whether the company expects its AI user base to decline as a result of these decisions.\nUpdate 10:35AM PT: This story was updated to note that these are interim changes, and that Meta plans to update its AI safety policies further in the future."}
{"title": "ChatGPT: Everything you need to know about the AI-powered chatbot", "url": "https://techcrunch.com/2025/08/29/chatgpt-everything-to-know-about-the-ai-chatbot/", "source": "TechCrunch (AI)", "published": "2025-08-29T15:46:27+00:00", "text": "ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to supercharge productivity through writing essays and code with short text prompts has evolved into a behemoth with 300 million weekly active users.\n2024 was a big year for OpenAI, from its partnership with Apple for its generative AI offering, Apple Intelligence, the release of GPT-4o with voice capabilities, and the highly-anticipated launch of its text-to-video model Sora.\nOpenAI also faced its share of internal drama, including the notable exits of high-level execs like co-founder and longtime chief scientist Ilya Sutskever and CTO Mira Murati. OpenAI has also been hit with lawsuits from Alden Global Capital-owned newspapers alleging copyright infringement, as well as an injunction from Elon Musk to halt OpenAI’s transition to a for-profit.\nIn 2025, OpenAI is battling the perception that it’s ceding ground in the AI race to Chinese rivals like DeepSeek. The company has been trying to shore up its relationship with Washington as it simultaneously pursues an ambitious data center project, and as it reportedly lays the groundwork for one of the largest funding rounds in history.\nBelow, you’ll find a timeline of ChatGPT product updates and releases, starting with the latest, which we’ve been updating throughout the year. If you have any other questions, check out our ChatGPT FAQ here.\nTo see a list of 2024 updates, go here.\nTimeline of the most recent ChatGPT updates\n- August 2025\n- July 2025\n- June 2025\n- May 2025\n- April 2025\n- March 2025\n- February 2025\n- January 2025\n- ChatGPT FAQs\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nAugust 2025\nOpenAI to strengthen ChatGPT safeguards after teen suicide lawsuit\nOpenAI, facing a lawsuit from the parents of a 16-year-old who died by suicide, said in its blog that it has implemented new safeguards for ChatGPT, including stronger detection of mental health risks and parental control features. The AI company said the updates aim to provide tighter protections around suicide-related conversations and give parents more oversight of their children’s use.\nxAI claims Apple’s App Store practices give OpenAI an unfair advantage\nElon Musk’s AI startup, xAI, filed a federal lawsuit in Texas against Apple and OpenAI, alleging that the two companies colluded to lock up key markets and shut out rivals.\nOpenAI targets India with cheaper monthly ChatGPT subscription\nOpenAI introduced its most affordable subscription plan, ChatGPT Go, in India, priced at 399 rupees per month (approximately $4.57). This move aims to expand OpenAI’s presence in its second-largest market, offering enhanced access to the latest GPT-5 model and additional features.\nChatGPT mobile app hits $2B in revenue, $2.91 earned per install\nSince its May 2023 launch, ChatGPT’s mobile app has amassed $2 billion in global consumer spending, dwarfing competitors like Claude, Copilot, and Grok by roughly 30 times, according to Appfigures. This year alone, the app has generated $1.35 billion, a 673% increase from the same period in 2024, averaging nearly $193 million per month, or 53 times more than its nearest rival, Grok.\nOpenAI keeps multiple GPT models despite GPT-5 launch\nDespite unveiling GPT-5 as a “one-size-fits-all” AI, OpenAI is still offering several legacy AI options, including GPT-4o, GPT-4.1, and o3. Users can choose between new “Auto,” “Fast,” and “Thinking” modes for GPT-5, and paid subscribers regain access to legacy models like GPT-4o and GPT-4.1.\nSam Altman addresses GPT-5 glitches and “chart crime” during Reddit AMA\nOpenAI CEO Sam Altman told Reddit users that GPT-5’s “dumber” behavior at launch was due to a router issue and promised fixes, double rate limits for Plus users, and transparency on which model is answering, while also shrugging off the infamous “chart crime” from the live presentation.\nOpenAI unveils GPT-5, a smarter, task-ready ChatGPT\nOpenAI released GPT-5, a next-gen AI that’s not just smarter but more useful — able to handle tasks like coding apps, managing calendars, and creating research briefs — while automatically figuring out the fastest or most thoughtful way to answer your questions.\nOpenAI offers ChatGPT Enterprise to federal agencies for just $1\nOpenAI is making a major push into federal government workflows, offering ChatGPT Enterprise to agencies for just $1 for the next year. The move comes after the U.S. General Services Administration (GSA) added OpenAI, Google, and Anthropic to its approved AI vendor list, allowing agencies to access these tools through preset contracts without negotiating pricing.\nOpenAI returns to open source with new AI models\nOpenAI unveiled its first open source language models since GPT-2, introducing two new open-weight AI releases: gpt-oss-120b, a high-performance model capable of running on a single Nvidia GPU, and gpt-oss-20b, a lighter model optimized for laptop use. The move comes amid growing competition in the global AI market and a push for more open technology in the U.S. and abroad.\nChatGPT nears 700M weekly users, quadruples growth in a year\nChatGPT’s rapid growth is accelerating. OpenAI said the chatbot was on track to hit 700 million weekly active users in the first week of August, up from 500 million at the end of March. Nick Turley, OpenAI’s VP and head of the ChatGPT app, highlighted the app’s growth on X, noting it has quadrupled in size over the past year.\nJuly 2025\nChatGPT now has study mode\nOpenAI unveiled Study Mode, a new ChatGPT feature designed to promote critical thinking by prompting students to engage with material rather than simply receive answers. The tool is now rolling out to Free, Plus, Pro, and Team users, with availability for Edu subscribers expected in the coming weeks.\nAltman warns that ChatGPT therapy isn’t confidential\nChatGPT users should be cautious when seeking emotional support from AI, as the AI industry lacks safeguards for sensitive conversations, OpenAI CEO Sam Altman said on a recent episode of This Past Weekend w/ Theo Von. Unlike human therapists, AI tools aren’t bound by doctor-patient confidentiality, he noted.\nChatGPT hits 2.5B prompts daily\nChatGPT now receives 2.5 billion prompts daily from users worldwide, including roughly 330 million from the U.S. That’s more than double the volume reported by CEO Sam Altman just eight months ago, highlighting the chatbot’s explosive growth.\nOpenAI launches a general-purpose agent in ChatGPT\nOpenAI has introduced ChatGPT Agent, which completes a wide variety of computer-based tasks on behalf of users and combines several capabilities like Operator and Deep Research, according to the company. OpenAI says the agent can automatically navigate a user’s calendar, draft editable presentations and slideshows, run code, shop online, and handle complex workflows from end to end, all within a secure virtual environment.\nStudy warns of major risks with AI therapy chatbots\nResearchers at Stanford University have observed that therapy chatbots powered by large language models can sometimes stigmatize people with mental health conditions or respond in ways that are inappropriate or could be harmful. While chatbots are “being used as companions, confidants, and therapists,” the study found “significant risks.”\nOpenAI delays releasing its open model again\nCEO Sam Altman said that the company is delaying the release of its open model, which had already been postponed by a month earlier this summer. The ChatGPT maker, which initially planned to release the model around mid-July, has indefinitely postponed its launch to conduct additional safety testing.\nOpenAI is reportedly releasing an AI browser in the coming weeks\nOpenAI plans to release an AI-powered web browser to challenge Alphabet’s Google Chrome. It will keep some user interactions within ChatGPT, rather than directing people to external websites.\nChatGPT is testing a mysterious new feature called “study together”\nSome ChatGPT users have noticed a new feature called “Study Together” appearing in their list of available tools. This is the chatbot’s approach to becoming a more effective educational tool, rather than simply providing answers to prompts. Some people also wonder whether there will be a feature that allows multiple users to join the chat, similar to a study group.\nReferrals from ChatGPT to news sites are rising but not enough to offset search declines\nReferrals from ChatGPT to news publishers are increasing. But this rise is insufficient to offset the decline in clicks as more users now obtain their news directly from AI or AI-powered search results, according to a report by digital market intelligence company Similarweb. Since Google launched its AI Overviews in May 2024, the percentage of news searches that don’t lead to clicks on news websites has increased from 56% to nearly 69% by May 2025.\nJune 2025\nOpenAI uses Google’s AI chips to power its products\nOpenAI has started using Google’s AI chips to power ChatGPT and other products, as reported by Reuters. The ChatGPT maker is one of the biggest buyers of Nvidia’s GPUs, using the AI chips to train models, and this is the first time that OpenAI is using non-Nvidia chips in an important way.\nA new MIT study suggests that ChatGPT might be harming critical thinking skills\nResearchers from MIT’s Media Lab monitored the brain activity of writers in 32 regions. They found that ChatGPT users showed minimal brain engagement and consistently fell short in neural, linguistic, and behavioral aspects. To conduct the test, the lab split 54 participants from the Boston area into three groups, each consisting of individuals ages 18 to 39. The participants were asked to write multiple SAT essays using tools such as OpenAI’s ChatGPT, the Google search engine, or without any tools.\nChatGPT was downloaded 30 million times last month\nThe ChatGPT app for iOS was downloaded 29.6 million times in the last 28 days, while TikTok, Facebook, Instagram, and X were downloaded a total of 32.9 million times during the same period, representing a difference of about 10.6%, according to ZDNET report citing Similarweb’s X post.\nThe energy needed for an average ChatGPT query can power a lightbulb for a couple of minutes\nSam Altman said that the average ChatGPT query uses about one-fifteenth of a teaspoon of water, equivalent to 0.000083 gallons of water, or the energy required to power a lightbulb for a few minutes, per Business Insider. In addition to that, the chatbot requires 0.34 watt-hours of electricity to operate.\nOpenAI has launched o3-pro, an upgraded version of its o3 AI reasoning model\nOpenAI has unveiled o3-pro, an enhanced version of its o3, a reasoning model that the chatGPT maker launched earlier this year. O3-pro is available for ChatGPT and Team users and in the API, while Enterprise and Edu users will get access in the third week of June.\nChatGPT’s conversational voice mode has been upgraded\nOpenAI upgraded ChatGPT’s conversational voice mood for all paid users across different markets and platforms. The startup has launched an update to Advanced Voice that enables users to converse with ChatGPT out loud in a more natural and fluid sound. The feature also helps users translate languages more easily, the comapny said.\nChatGPT has added new features like meeting recording and connectors for Google Drive, Box, and more\nOpenAI’s ChatGPT now offers new funtions for business users, including integrations with various cloud services, meeting recordings, and MCP connection support for connecting to tools for in-depth research. The feature enables ChatGPT to retrieve information across users’ own services to answer their questions. For instance, an analyst could use the company’s slide deck and documents to develop an investment thesis.\nMay 2025\nOpenAI CFO says hardware will drive ChatGPT’s growth\nOpenAI plans to purchase Jony Ive’s devices startup io for $6.4 billion. Sarah Friar, CFO of OpenAI, thinks that the hardware will significantly enhance ChatGPT and broaden OpenAI’s reach to a larger audience in the future.\nOpenAI’s ChatGPT unveils its AI coding agent, Codex\nOpenAI has introduced its AI coding agent, Codex, powered by codex-1, a version of its o3 AI reasoning model designed for software engineering tasks. OpenAI says codex-1 generates more precise and “cleaner” code than o3. The coding agent may take anywhere from one to 30 minutes to complete tasks such as writing simple features, fixing bugs, answering questions about your codebase, and running tests.\nSam Altman aims to make ChatGPT more personalized by tracking every aspect of a person’s life\nSam Altman, the CEO of OpenAI, said during a recent AI event hosted by VC firm Sequoia that he wants ChatGPT to record and remember every detail of a person’s life when one attendee asked about how ChatGPT can become more personalized.\nOpenAI releases its GPT-4.1 and GPT-4.1 mini AI models in ChatGPT\nOpenAI said in a post on X that it has launched its GPT-4.1 and GPT4.1 mini AI models in ChagGPT.\nChatGPT deep research now connects with GitHub (in beta) to answer code-related questions\nOpenAI has launched a new feature for ChatGPT deep research to analyze code repositories on GitHub. The ChatGPT deep research feature is in beta and lets developers connect with GitHub to ask questions about codebases and engineering documents. The connector will soon be available for ChatGPT Plus, Pro, and Team users, with support for Enterprise and Education coming shortly, per an OpenAI spokesperson.\nOpenAI launches a new data residency program in Asia\nAfter introducing a data residency program in Europe in February, OpenAI has now launched a similar program in Asian countries including India, Japan, Singapore, and South Korea. The new program will be accessible to users of ChatGPT Enterprise, ChatGPT Edu, and API. It will help organizations in Asia meet their local data sovereignty requirements when using OpenAI’s products.\nOpenAI to introduce a program to grow AI infrastructure\nOpenAI is unveiling a program called OpenAI for Countries, which aims to develop the necessary local infrastructure to serve international AI clients better. The AI startup will work with governments to assist with increasing data center capacity and customizing OpenAI’s products to meet specific language and local needs. OpenAI for Countries is part of efforts to support the company’s expansion of its AI data center Project Stargate to new locations outside the U.S., per Bloomberg.\nOpenAI promises to make changes to prevent future ChatGPT sycophancy\nOpenAI has announced its plan to make changes to its procedures for updating the AI models that power ChatGPT, following an update that caused the platform to become overly sycophantic for many users.\nApril 2025\nOpenAI clarifies the reason ChatGPT became overly flattering and agreeable\nOpenAI has released a post on the recent sycophancy issues with the default AI model powering ChatGPT, GPT-4o, leading the company to revert an update to the model released last week. CEO Sam Altman acknowledged the issue on Sunday and confirmed two days later that the GPT-4o update was being rolled back. OpenAI is working on “additional fixes” to the model’s personality. Over the weekend, users on social media criticized the new model for making ChatGPT too validating and agreeable. It became a popular meme fast.\nOpenAI is working to fix a “bug” that let minors engage in inappropriate conversations\nAn issue within OpenAI’s ChatGPT enabled the chatbot to create graphic erotic content for accounts registered by users under the age of 18, as demonstrated by TechCrunch’s testing, a fact later confirmed by OpenAI. “Protecting younger users is a top priority, and our Model Spec, which guides model behavior, clearly restricts sensitive content like erotica to narrow contexts such as scientific, historical, or news reporting,” a spokesperson told TechCrunch via email. “In this case, a bug allowed responses outside those guidelines, and we are actively deploying a fix to limit these generations.”\nChatGPT helps users by giving recommendations, showing images, and reviewing products for online shopping\nOpenAI has added a few features to its ChatGPT search, its web search tool in ChatGPT, to give users an improved online shopping experience. The company says people can ask super-specific questions using natural language and receive customized results. The chatbot provides recommendations, images, and reviews of products in various categories such as fashion, beauty, home goods, and electronics.\nOpenAI wants its AI model to access cloud models for assistance\nOpenAI leaders have been talking about allowing the open model to link up with OpenAI’s cloud-hosted models to improve its ability to respond to intricate questions, two sources familiar with the situation told TechCrunch.\nOpenAI aims to make its new “open” AI model the best on the market\nOpenAI is preparing to launch an AI system that will be openly accessible, allowing users to download it for free without any API restrictions. Aidan Clark, OpenAI’s VP of research, is spearheading the development of the open model, which is in the very early stages, sources familiar with the situation told TechCrunch.\nOpenAI’s GPT-4.1 may be less aligned than earlier models\nOpenAI released a new AI model called GPT-4.1 in mid-April. However, multiple independent tests indicate that the model is less reliable than previous OpenAI releases. The company skipped that step — sending safety cards for GPT-4.1 — claiming in a statement to TechCrunch that “GPT-4.1 is not a frontier model, so there won’t be a separate system card released for it.”\nOpenAI’s o3 AI model scored lower than expected on a benchmark\nQuestions have been raised regarding OpenAI’s transparency and procedures for testing models after a difference in benchmark outcomes was detected by first- and third-party benchmark results for the o3 AI model. OpenAI introduced o3 in December, stating that the model could solve approximately 25% of questions on FrontierMath, a difficult math problem set. Epoch AI, the research institute behind FrontierMath, discovered that o3 achieved a score of approximately 10%, which was significantly lower than OpenAI’s top-reported score.\nOpenAI unveils Flex processing for cheaper, slower AI tasks\nOpenAI has launched a new API feature called Flex processing that allows users to use AI models at a lower cost but with slower response times and occasional resource unavailability. Flex processing is available in beta on the o3 and o4-mini reasoning models for non-production tasks like model evaluations, data enrichment, and asynchronous workloads.\nOpenAI’s latest AI models now have a safeguard against biorisks\nOpenAI has rolled out a new system to monitor its AI reasoning models, o3 and o4 mini, for biological and chemical threats. The system is designed to prevent models from giving advice that could potentially lead to harmful attacks, as stated in OpenAI’s safety report.\nOpenAI launches its latest reasoning models, o3 and o4-mini\nOpenAI has released two new reasoning models, o3 and o4 mini, just two days after launc"}
{"title": "Trillion with a ‘T’? That’s a lot of dollars, Nvidia", "url": "https://techcrunch.com/podcast/trillion-with-a-t-thats-a-lot-of-dollars-nvidia/", "source": "TechCrunch (AI)", "published": "2025-08-29T15:43:23+00:00", "text": "Nvidia reported another massive quarter this week with $46.7 billion in revenue, a 56% year-over-year increase driven almost entirely by AI demand. But despite CEO Jensen Huang’s bold prediction of $3 trillion to $4 trillion in global AI infrastructure spending in the next five years, the stock slid as investors questioned how long this kind of growth can last.\nToday on Equity, Kirsten Korosec, Max Zeff, and Anthony Ha dive into Nvidia’s earnings and what the market’s response reveals about investor confidence in the AI boom’s longevity.\nListen to the full episode to hear:\n- Who made the cut for the 2025 Startup Battlefield 200, and how Equity’s hitting the stage at this year’s TechCrunch Disrupt\n- OpenAI and Anthropic’s rare AI safety testing collaboration, despite recent moves to cut each other off from their APIs\n- RoboMart’s new autonomous delivery robot, which could challenge Uber Eats with $3 flat fees\n- Why the U.S. government’s potential 10% stake in Intel might not be the salvation the chipmaker needs\n- How venture capital firms like a16z are flooding Washington, D.C. with lobbying dollars, outspending entire industry groups\nAs always, Equity will be back for you next week, so don’t miss it!\nEquity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.\nSubscribe to us on Apple Podcasts, Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod."}
{"title": "Host an event beyond the main event: Apply to host a Side Event at TechCrunch Disrupt 2025", "url": "https://techcrunch.com/2025/08/29/host-an-event-beyond-the-main-event-apply-to-host-a-side-event-at-techcrunch-disrupt-2025/", "source": "TechCrunch (AI)", "published": "2025-08-29T15:00:00+00:00", "text": "TechCrunch Disrupt 2025 is where over 10,000 founders, VCs, and tech innovators collide on October 27-29 at San Francisco’s Moscone West. But the conversations don’t end when the venue doors close. They spill across San Francisco — throughout the week — into the rooms, rooftops, and lounges where the real connections happen.\nThat’s where Side Events come in\nHost your own gathering during Disrupt Week, October 25-31. From a salon-style dinner, to a hands-on workshop, to a pitch-off in a packed bar. The format is yours to own.\nWe’ll amplify and promote your event so that it reaches the people who matter most.\nThere’s no cost to apply. Just bring your idea, and we’ll help bring the attendees. Learn more about hosting a Side Event here.\nDon’t just attend Disrupt — lead\nApply now to put your brand at the center of the Disrupt conversation. And don’t forget to register for your TechCrunch Disrupt 2025 pass before prices go up in September."}
{"title": "Billionaire Ambani taps Google, Meta to build India’s AI backbone", "url": "https://techcrunch.com/2025/08/29/billionaire-ambani-taps-google-meta-to-build-indias-ai-backbone/", "source": "TechCrunch (AI)", "published": "2025-08-29T13:19:10+00:00", "text": "Mukesh Ambani, India’s richest man and chairman of Reliance Industries, has unveiled an ambitious plan to build the country’s AI backbone through a new subsidiary — starting with strategic partnerships with Google Cloud and Meta.\nAt the company’s 48th annual general meeting on Friday, Ambani launched a new venture called Reliance Intelligence, a subsidiary of Reliance Industries. The new venture aims to create a national-scale AI infrastructure, including enterprise tools and services for a variety of sectors. The move comes as India looks to catch up in the global AI race long dominated by the U.S. and China.\n“Reliance Intelligence will create a home for world-class researchers, engineers, designers, and product builders, combining the speed of research with the rigor of engineering,” Ambani said in his keynote, “so that ideas become innovations and applications, providing solutions to India and the world.”\nTo kick things off, Reliance has partnered with Google — one of its major tech partners — to build a dedicated AI cloud infrastructure in India. The network will start with a major data center in Jamnagar, a city in the western state of Gujarat.\nThe dedicated cloud region will enable Reliance to offer AI-focused services to businesses of all sizes, developers, and government bodies, utilizing Jio’s network and its own energy assets to support large-scale deployments, the companies said in their joint statement.\n“As Reliance’s largest public cloud partner, Google Cloud is not only powering the company’s mission-critical workloads, but we are also innovating with you on advanced AI initiatives,” Google CEO Sundar Pichai said in a video message during the company’s virtual AGM. “This is only the beginning.”\nGoogle did not immediately respond to a query about the financial terms of the partnership.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nReliance has also announced a joint venture with Meta, another of its major tech investors, to build and scale enterprise AI solutions for customers in India and select international markets. Under the agreement, Reliance and Meta have committed a combined investment of ₹8.55 billion (approximately $100 million) under a 70/30 ownership split, respectively.\nThe partnership will offer Meta’s Llama-based enterprise AI platform-as-a-service, allowing businesses to customize, deploy, and manage generative AI models for use cases across sales, marketing, IT, customer service, and finance. The joint venture will also provide a suite of pre-configured AI solutions, the companies said.\nReliance’s collaboration comes just weeks after Meta restructured its AI ventures into a new Superintelligence Labs, fueled by an expensive string of top AI hires. (Meta is reportedly pausing the hiring spree after concern from shareholders.)\n“Through this joint venture, we’re putting Meta’s Llama models into real-world use,” Meta CEO Mark Zuckerberg said in a prepared statement.\nThe transaction is subject to customary regulatory approvals and is expected to close in the fourth quarter of 2025.\nReliance is planning to expand beyond India and take its flagship subsidiary Reliance Jio Platforms to international markets, Ambani said. Ambani also revealed that Jio aims to file for an initial public offering in the first half of 2026, following much anticipation and initial delays.\nReliance is also reportedly eyeing a partnership with OpenAI, which recently introduced its sub-$5 ChatGPT subscription in India and announced its plans to set up an office in New Delhi later this year. The details of the partnership are likely to be announced during Sam Altman’s upcoming visit to India next month, two people familiar with the matter told TechCrunch.\nReliance and OpenAI did not immediately respond to a request for comment.\nEarlier this year, Reliance’s arch-rival Bharti Airtel, the country’s second-largest telco after Jio, partnered with Perplexity to offer more than 360 million Airtel subscribers access to Perplexity Pro for 12 months.\nReliance has already partnered with Microsoft to offer its Azure cloud platform to Indian enterprises. The company also offers JioAICloud, a consumer-focused service that provides 100GB of free storage. The consumer cloud service has been used by 40 million users and is updated with voice search support and an AI Create Hub to turn photos into AI-powered reels, collages, and promo videos, the company announced at its annual general meeting.\nReliance also showcased its AI-based smart glasses, JioFrames, as its answer to Snap’s Spectacles and Ray-Ban Meta glasses. Similarly, the company is integrating AI into its streaming platform, JioHotstar, which has attracted over 600 million users and 300 million paying subscribers in the three months since its launch in February.\nThe new AI features include “Riya” voice assistant and content translation into Indian languages using AI-voice cloning and lip-syncing tech."}
{"title": "Vocal Image is using AI to help people communicate better", "url": "https://techcrunch.com/2025/08/29/vocal-image-is-using-ai-to-help-people-communicate-better/", "source": "TechCrunch (AI)", "published": "2025-08-29T08:34:00+00:00", "text": "With 4 million app downloads, Estonia-based startup Vocal Image aims to help people improve their voice and communication skills with AI-powered coaching. But out of its 160,000 active users, it may be its CEO, Nick Lahoika, who best embodies its mission.\nLahoika was born in Belarus, didn’t speak English until his relocation to Estonia, and once struggled with speaking anxiety. Yet, he went on to “win a lot of pitch competitions” on behalf of the voice coaching startup, which was inspired by his journey, he told TechCrunch.\n“When I was at school, I was a little bit bullied for unclear diction,” Lahoika said. In his early twenties, as a young, insecure founder, he met a vocal coach, Maryna “Rusia” Shukiurava, who taught him that voice and communication could be trained.\nTo help others, they started a YouTube channel that eventually morphed into Vocal Image, which positions its subscription-based app as an affordable alternative to one-on-one coaching you can use at home. “You can make strange movements, strange sounds […] and feel safe,” Lahoika said.\nWith an interactive library that includes tongue twisters, breathing exercises, and advice on gestures, Vocal Image is also leaning more and more into AI to give automated feedback and personalized tips, thanks in big part to the addition of co-founder and CTO Mikalai Karaliou, Lahoika said.\nThese guided journeys mostly revolve around work-related goals such as enhancing professional or leadership skills, and developing public speaking or presentation abilities. But Vocal Image also supports people who simply want to increase their self-confidence, as well as LGBTQ people, whose rights Shukiurava had been supporting in Belarus.\nWhile the trio is from Belarus, they were among the many Belarusian founders who left their home country after protests failed to oust President Alexander Lukashenko and were met with brutal repression. Lahoika picked Estonia for its business environment, which has so far proven favorable for the startup.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nShortly after relocating to Tallinn, Vocal Image joined local accelerator Startup Wise Guys, which considers the startup as one of its “success stories” due to its fast growth. According to Lahoika, the startup subsequently reached $6.5 million in annual recurring revenue (ARR) on less than $1 million in pre-seed funding.\nMore recently, the startup raised a $3.6 million seed round led by French edtech VC firm Educapital, with participation from Specialist VC out of Estonia and Generations Fund out of Germany, TechCrunch learned exclusively.\nAs of August, the startup now claims $12 million of ARR and some 50,000 paid users, Lahoika said. With a team of 20 people, including a majority of Belarusian exiles, Vocal Image now plans to grow its development team and deploy more localizations (in addition to English, Spanish, German, French, Ukrainian, and Russian).\nThis funding comes not long after the startup was picked by Hugging Face, Meta, and Scaleway as one of the five winners of their European AI Startup Program, but also at a time when it is facing increased competition. For instance, edtech company Headway recently added an AI-powered speech trainer to its social skills app, Skillsta. But Vocal Image can count on its own GDPR-compliant AI trove.\nWith some 35,000 recordings a day, Vocal Image has amassed more than 1 million real-voice samples. Even better, these recordings are labeled by the community through Voice Rating, a collaborative feature that lets users decide whether others sound “confident” or “childlike.”\nThis is the kind of dataset that apps like Vocal Image sorely need in order to improve their accuracy. It could also help AI startups fine-tune their artificial voices, creating further tailwinds for the startup beyond its B2C roots."}
{"title": "Trump administration’s deal is structured to prevent Intel from selling foundry unit", "url": "https://techcrunch.com/2025/08/28/trump-administrations-deal-is-structured-to-prevent-intel-from-selling-foundry-unit/", "source": "TechCrunch (AI)", "published": "2025-08-28T21:56:27+00:00", "text": "The Trump administration seems intent on controlling Intel’s ability to make key business decisions around its floundering foundry business unit.\nAccording to reporting from the Financial Times, at a Deutsche Bank conference on Thursday, Intel’s CFO David Zinsner shared new details about the company’s recent deal with the Trump administration, which gave the U.S. government a 10% equity stake.\nThe deal was structured in a way to penalize Intel if it spins out its foundry business unit, which makes custom chips for outside customers, within the next few years.\nLast week’s deal included a five-year warrant that would allow the U.S. government to take an additional 5% of Intel, at $20 a share, if the company held less than 51% equity in its foundry business. Zinsner said he expects that warrant to expire.\n“I think from the government’s perspective, they were aligned with that; they didn’t want to see us take the business and spin it off or sell it to somebody,” he said.\nZinsner added that the company received $5.7 billion in cash on Wednesday, as a result of last week’s deal, according to Reuters. (That cash comes from the remaining grants previously awarded, but not yet paid, to Intel under the U.S. CHIPS and Science Act.)\nWhite House press secretary Karoline Leavitt told reporters today that the deal was still being ironed out.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nIntel declined to comment on the deal beyond Zinsner’s remarks.\nThis deal structure is clearly a testament to the Trump administration’s desire to bring more chip manufacturing to the United States as many players in the industry turn to Taiwan Semiconductor Manufacturing Company’s offshore manufacturing instead.\nBut this warrant also forces Intel to keep a business unit that is losing money. Intel Foundry reported an operating income loss of $3.1 billion during the second quarter and has been a source of strife for the semiconductor business.\nThere have been calls from analysts, board members, and investors alike to spin out the struggling foundry unit, which looked like it might actually happen last fall, before Intel Foundry’s architect, former CEO Pat Gelsinger, retired suddenly in December."}
{"title": "Anthropic users face a new choice – opt out or share your chats for AI training", "url": "https://techcrunch.com/2025/08/28/anthropic-users-face-a-new-choice-opt-out-or-share-your-data-for-ai-training/", "source": "TechCrunch (AI)", "published": "2025-08-28T20:43:12+00:00", "text": "Anthropic is making some big changes to how it handles user data, requiring all Claude users to decide by September 28 whether they want their conversations used to train AI models. While the company directed us to its blog post on the policy changes when asked about what prompted the move, we’ve formed some theories of our own.\nBut first, what’s changing: Previously, Anthropic didn’t use consumer chat data for model training. Now, the company wants to train its AI systems on user conversations and coding sessions, and it said it’s extending data retention to five years for those who don’t opt out.\nThat is a massive update. Previously, users of Anthropic’s consumer products were told that their prompts and conversation outputs would be automatically deleted from Anthropic’s back end within 30 days “unless legally or policy‑required to keep them longer” or their input was flagged as violating its policies, in which case a user’s inputs and outputs might be retained for up to two years.\nBy consumer, we mean the new policies apply to Claude Free, Pro, and Max users, including those using Claude Code. Business customers using Claude Gov, Claude for Work, Claude for Education, or API access will be unaffected, which is how OpenAI similarly protects enterprise customers from data training policies.\nSo why is this happening? In that post about the update, Anthropic frames the changes around user choice, saying that by not opting out, users will “help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations.” Users will “also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.”\nIn short, help us help you. But the full truth is probably a little less selfless.\nLike every other large language model company, Anthropic needs data more than it needs people to have fuzzy feelings about its brand. Training AI models requires vast amounts of high-quality conversational data, and accessing millions of Claude interactions should provide exactly the kind of real-world content that can improve Anthropic’s competitive positioning against rivals like OpenAI and Google.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nBeyond the competitive pressures of AI development, the changes would also seem to reflect broader industry shifts in data policies, as companies like Anthropic and OpenAI face increasing scrutiny over their data retention practices. OpenAI, for instance, is currently fighting a court order that forces the company to retain all consumer ChatGPT conversations indefinitely, including deleted chats, because of a lawsuit filed by The New York Times and other publishers.\nIn June, OpenAI COO Brad Lightcap called this “a sweeping and unnecessary demand” that “fundamentally conflicts with the privacy commitments we have made to our users.” The court order affects ChatGPT Free, Plus, Pro, and Team users, though enterprise customers and those with Zero Data Retention agreements are still protected.\nWhat’s alarming is how much confusion all of these changing usage policies are creating for users, many of whom remain oblivious to them.\nIn fairness, everything is moving quickly now, so as the tech changes, privacy policies are bound to change. But many of these changes are fairly sweeping and mentioned only fleetingly amid the companies’ other news. (You wouldn’t think Tuesday’s policy changes for Anthropic users were very big news based on where the company placed this update on its press page.)\nBut many users don’t realize the guidelines to which they’ve agreed have changed because the design practically guarantees it. Most ChatGPT users keep clicking on “delete” toggles that aren’t technically deleting anything. Meanwhile, Anthropic’s implementation of its new policy follows a familiar pattern.\nHow so? New users will choose their preference during signup, but existing users face a pop-up with “Updates to Consumer Terms and Policies” in large text and a prominent black “Accept” button with a much tinier toggle switch for training permissions below in smaller print — and automatically set to “On.”\nAs observed earlier today by The Verge, the design raises concerns that users might quickly click “Accept” without noticing they’re agreeing to data sharing.\nMeanwhile, the stakes for user awareness couldn’t be higher. Privacy experts have long warned that the complexity surrounding AI makes meaningful user consent nearly unattainable. Under the Biden administration, the Federal Trade Commission even stepped in, warning that AI companies risk enforcement action if they engage in “surreptitiously changing its terms of service or privacy policy, or burying a disclosure behind hyperlinks, in legalese, or in fine print.”\nWhether the commission — now operating with just three of its five commissioners — still has its eye on these practices today is an open question, one we’ve put directly to the FTC."}
{"title": "AI or not, Will Smith’s crowd video is fresh cringe", "url": "https://techcrunch.com/2025/08/28/ai-or-not-will-smiths-crowd-video-is-fresh-cringe-2/", "source": "TechCrunch (AI)", "published": "2025-08-28T20:02:52+00:00", "text": "Will Smith posted a video on social media that shows oceans of fans cheering him on during his recent European tour.\n“My favorite part of the tour is seeing you all up close,” the caption says. “Thank you for seeing me too.”\nIn these thousands-deep crowds, some fans are holding up signs espousing their love for Smith, with one even saying that his music helped them survive cancer.\nBut the video gives off an odd aura — it looks believably real at first glance, until you look closer and find digitally mangled faces, nonsensical finger placements, and oddly augmented features across the series of clips.\nThe video looks strange enough that fans responded with accusations that the crowd footage was created using AI. It’s bad news for Smith, who’s already suffered reputational damage after “the slap.” If he were using AI to make his concerts look more impressive, or even spinning up stories of fans using his music to cope with cancer treatment, that would be pretty indefensible.\nThese fans aren’t fake, though — or at least, that’s our best guess. (There’s not a reliable way to determine whether content was created using AI, which has made the current online landscape a nightmare of misinformation.)\nAs tech blogger Andy Baio pointed out, Will Smith has posted photos and videos throughout his tour that show some of the same fans and signs depicted in the questionable video.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nThere’s nothing about these older posts that indicates that the photos and videos are synthetic, yet when they’re depicted in this new video, they look like they’ve been generated using AI. It seems like Smith’s team has collaged real footage with AI-generated videos that use real crowd photos as source images, which makes the video even more difficult to interpret.\nBut social media audiences will not take the time to scroll through past Will Smith posts, find evidence that a fan really did listen to his music during cancer treatment, and give him the benefit of the doubt. What fans will take away from the post is that Smith is posting fake videos of his fans, which is deeply cringe, even if the reality is a bit less egregious.\nIt’s bad timing for Smith, too, that YouTube had recently begun testing a feature that would use “traditional machine learning technology to unblur, denoise, and improve clarity” on some Shorts posts — these edits made Smith’s YouTube Short look even more fake than the videos on other platforms.\nYouTube’s creator liaison Rene Ritchie has since shared that the platform will soon allow creators to opt out of this feature, which has proven unpopular thus far.\nYou could make the argument that Will Smith has not duped his fans — that his team simply used AI to generate footage from photographs to create a more visually gripping social media post and that this practice could be compared to other forms of video editing.\nFans don’t see it this way, though. The public is more resistant to generative AI technology than existing creative tools, like autotune or Photoshop. But even in those cases, many fans remain turned off by artists who rely on these tools in ways that feel untruthful.\nIf a fan buys tickets to see a pop star, but it turns out that his recordings only sound good because his terrible voice has been autotuned, then they’d feel duped. It’s like photographing a model to advertise a facial moisturizer, only to edit acne off the model’s face.\nOnce an artist breaks their audience’s trust, it’s hard to win it back — even if you’re the Fresh Prince of Bel-Air."}
{"title": "MathGPT.ai, the ‘cheat-proof’ tutor and teaching assistant, expands to over 50 institutions", "url": "https://techcrunch.com/2025/08/28/mathgpt-the-cheat-proof-ai-tutor-and-teaching-assistant-expands-to-over-50-institutions/", "source": "TechCrunch (AI)", "published": "2025-08-28T16:00:00+00:00", "text": "As AI becomes more prevalent in the classroom — where students use it to complete assignments and teachers are uncertain about how to address it — an AI platform called MathGPT.ai launched last year with the goal of providing an “anti-cheating” tutor to college students and a teaching assistant to professors.\nFollowing a successful pilot program at 30 colleges and universities in the U.S., MathGPT.ai is preparing to nearly double its availability this fall, with hundreds of instructors planning to incorporate the tool. Schools implementing MathGPT.ai in their classrooms include Penn State University, Tufts University, and Liberty University, among others.\nThe most notable aspect of the platform is that its AI chatbot is trained to never directly give the answer, but instead ask students questions and provide support, much like a human tutor would. This technique, known as Socratic questioning, encourages students to think critically rather than simply memorizing answers.\nFor instructors, MathGPT.ai serves as a teaching assistant, generating questions and schoolwork based on uploaded textbooks and learning materials, as well as offering auto-grading capabilities and additional AI features.\nMathGPT.ai supports college-level math, including Algebra, Calculus, Trigonometry, and more.\nIn addition to the expansion, MathGPT.ai launched an upgraded version of its platform, introducing new features that give professors more control over how their students use the tools.\nThe main feature that sets MathGPT.ai apart from other AI companies is its instructor-centric approach. Recently, the platform has become even more focused on instructors’ needs. For example, instructors can now determine when students are allowed to interact with the chatbot. They can specify whether the AI should provide tutoring support for specific assignments while encouraging students to work independently on others.\nAnother new feature allows professors to set the number of attempts a student has to answer a question correctly. To promote a low-pressure learning environment, MathGPT.ai has also introduced unlimited practice questions for students. These questions don’t affect their score, allowing students to test their knowledge without stressing about grades.\nAdditional features that MathGPT.ai offers to instructors include an optional requirement for students to upload images of their work. This enables professors to review submissions and verify the authenticity of the students’ work.\nOther recent updates include integrations with the three largest Learning Management Systems (LMS): Canvas, Blackboard, and Brightspace. It also added screen reader compatibility and an audio mode, making it more accessible to individuals with disabilities. The platform already offers closed captions for its summarized video lessons, which are notably AI-narrated to sound like historical figures like Ben Franklin and Albert Einstein.\nThe company claims it complies with the Americans with Disabilities Act (ADA).\nWhile chatbots like Meta AI, Character.AI, and ChatGPT have faced criticism for inappropriate interactions with young users, MathGPT.ai says it has strict guardrails in place to ensure a safe learning environment.\n“It will not have discussions with you about your girlfriend, boyfriend, or the meaning of life,” Peter Relan, the chairman of MathGPT.ai, told TechCrunch. “It will simply not engage. Because these freestanding chatbots will go in that direction, right? We are not here to entertain those kinds of conversations.” (Relan helped incubate Got It AI and was an early Discord investor.)\nIt’s important to note that, like any chatbot, MathGPT.ai’s assistant still has the potential to produce inaccurate information. The chatbot has a disclosure at the bottom that warns the AI may make mistakes. Users can report the responses to the company if they believe the questions were answered incorrectly.\n“If you find a mistake, we will reward you with a gift card to tell us what it is. Year one, there were five [hallucinations]. Year two, there was one. So far [this year], none. So we take it very seriously,” Relan said, adding that MathGPT.ai has a team of human annotators to double-check every piece of work, textbook, and all other content to ensure “100% accuracy.”\nTo continue its growth, the company plans to develop a mobile app in the future and expand to more subjects, such as chemistry, economics, and accounting.\nMathGPT.ai offers a free option, as well as a $25 per student per course option. The paid option includes several benefits, such as unlimited AI assignments and LMS integration."}
{"title": "Investors are loving Lovable", "url": "https://techcrunch.com/2025/08/28/investors-are-loving-lovable/", "source": "TechCrunch (AI)", "published": "2025-08-28T14:04:06+00:00", "text": "Investors are clamoring to get onto Swedish vibe-coding startup Lovable’s cap table, making unsolicited offers of investment that value the company at more than $4 billion, reports Financial Times.\nLovable CEO Anton Osika isn’t currently engaging with the flurry of inbound interest, the Times says, which comes a few weeks after the startup announced a $200 million round at a $1.8 billion valuation in a deal led by Accel.\nA Lovable spokesperson told TechCrunch that the company isn’t fundraising now.\nLovable has grown quickly over its short lifespan. In July, the startup said its annual recurring revenue had surpassed $100 million with more than 10 million projects built using the platform.\nThe astounding trajectory of Europe’s hottest unicorn comes just nine months after Lovable launched and comes on the heels of investor interest in vibe-coding startups. Cursor-maker Anysphere raised $900 million in May, more than tripling its valuation to $9 billion.\nGot a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at rebecca.bellan@techcrunch.com and Maxwell Zeff at maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at @rebeccabellan.491 and @mzeff.88."}
{"title": "AI hires or human hustle? Inside the next frontier of startup operations at TechCrunch Disrupt 2025", "url": "https://techcrunch.com/2025/08/28/ai-hires-or-human-hustle-inside-the-next-frontier-of-startup-operations-at-techcrunch-disrupt-2025/", "source": "TechCrunch (AI)", "published": "2025-08-28T14:00:00+00:00", "text": "What happens when your first 10 hires aren’t people at all? At TechCrunch Disrupt 2025, happening October 27–29 at San Francisco’s Moscone West, we’re digging into the new wave of startups replacing or augmenting early employees with AI agents. Think outbound sales, billing, and customer support — automated from day one.\nThis panel, hosted on the Builders Stage, features a mix of technical founders and seasoned operators who are actually doing it, debating where the line between human and machine should be drawn — and how far is too far.\nMeet the speakers\nCaleb Peffer, founder and CEO of Firecrawl, is helping over 350,000 developers (and companies like Shopify and Zapier) plug AI directly into the live web. His dev-first platform is already reshaping how AI agents interact with the internet and scale with clean data.\nJaspar Carmichael-Jack, founder and CEO of Artisan, made waves with his “Stop Hiring Humans” campaign — and he’s serious. His company raised $35 million to build AI employees, starting with sales. Expect bold insights on replacing go-to-market teams with code.\nSarah Franklin, CEO of Lattice and former Salesforce president and CMO, brings hard-won wisdom on scaling companies with impact. She’s built and led real teams at the highest level and knows exactly where AI helps — and where it hurts.\nWhy this session matters\nWhether you’re already embedding AI into your stack or just testing prompts on your product team, this conversation is about more than hype. It’s about getting real on ROI, trust, team dynamics, and what it means to build a business that moves faster than ever with fewer human hands.\nReady to find your edge?\nThis session is just one of hundreds featured across five industry stages, plus breakouts and roundtables. Grab your pass to Disrupt 2025 before prices jump in mid-September. Get your ticket now."}
{"title": "How a 16-year-old company is easing small businesses into AI", "url": "https://techcrunch.com/2025/08/28/how-a-16-year-old-company-is-easing-small-businesses-into-ai/", "source": "TechCrunch (AI)", "published": "2025-08-28T12:30:00+00:00", "text": "Amid all the “is this a bubble?” talk about artificial intelligence, the supply chain and logistics industries have become breeding grounds for seemingly genuine uses of the technology. Flexport, Uber Freight, and dozens of startups are developing different applications and winning blue-chip customers.\nBut while AI helps Fortune 500s pad their bottom line (and justify the next layoff to Wall Street), the right use of the tech is proving useful to smaller businesses.\nNetstock, an inventory management software company founded in 2009, is working on just that. It recently rolled out a generative AI-powered tool called the “Opportunity Engine” that slots into its existing customer dashboard. The tool pulls info from a customer’s Enterprise Resource Planning software and uses that information to make regular, real-time recommendations.\nNetstock claims the tool is saving those businesses thousands. On Thursday, the company announced it has served up 1 million recommendations to date, and that 75% of its customers have received an Opportunity Engine suggestion valued at $50,000 or more.\nWhile tantalizing, one of those customers — Bargreen Ellingson, a family-run 65-year-old restaurant supply company — was initially apprehensive about using an artificial intelligence product.\n“Old family companies don’t trust blind change a lot,” chief innovation officer Jacob Moody told TechCrunch. “I could not have gone into our warehouse and said, ‘Hey, this black box is going to start managing.’”\nInstead, Moody pitched Netstock’s AI internally as a tool that warehouse managers could “either choose to use, or not use” — a process he describes as “eagerly, but cautiously dipping our toes” into AI.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nMoody says it’s helping avoid mistakes, in part because it’s sifting through myriad reports his staff uses to make inventory decisions. He acknowledged the AI summaries of this info are not 100% accurate, but said it “helps create signals from the noise” quickly, especially during off-hours.\nThe “more profound” change Moody noticed is the software made some of Bargreen Ellingson’s less-senior warehouse staff “more effective.”\nHe highlighted an employee in one of Bargreen’s 25 warehouses who has worked there for two years. The employee has a high school diploma but no college degree. Training this employee to understand all of the inventory management tools and the forecasting information Bargreen uses to plan inventory levels will take time, he said.\n“But he knows our customers, he knows what he’s putting on the truck every day, so for him, he can look at the system and have this prosaic AI-driven insight and very quickly understand whether it makes sense or doesn’t make sense,” he said. “So he feels empowered.”\nNetstock co-founder Barry Kukkuk told TechCrunch that he understands the hesitancy around new technologies — especially because so many products are essentially mediocre chatbots attached to existing software.\nHe attributes the early success of Netstock’s Opportunity Engine to a few things. The company has more than a decade’s worth of data from working with retailers, distributors, and light manufacturers. That data is tightly protected to adhere to ISO frameworks, but it’s what powers the models that make the recommendations. (He said Netstock is using a combination of AI tech from the open source community and private companies.)\nEach recommendation can be rated with a thumbs up or thumbs down, but the models also get reinforced by whether the customer takes the suggested action or not.\nWhile that kind of reinforcement learning can lead to weird, sometimes harmful results when applied to things like social media, Kukkuk said he’s chasing different incentives.\n“I don’t really care about eyeballs, you know?” he said. “Facebook and Instagram care about eyeballs, so they want you to look at their stuff. We care about: ‘what is the outcome for the customer?’”\nKukkuk’s wary of expanding those interactions due to the limitations of current generative AI tech. While it might make sense for a customer to converse with Netstock’s AI about why a recommendation is or isn’t useful, Kukkuk said that could ultimately lead to a breakdown in accuracy.\n“It’s a tightrope to walk, because the more freedom you give the users, the more freedom you give a large language model to start hallucinating stuff,” he said.\nThis explains the Opportunity Engine’s placement in Netstock’s typical customer dashboard. The suggestions are prominent, but easily dismissed. Google Docs cramming 20 AI features down a user’s throat, this is not.\nMoody said he appreciated that the AI isn’t in-your-face.\n“We’re not letting the AI engine make any inventory decisions that a human hasn’t looked at and screened and said, ‘Yes, I agree with that,’” he said. “If and when we ever get to a point where they agree with 90% of the stuff that it’s suggesting, maybe we’ll take the next step and say ‘we’ll give you control now.’ But we’re not there yet.”\nIt’s a promising start at a time when many enterprise deployments of generative AI seem to go nowhere.\nBut if the tech gets better, Moody said he’s nevertheless worried about the implications.\n“Personally, I’m afraid of what this means. I think there’s going to be a lot of change, and none of us is really sure what that’s going to look like at Bargreen,” he said. It could lead to there being fewer data science experts on staff, he suggested. But even if that means moving those employees out of the warehouse and into the corporate office, he said preserving knowledge is important.\nBargreen needs people who “deeply understand the theory and the philosophy and can rationalize how and why Netstock is making certain recommendations,” and to “make sure that we are not blindly going down” the wrong path, he said."}
{"title": "Maisa AI gets $25M to fix enterprise AI’s 95% failure rate", "url": "https://techcrunch.com/2025/08/27/maisa-ai-gets-25m-to-fix-enterprise-ais-95-failure-rate/", "source": "TechCrunch (AI)", "published": "2025-08-28T05:00:00+00:00", "text": "A staggering 95% of generative AI pilots at companies are failing, according to a recent report published by MIT’s NANDA initiative. But rather than giving up on the technology altogether, the most advanced organizations are experimenting with agentic AI systems that can learn and be supervised.\nThat’s where Maisa AI comes in. The year-old startup has built its entire approach around the premise that enterprise automation requires accountable AI agents, not opaque black boxes. With a new, $25 million seed round led by European VC firm Creandum, it has now launched Maisa Studio, a model-agnostic self-serve platform that helps users deploy digital workers that can be trained with natural language.\nWhile that might sound familiar — reminiscent of so-called vibe-coding platforms like Cursor and the Creandum-backed Lovable — Maisa argues that its approach is fundamentally different. “Instead of using AI to build the responses, we use AI to build the process that needs to be executed to get to the response — what we call ‘chain-of-work,’” Maisa CEO David Villalón told TechCrunch.\nThe principal architect behind this process is Maisa’s co-founder and chief scientific officer, Manuel Romero, who had previously worked with Villalón at Spanish AI startup Clibrain. In 2024, the duo teamed up to build a solution to hallucinations after seeing firsthand that “you could not rely on AI,” Villalón said.\nThe pair isn’t skeptical about AI, but they think it won’t be feasible for humans to review “three months of work done in five minutes.” To address this, Maisa employs a system called HALP (human-augmented LLM processing). This custom method works like students at the blackboard — it asks users about their needs while the digital workers outline each step they will follow.\nThe startup also developed the Knowledge Processing Unit (KPU), a deterministic system designed to limit hallucinations. While Maisa started out from this technical challenge rather than a use case, it soon found that its bet on trustworthiness and accountability resonated with companies hoping to apply AI to critical tasks. For instance, clients that currently use Maisa in production include a large bank, as well as companies in the car-manufacturing and energy sectors.\nBy serving these enterprise clients, Maisa hopes to position itself as a more advanced form of robotic process automation (RPA) that unlocks productivity gains without requiring companies to rely on rigid predefined rules or extensive manual programming. To meet their needs, the startup also offers them either deployment in its secure cloud or through on-premise deployment.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nThis enterprise-first approach means Maisa’s customer base is still very small compared to the millions flocking to freemium vibe-coding platforms. But as these platforms are now exploring how to win enterprise customers, Maisa is moving in the opposite direction with Maisa Studio, which is designed to grow its customer funnel and ease adoption.\nThe startup also plans to expand with existing customers that have operations in multiple countries. With dual headquarters in Valencia and San Francisco, Maisa already has a foothold in the U.S., as reflected in its cap table; its $5 million pre-seed round last December was led by the San Francisco-based venture firms NFX and Village Global.\nIn addition, TechCrunch learned exclusively that U.S. firm Forgepoint Capital International participated in this new round via its European joint venture with Spanish bank Banco Santander, highlighting its appeal for regulated sectors.\nFocusing on complex use cases demanding accountability from nontechnical users could be a differentiator for Maisa, whose competitors include CrewAI and many other AI-powered, business-focused workflow automation products. In a LinkedIn post, Villalón highlighted this “AI framework gold rush,” warning that the “quick start” becomes a long nightmare when you need reliability, auditability, or the ability to fix what went wrong.”\nDoubling down on its goal to help AI scale, Maisa plans to use its funding to grow from 35 to as many as 65 people by the first quarter of 2026 in order to meet demand. Starting in the last quarter of this year, the startup anticipates rapid growth as it begins serving its waitlist. “We are going to show the market that there is a company that is delivering what has been promised, and that it’s working,” Villalón said."}
{"title": "Nvidia reports record sales as the AI boom continues", "url": "https://techcrunch.com/2025/08/27/nvidia-reports-record-sales-as-the-ai-boom-continues/", "source": "TechCrunch (AI)", "published": "2025-08-27T21:18:39+00:00", "text": "Nvidia, the world’s most valuable company, reported another quarter of sustained sales growth in its earnings statement Wednesday, with $46.7 billion in revenue, a 56% increase compared to the same period last year. That growth was largely fueled by AI-dominated data center business, which saw a 56% year-over-year increase in revenue.\nNvidia also saw its net income grow substantially since last year. The company reported a net income of $26.4 billion in the second quarter, a 59% spike since the same period last year.\nAll told, the company brought in $41.1 billion in revenue from data center sales in the quarter, suggesting that AI companies’ demand for cutting-edge GPUs continues to grow. The company’s most advanced generation of chips, Blackwell, accounted for $27 billion of those sales.\n“Blackwell is the AI platform the world has been waiting for,” said CEO Jensen Huang in a statement accompanying the release. “The AI race is on, and Blackwell is the platform at its center.”\nHuang said that the company expects to see $3 to 4 trillion in AI infrastructure spending by the end of the decade. “$3 to 4 trillion is fairly sensible for the next five years,” he told one analyst.\nThe company made particular note of its role in the launch of OpenAI’s open source gpt-oss models earlier this month, which involved processing “1.5 million tokens per second on a single Nvidia Blackwell GB200 NVL72 rack-scale system.”\nThe earnings also gave a look at Nvidia’s ongoing struggle to sell its chips in Chinese markets. The company reported no sales of its China-focused H20 chip to Chinese customers in the past quarter; Nvidia did report $650 million worth of H20 chips had been sold to a customer outside China.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nThe United States has long restricted sales of advanced GPUs to Chinese customers — but the geopolitical situation has changed significantly under President Trump. The company is now permitted to sell chips to China as long as it pays a 15% export tax to the U.S. Treasury, as a result of an unconventional arrangement that legal scholars have described as an unconstitutional abuse of power.\nOn the earnings call, Nvidia CFO Colette Kress made clear that the lack of shipment was a result of uncertainty around the arrangement, which has not been officially codified into a federal regulation. “While a select number of our China-based customers have received licenses over the past few weeks,” Kress said, “we have not shipped any H20 devices based on those licenses.”\nStill, the Chinese government has officially discouraged the use of Nvidia chips by local businesses, leading the company to reportedly halt production of the H20 chip earlier this month.\nNvidia said it expects $54 billion in revenue in the third quarter. The company noted that its outlook for the third quarter, which could shift 2% in either direction, doesn’t include any H20 shipments to China."}
{"title": "911 centers are so understaffed, they’re turning to AI to answer calls", "url": "https://techcrunch.com/2025/08/27/911-centers-are-so-understaffed-theyre-turning-to-ai-to-answer-calls/", "source": "TechCrunch (AI)", "published": "2025-08-27T20:42:12+00:00", "text": "When Max Keenan joined Y Combinator’s summer 2022 batch, he was working on Aurelian, a company that automated appointment bookings for hair salons. But less than a year later, a conversation with one of his clients led him to a far more significant problem.\nA nearby school’s carpool line was constantly blocking the parking lot of one of Aurelian’s hair salon clients. The salon owner called the city’s non-emergency line and was put on hold for 45 minutes before reaching a dispatcher. “She called me into her office afterwards, and was like, ‘Max, do you want to help me out?’” Keenan told TechCrunch.\nWhen he started to research how municipal non-emergency response call centers work, he discovered that they are often handled by the same people who are answering actual 911 emergencies.\nAurelian pivoted to building an AI voice assistant that helps 911 call centers offload non-emergency call volume. The company announced on Wednesday that it raised a $14 million Series A led by NEA.\nThe company’s AI voice agent is designed to triage non-urgent issues like noise complaints, parking violations, and even stolen wallet reports — situations that don’t need an officer’s immediate response or can be handled without dispatching personnel to the scene.\nAurelian’s AI is trained to recognize a real emergency and immediately transfer those calls to a human dispatcher, Keenan said. In other situations, the system collects key information and either creates a report for or relays the details directly to the police department for follow-up action.\nSince launching its AI assistant in May 2024, Aurelian has been deployed at more than a dozen 911 dispatch centers, including those serving Snohomish County, Washington; Chattanooga, Tennessee; and Kalamazoo, Michigan.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital — just a few of the heavy hitters joining the Disrupt 2025 agenda. They’re here to deliver the insights that fuel startup growth and sharpen your edge. Don’t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech — grab your ticket now and save up to $675 before prices rise.\nEmergency call centers are adopting Aurelian largely because they are consistently understaffed — a direct result of dispatching being a high-pressure job that ranks among the top 10 industries with the highest turnover rates. Emergency dispatchers are often asked to work overtime, with reports of 12- to 16-hour workdays in certain counties.\n“The reason why we’re most focused on 911 is because it’s the industry that has this pain point most acutely,” Keenan said. “We think that these telecommunicators should have a chance of taking a break or go to the bathroom.”\nMustafa Neemuchwala, a partner at NEA, said, “One of the things that blows my mind, you’re not replacing an existing human being; you’re replacing a person they wanted to hire but couldn’t.”\nAurelian isn’t the only AI startup tackling non-emergency calls. Hyper, which raised a $6.3 million seed round, came out of stealth last month. Prepared, a company founded in 2019, also recently added an AI voice solution for emergency response.\nBut Aurelian believes its product is ahead of the competition. According to Neemuchwala, Aurelian is the only company actually deployed and handling live calls. “As far as we know, nobody else is actually live,” he said, referring to Aurelian responding to thousands of actual calls daily."}
{"title": "Detect Amazon Bedrock misconfigurations with Datadog Cloud Security", "url": "https://aws.amazon.com/blogs/machine-learning/detect-amazon-bedrock-misconfigurations-with-datadog-cloud-security/", "source": "AWS Machine Learning Blog", "published": "2025-08-29T16:55:37+00:00", "text": "Artificial Intelligence\nDetect Amazon Bedrock misconfigurations with Datadog Cloud Security\nThis post was co-written with Nick Frichette and Vijay George from Datadog.\nAs organizations increasingly adopt Amazon Bedrock for generative AI applications, protecting against misconfigurations that could lead to data leaks or unauthorized model access becomes critical. The AWS Generative AI Adoption Index, which surveyed 3,739 senior IT decision-makers across nine countries, revealed that 45% of organizations selected generative AI tools as their top budget priority in 2025. As more AWS and Datadog customers accelerate their adoption of AI, building AI security into existing processes will become essential, especially as more stringent regulations emerge. But looking at AI risks in a silo isn’t enough; AI risks must be contextualized alongside other risks such as identity exposures and misconfigurations. The combination of Amazon Bedrock and Datadog’s comprehensive security monitoring helps organizations innovate faster while maintaining robust security controls.\nAmazon Bedrock delivers enterprise-grade security by incorporating built-in protections across data privacy, access controls, network security, compliance, and responsible AI safeguards. Customer data is encrypted both in transit using TLS 1.2 or above and at rest with AWS Key Management Service (AWS KMS), and organizations have full control over encryption keys. Data privacy is central: your input, prompts, and outputs are not shared with model providers nor used to train or improve foundation models (FMs). Fine-tuning and customizations occur on private copies of models, providing data confidentiality. Access is tightly governed through AWS Identity and Access Management (IAM) and resource-based policies, supporting granular authorization for users and roles. Amazon Bedrock integrates with AWS PrivateLink and supports virtual private cloud (VPC) endpoints for private, internal communication, so traffic doesn’t leave the Amazon network. The service complies with key industry standards such as ISO, SOC, CSA STAR, HIPAA eligibility, GDPR, and FedRAMP High, making it suitable for regulated industries. Additionally, Amazon Bedrock includes configurable guardrails to filter sensitive or harmful content and promote responsible AI use. Security is structured under the AWS Shared Responsibility Model, where AWS manages infrastructure security and customers are responsible for secure configurations and access controls within their Amazon Bedrock environment.\nBuilding on these robust AWS security features, Datadog and AWS have partnered to provide a holistic view of AI infrastructure risks, vulnerabilities, sensitive data exposure, and other misconfigurations. Datadog Cloud Security employs both agentless and agent-based scanning to help organizations identify, prioritize, and remediate risks across cloud resources. This integration helps AWS users prioritize risks based on business criticality, with security findings enriched by observability data, thereby enhancing their overall security posture in AI implementations.\nWe’re excited to announce new security capabilities in Datadog Cloud Security that can help you detect and remediate Amazon Bedrock misconfigurations before they become security incidents. This integration helps organizations embed robust security controls and secure their use of the powerful capabilities of Amazon Bedrock by offering three critical advantages: holistic AI security by integrating AI security into your broader cloud security strategy, real-time risk detection through identifying potential AI-related security issues as they emerge, and simplified compliance to help meet evolving AI regulations with pre-built detections.\nAWS and Datadog: Empowering customers to adopt AI securely\nThe partnership between AWS and Datadog is focused on helping customers operate their cloud infrastructure securely and efficiently. As organizations rapidly adopt AI technologies, extending this partnership to include Amazon Bedrock is a natural evolution. Amazon Bedrock is a fully managed service that makes high-performing FMs from leading AI companies and Amazon available through a unified API, making it an ideal starting point for Datadog’s AI security capabilities.\nThe decision to prioritize Amazon Bedrock integration is driven by several factors, including strong customer demand, comprehensive security needs, and the existing integration foundation. With over 900 integrations and a partner-built Marketplace, Datadog’s long-standing partnership with AWS and deep integration capabilities have helped Datadog quickly develop comprehensive security monitoring for Amazon Bedrock while using their existing cloud security expertise.\nThroughout Q4 2024, Datadog Security Research observed increasing threat actor interest in cloud AI environments, making this integration particularly timely. By combining the powerful AI capabilities of AWS with Datadog’s security expertise, you can safely accelerate your AI adoption while maintaining robust security controls.\nHow Datadog Cloud Security helps secure Amazon Bedrock resources\nAfter adding the AWS integration to your Datadog account and enabling Datadog Cloud Security, Datadog Cloud Security continuously monitors your AWS environment, identifying misconfigurations, identity risks, vulnerabilities, and compliance violations. These detections use the Datadog Severity Scoring system to prioritize them based on infrastructure context. The scoring considers a variety of variables, including if the resource is in production, is publicly accessible, or has access to sensitive data. This multi-layer analysis can help you reduce noise and focus your attention to the most critical misconfigurations by considering runtime behavior.\nPartnering with AWS, Datadog is excited to offer detections for Datadog Cloud Security customers, such as:\n- Amazon Bedrock custom models should not output model data to publicly accessible S3 buckets\n- Amazon Bedrock custom models should not train from publicly writable S3 buckets\n- Amazon Bedrock guardrails should have a prompt attack filter enabled and block prompt attacks at high sensitivity\n- Amazon Bedrock agent guardrails should have the sensitive information filter enabled and block highly sensitive PII entities\nDetect AI misconfigurations with Datadog Cloud Security\nTo understand how these detections can help secure your Amazon Bedrock infrastructure, let’s look at a specific use case, in which Amazon Bedrock custom models should not train from publicly writable Amazon Simple Storage Service (Amazon S3) buckets.\nWith Amazon Bedrock, you can customize AI models by fine-tuning on domain specific data. To do this, that data is stored in an S3 bucket. Threat actors are constantly evaluating the configuration of S3 buckets, looking for the potential to access sensitive data or even the ability to write to S3 buckets.\nIf a threat actor finds an S3 bucket that was misconfigured to permit public write access, and that same bucket contained data that was used to train an AI model, a bad actor could poison that dataset and introduce malicious behavior or output to the model. This is known as a data poisoning attack.\nNormally, detecting these types of misconfigurations requires multiple steps: one to identify the S3 bucket misconfigured with write access, and one to identify that the bucket is being used by Amazon Bedrock. With Datadog Cloud Security, this detection is one of hundreds that are activated out of the box.\nIn the Datadog Cloud Security system, you can view this issue alongside surrounding infrastructure using Cloud Map. This provides live diagrams of your cloud architecture, as shown in the following screenshot. AI risks are then contextualized alongside sensitive data exposure, identity risks, vulnerabilities, and other misconfigurations to give you a 360-view of risks.\nFor example, you might see that your application is using Anthropic’s Claude 3.7 on Amazon Bedrock and accessing training or prompt data stored in an S3 bucket that also has public write access. This could inadvertently impact model integrity by introducing unapproved data to the large language model (LLM), so you will want to update this configuration. Though basic, the first step for most security initiatives is identifying the issue. With agentless scanning, Datadog scans your AWS environment at intervals between 15 minutes and 2 hours, so users can identify misconfigurations as they are introduced to their environment. The next step is to remediate this risk. Datadog Cloud Security offers automatically generated remediation guidance, specifically for each risk (see the following screenshot). You will get a step-by-step explanation of how to fix each finding. In this situation, we can remediate this issue by modifying the S3 bucket’s policy, helping prevent public write access. You can do this directly in AWS, create a JIRA ticket, or use the built-in workflow automation tools. From here, you can apply remediation steps directly within Datadog and confirm that the misconfiguration has been resolved.\nResolving this issue will positively impact your compliance posture, as illustrated by the posture score in Datadog Cloud Security, helping teams meet internal benchmarks and regulatory standards. Teams can also create custom frameworks or iterate on existing ones for tailored compliance controls.\nAs generative AI is embraced across industries, the regulatory environment will evolve. Datadog will continue partnering with AWS to expand their detection library and support secure AI adoption and compliance.\nHow Datadog Cloud Security detects misconfigurations in your cloud environment\nYou can deploy Datadog Cloud Security either with the Datadog agent, agentlessly, or both to maximize security coverage in your cloud environment. Datadog customers can start monitoring their AWS accounts for misconfigurations by first adding the AWS integration to Datadog. This enables Datadog to crawl cloud resources in customer AWS accounts.\nAs the Datadog system finds resources, it runs through a catalog of hundreds of out-of-the-box detection rules against these resources, looking for misconfigurations and threat paths that adversaries can exploit.\nSecure your AI infrastructure with Datadog\nMisconfigurations in AI systems can be risky, but with the right tools, you can have the visibility and context needed to manage them. With Datadog Cloud Security, teams gain visibility into these risks, detect threats early, and remediate issues with confidence. In addition, Datadog has also released numerous agentic AI security features, designed to help teams gain visibility into the health and security of critical AI workload, which includes new announcements made to Datadog’s LLM observability features.\nLastly, Datadog announced Bits AI Security Analyst alongside other Bits AI agents at DASH. Included as part of Cloud SIEM, Bits is an agentic AI security analyst that automates triage for AWS CloudTrail signals. Bits investigates each alert like a seasoned analyst: pulling in relevant context from across your Datadog environment, annotating key findings, and offering a clear recommendation on whether the signal is likely benign or malicious. By accelerating triage and surfacing real threats faster, Bits helps reduce mean time to remediation (MTTR) and frees analysts to focus on important threat hunting and response initiatives. This helps across different threats, including AI-related threats.\nTo learn more about how Datadog helps secure your AI infrastructure, see Monitor Amazon Bedrock with Datadog or check out our security documentation. If you’re not already using Datadog, you can get started with Datadog Cloud Security with a 14-day free trial.\nAbout the Authors\nNina Chen is a Customer Solutions Manager at AWS specializing in leading software companies to use the power of the AWS Cloud to accelerate their product innovation and growth. With over 4 years of experience working in the strategic independent software vendor (ISV) vertical, Nina enjoys guiding ISV partners through their cloud transformation journeys, helping them optimize their cloud infrastructure, driving product innovation, and delivering exceptional customer experiences.\nSujatha Kuppuraju is a Principal Solutions Architect at AWS, specializing in cloud and generative AI security. She collaborates with software companies’ leadership teams to architect secure, scalable solutions on AWS and guide strategic product development. Using her expertise in cloud architecture and emerging technologies, Sujatha helps organizations optimize offerings, maintain robust security, and bring innovative products to market in an evolving tech landscape.\nNick Frichette is a Staff Security Researcher for Cloud Security Research at Datadog.\nVijay George is a Product Manager for AI Security at Datadog."}
{"title": "Set up custom domain names for Amazon Bedrock AgentCore Runtime agents", "url": "https://aws.amazon.com/blogs/machine-learning/set-up-custom-domain-names-for-amazon-bedrock-agentcore-runtime-agents/", "source": "AWS Machine Learning Blog", "published": "2025-08-29T16:46:13+00:00", "text": "Artificial Intelligence\nSet up custom domain names for Amazon Bedrock AgentCore Runtime agents\nWhen deploying AI agents to Amazon Bedrock AgentCore Runtime (currently in preview), customers often want to use custom domain names to create a professional and seamless experience.\nBy default, AgentCore Runtime agents use endpoints like https://bedrock-agentcore.{region}.amazonaws.com/runtimes/{EncodedAgentARN}/invocations\n.\nIn this post, we discuss how to transform these endpoints into user-friendly custom domains (like https://agent.yourcompany.com\n) using Amazon CloudFront as a reverse proxy. The solution combines CloudFront, Amazon Route 53, and AWS Certificate Manager (ACM) to create a secure, scalable custom domain setup that works seamlessly with your existing agents.\nBenefits of Amazon Bedrock AgentCore Runtime\nIf you’re building AI agents, you have probably wrestled with hosting challenges: managing infrastructure, handling authentication, scaling, and maintaining security. Amazon Bedrock AgentCore Runtime helps address these problems.\nAmazon Bedrock AgentCore Runtime is framework agnostic; you can use it with LangGraph, CrewAI, Strands Agents, or custom agents you have built from scratch. It supports extended execution times up to 8 hours, perfect for complex reasoning tasks that traditional serverless functions can’t handle. Each user session runs in its own isolated microVM, providing security that’s crucial for enterprise applications.\nThe consumption-based pricing model means you only pay for what you use, not what you provision. And unlike other hosting solutions, Amazon Bedrock AgentCore Runtime includes built-in authentication and specialized observability for AI agents out of the box.\nBenefits of custom domains\nWhen using Amazon Bedrock AgentCore Runtime with Open Authorization (OAuth) authentication, your applications make direct HTTPS requests to the service endpoint. Although this works, custom domains offer several benefits:\n- Custom branding – Client-side applications (web browsers, mobile apps) display your branded domain instead of AWS infrastructure details in network requests\n- Better developer experience – Development teams can use memorable, branded endpoints instead of copying and pasting long AWS endpoints across code bases and configurations\n- Simplified maintenance – Custom domains make it straightforward to manage endpoints when deploying multiple agents or updating configurations across environments\nSolution overview\nIn this solution, we use CloudFront as a reverse proxy to transform requests from your custom domain into Amazon Bedrock AgentCore Runtime API calls. Instead of using the default endpoint, your applications can make requests to a user-friendly URL like https://agent.yourcompany.com/\n.\nThe following diagram illustrates the solution architecture.\nThe workflow consists of the following steps:\n- A client application authenticates with Amazon Cognito and receives a bearer token.\n- The client makes an HTTPS request to your custom domain.\n- Route 53 resolves the DNS request to CloudFront.\n- CloudFront forwards the authenticated request to the Amazon Bedrock Runtime agent.\n- The agent processes the request and returns the response through the same path.\nYou can use the same CloudFront distribution to serve both your frontend application and backend agent endpoints, avoiding cross-origin resource sharing (CORS) issues because everything originates from the same domain.\nPrerequisites\nTo follow this walkthrough, you must have the following in place:\n- An AWS account with appropriate permissions\n- The AWS Cloud Development Kit (AWS CDK) version 2.x or later\n- An AWS Identity and Access Management (IAM) execution role with appropriate permissions for Amazon Bedrock AgentCore Runtime\nAlthough Amazon Bedrock AgentCore Runtime can be in other supported AWS Regions, CloudFront requires SSL certificates to be in the us-east-1\nRegion.\nYou can choose from the following domain options:\n- Use an existing domain – Add a subdomain like\nagent.yourcompany.com\n- Register a new domain – Use Route 53 to register a domain if you don’t have one\n- Use the default URL from CloudFront – No domain registration or configuration required\nChoose the third option if you want to test the solution quickly before setting up a custom domain.\nCreate an agent with inbound authentication\nIf you already have an agent deployed with OAuth authentication, you can skip to the next section to set up the custom domain. Otherwise, follow these steps to create a new agent using Amazon Cognito as your OAuth provider:\n- Create a new directory for your agent with the following structure:\n- Create the main agent code in\nagent_example.py\n:\n- Add dependencies to\nrequirements.txt\n:\n- Run the following commands to create an Amazon Cognito user pool and test user:\n- Deploy the agent using the Amazon Bedrock AgentCore command line interface (CLI) provided by the starter toolkit:\nMake note of your agent runtime Amazon Resource Name (ARN) after deployment. You will need this for the custom domain configuration.\nFor additional examples and details, see Authenticate and authorize with Inbound Auth and Outbound Auth.\nSet up the custom domain solution\nNow let’s implement the custom domain solution using the AWS CDK. This section shows you how to create the CloudFront distribution that proxies your custom domain requests to Amazon Bedrock AgentCore Runtime endpoints.\n- Create a new directory and initialize an AWS CDK project:\n- Encode the agent ARN and prepare the CloudFront origin configuration:\nIf your frontend application runs on a different domain than your agent endpoint, you must configure CORS headers. This is common if your frontend is hosted on a different domain (for example, https://app.yourcompany.com\ncalling https://agent.yourcompany.com\n), or if you’re developing locally (for example, http://localhost:3000\ncalling your production agent endpoint).\n- To handle CORS requirements, create a CloudFront response headers policy:\n- Create a CloudFront distribution to act as a reverse proxy for your agent endpoints:\nSet cache_policy=CachePolicy.CACHING_DISABLED\nto make sure your agent responses remain dynamic and aren’t cached by CloudFront.\n- If you’re using a custom domain, add an SSL certificate and DNS configuration to your stack:\nThe following code is the complete AWS CDK stack that combines all the components:\n- Configure the AWS CDK\napp\nentry point:\nDeploy your custom domain\nNow you can deploy the solution and verify it works with both custom and default domains. Complete the following steps:\n- Update the following values in\nagentcore_custom_domain_stack.py\n:- Your Amazon Bedrock AgentCore Runtime ARN\n- Your domain name (if using a custom domain)\n- Your hosted zone ID (if using a custom domain)\n- Deploy using the AWS CDK:\nTest your endpoint\nAfter you deploy the custom domain, you can test your endpoints using either the custom domain or the CloudFront default domain.First, get a JWT token from Amazon Cognito:\nUse the following code to test with your custom domain:\nAlternatively, use the following code to test with the CloudFront default domain:\nConsiderations\nAs you implement this solution in production, the following are some important considerations:\n- Cost implications – CloudFront adds costs for data transfer and requests. Review Amazon CloudFront pricing to understand the impact for your usage patterns.\n- Security enhancements – Consider implementing the following security measures:\n- AWS WAF rules to help protect against common web exploits.\n- Rate limiting to help prevent abuse.\n- Geo-restrictions if your agent should only be accessible from specific Regions.\n- Monitoring – Enable CloudFront access logs and set up Amazon CloudWatch alarms to monitor error rates, latency, and request volume.\nClean up\nTo avoid ongoing costs, delete the resources when you no longer need them:\nYou might need to manually delete the Route 53 hosted zones and ACM certificates from their respective service consoles.\nConclusion\nIn this post, we showed you how to create custom domain names for your Amazon Bedrock AgentCore Runtime agent endpoints using CloudFront as a reverse proxy. This solution provides several key benefits: simplified integration for development teams, custom domains that align with your organization, cleaner infrastructure abstraction, and straightforward maintenance when endpoints need updates. By using CloudFront as a reverse proxy, you can also serve both your frontend application and backend agent endpoints from the same domain, avoiding common CORS challenges.\nWe encourage you to explore this solution further by adapting it to your specific needs. You might want to enhance it with additional security features, set up monitoring, or integrate it with your existing infrastructure.\nTo learn more about building and deploying AI agents, see the Amazon Bedrock AgentCore Developer Guide. For advanced configurations and best practices with CloudFront, refer to the Amazon CloudFront documentation. You can find detailed information about SSL certificates in the AWS Certificate Manager documentation, and domain management in the Amazon Route 53 documentation.\nAmazon Bedrock AgentCore is currently in preview and subject to change. Standard AWS pricing applies to additional services used, such as CloudFront, Route 53, and Certificate Manager.\nAbout the authors\nRahmat Fedayizada is a Senior Solutions Architect with the AWS Energy and Utilities team. He works with energy companies to design and implement scalable, secure, and highly available architectures. Rahmat is passionate about translating complex technical requirements into practical solutions that drive business value.\nParas Bhuva is a Senior Manager of Solutions Architecture at AWS, where he leads a team of solution architects helping energy customers innovate and accelerate their transformation. Having started as a Solution Architect in 2012, Paras is passionate about architecting scalable solutions and building organizations focused on application modernization and AI initiatives."}
{"title": "Introducing auto scaling on Amazon SageMaker HyperPod", "url": "https://aws.amazon.com/blogs/machine-learning/introducing-auto-scaling-on-amazon-sagemaker-hyperpod/", "source": "AWS Machine Learning Blog", "published": "2025-08-29T16:21:02+00:00", "text": "Artificial Intelligence\nIntroducing auto scaling on Amazon SageMaker HyperPod\nToday, we’re excited to announce that Amazon SageMaker HyperPod now supports managed node automatic scaling with Karpenter, so you can efficiently scale your SageMaker HyperPod clusters to meet your inference and training demands. Real-time inference workloads require automatic scaling to address unpredictable traffic patterns and maintain service level agreements (SLAs). As demand spikes, organizations must rapidly adapt their GPU compute without compromising response times or cost-efficiency. Unlike self-managed Karpenter deployments, this service-managed solution alleviates the operational overhead of installing, configuring, and maintaining Karpenter controllers, while providing tighter integration with the resilience capabilities of SageMaker HyperPod. This managed approach supports scale to zero, reducing the need for dedicated compute resources to run the Karpenter controller itself, improving cost-efficiency.\nSageMaker HyperPod offers a resilient, high-performance infrastructure, observability, and tooling optimized for large-scale model training and deployment. Companies like Perplexity, HippocraticAI, H.AI, and Articul8 are already using SageMaker HyperPod for training and deploying models. As more customers transition from training foundation models (FMs) to running inference at scale, they require the ability to automatically scale their GPU nodes to handle real production traffic by scaling up during high demand and scaling down during periods of lower utilization. This capability necessitates a powerful cluster auto scaler. Karpenter, an open source Kubernetes node lifecycle manager created by AWS, is a popular choice among Kubernetes users for cluster auto scaling due to its powerful capabilities that optimize scaling times and reduce costs.\nThis launch provides a managed Karpenter-based solution for automatic scaling that is installed and maintained by SageMaker HyperPod, removing the undifferentiated heavy lifting of setup and management from customers. The feature is available for SageMaker HyperPod EKS clusters, and you can enable auto scaling to transform your SageMaker HyperPod cluster from static capacity to a dynamic, cost-optimized infrastructure that scales with demand. This combines Karpenter’s proven node lifecycle management with the purpose-built and resilient infrastructure of SageMaker HyperPod, designed for large-scale machine learning (ML) workloads. In this post, we dive into the benefits of Karpenter, and provide details on enabling and configuring Karpenter in your SageMaker HyperPod EKS clusters.\nNew features and benefits\nKarpenter-based auto scaling in your SageMaker HyperPod clusters provides the following capabilities:\n- Service managed lifecycle – SageMaker HyperPod handles Karpenter installation, updates, and maintenance, alleviating operational overhead\n- Just-in-time provisioning – Karpenter observes your pending pods and provisions the required compute for your workloads from an on-demand pool\n- Scale to zero – You can scale down to zero nodes without maintaining dedicated controller infrastructure\n- Workload-aware node selection – Karpenter chooses optimal instance types based on pod requirements, Availability Zones, and pricing to minimize costs\n- Automatic node consolidation – Karpenter regularly evaluates clusters for optimization opportunities, shifting workloads to avoid underutilized nodes\n- Integrated resilience – Karpenter uses the built-in fault tolerance and node recovery mechanisms of SageMaker HyperPod\nThese capabilities are built on top of recently launched continuous provisioning capabilities, which enables SageMaker HyperPod to automatically provision remaining capacity in the background while workloads start immediately on available instances. When node provisioning encounters failures due to capacity constraints or other issues, SageMaker HyperPod automatically retries in the background until clusters reach their desired scale, so your auto scaling operations remain resilient and non-blocking.\nSolution overview\nThe following diagram illustrates the solution architecture.\nKarpenter works as a controller in the cluster and operates in the following steps:\n- Watching – Karpenter watches for un-schedulable pods in the cluster through the Kubernetes API server. These could be pods that go into pending state when deployed or automatically scaled to increase the replica count.\n- Evaluating – When Karpenter finds such pods, it computes the shape and size of a NodeClaim to fit the set of pods requirements (GPU, CPU, memory) and topology constraints, and checks if it can pair them with an existing NodePool. For each NodePool, it queries the SageMaker HyperPod APIs to get the instance types supported by the NodePool. It uses the information about instance type metadata (hardware requirements, zone, capacity type) to find a matching NodePool.\n- Provisioning – If Karpenter finds a matching NodePool, it creates a NodeClaim and tries to provision a new instance to be used as the new node. Karpenter internally uses the\nsagemaker:UpdateCluster\nAPI to increase the capacity of the selected instance group. - Disrupting – Karpenter periodically checks if a new node is needed or not. If it’s not needed, Karpenter deletes it, which internally translates to a delete node request to the SageMaker HyperPod cluster.\nPrerequisites\nVerify you have the required quotas for the instances you will create in the SageMaker HyperPod cluster. To review your quotas, on the Service Quotas console, choose AWS services in the navigation pane, then choose SageMaker. For example, the following screenshot shows the available quota for g5.12xlarge instances (three).\nTo update the cluster, you must first create AWS Identity and Access Management (IAM) permissions for Karpenter. For instructions, see Create an IAM role for HyperPod autoscaling with Karpenter.\nCreate and configure a SageMaker HyperPod cluster\nTo begin, launch and configure your SageMaker HyperPod EKS cluster and verify that continuous provisioning mode is enabled on cluster creation. Complete the following steps:\n- On the SageMaker AI console, choose HyperPod clusters in the navigation pane.\n- Choose Create HyperPod cluster and Orchestrated on Amazon EKS.\n- For Setup options, select Custom setup.\n- For Name, enter a name.\n- For Instance recovery, select Automatic.\n- For Instance provisioning mode, select Use continuous provisioning.\n- Choose Submit.\nThis setup creates the necessary configuration such as virtual private cloud (VPC), subnets, security groups, and EKS cluster, and installs operators in the cluster. You can also provide existing resources such as an EKS cluster if you want to use an existing cluster instead of creating a new one. This setup will take around 20 minutes.\nVerify that each InstanceGroup\nis limited to one zone by opting for the OverrideVpcConfig\nand selecting only one subnet per each InstanceGroup\n.\nAfter you create the cluster, you must update it to enable Karpenter. You can do this using Boto3 or the AWS Command Line Interface (AWS CLI) using the UpdateCluster API command (after configuring the AWS CLI to connect to your AWS account).\nThe following code uses Python Boto3:\nAfter you run this command and update the cluster, you can verify that Karpenter has been enabled by running the DescribeCluster API.\nThe following code uses Python:\nThe following code uses the AWS CLI:\nThe following code shows our output:\nNow you have a working cluster. The next step is to set up some custom resources in your cluster for Karpenter.\nCreate HyperpodNodeClass\nHyperpodNodeClass\nis a custom resource that maps to pre-created instance groups in SageMaker HyperPod, defining constraints around which instance types and Availability Zones are supported for Karpenter’s auto scaling decisions. To use HyperpodNodeClass\n, simply specify the names of the InstanceGroups\nof your SageMaker HyperPod cluster that you want to use as the source for the AWS compute resources to use to scale up your pods in your NodePools.\nThe HyperpodNodeClass\nname that you use here is carried over to the NodePool in the next section where you reference it. This tells the NodePool which HyperpodNodeClass\nto draw resources from. To create a HyperpodNodeClass\n, complete the following steps:\n- Create a YAML file (for example,\nnodeclass.yaml\n) similar to the following code. AddInstanceGroup\nnames that you used at the time of the SageMaker HyperPod cluster creation. You can also add new instance groups to an existing SageMaker HyperPod EKS cluster. - Reference the\nHyperPodNodeClass\nname in your NodePool configuration.\nThe following is a sample HyperpodNodeClass\nthat uses ml.g6.xlarge and ml.g6.4xlarge instance types:\n- Apply the configuration to your EKS cluster using\nkubectl\n:\n- Monitor the\nHyperpodNodeClass\nstatus to verify theReady\ncondition in status is set toTrue\nto ensure it was successfully created:\nThe SageMaker HyperPod cluster must have AutoScaling\nenabled and the AutoScaling\nstatus must change to InService\nbefore the HyperpodNodeClass\ncan be applied.\nFor more information and key considerations, see Autoscaling on SageMaker HyperPod EKS.\nCreate NodePool\nThe NodePool sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. The NodePool can be set to perform various actions, such as:\n- Define labels and taints to limit the pods that can run on nodes Karpenter creates\n- Limit node creation to certain zones, instance types, and computer architectures, and so on\nFor more information about NodePool, refer to NodePools. SageMaker HyperPod managed Karpenter supports a limited set of well-known Kubernetes and Karpenter requirements, which we explain in this post.\nTo create a NodePool, complete the following steps:\n- Create a YAML file named\nnodepool.yaml\nwith your desired NodePool configuration.\nThe following code is a sample configuration to create a sample NodePool. We specify the NodePool to include our ml.g6.xlarge SageMaker instance type, and we additionally specify it for one zone. Refer to NodePools for more customizations.\n- Apply the NodePool to your cluster:\n- Monitor the NodePool status to ensure the\nReady\ncondition in the status is set toTrue\n:\nThis example shows how a NodePool can be used to specify the hardware (instance type) and placement (Availability Zone) for pods.\nLaunch a simple workload\nThe following workload runs a Kubernetes deployment where the pods in deployment are requesting for 1 CPU and 256 MB memory per replica, per pod. The pods have not been spun up yet.\nWhen we apply this, we can see a deployment and a single node launch in our cluster, as shown in the following screenshot.\nTo scale this component, use the following command:\nWithin a few minutes, we can see Karpenter add the requested nodes to the cluster.\nImplement advanced auto scaling for inference with KEDA and Karpenter\nTo implement an end-to-end auto scaling solution on SageMaker HyperPod, you can set up Kubernetes Event-driven Autoscaling (KEDA) along with Karpenter. KEDA enables pod-level auto scaling based on a wide range of metrics, including Amazon CloudWatch metrics, Amazon Simple Queue Service (Amazon SQS) queue lengths, Prometheus queries, and resource utilization patterns. By configuring Keda ScaledObject\nresources to target your model deployments, KEDA can dynamically adjust the number of inference pods based on real-time demand signals.\nWhen integrating KEDA and Karpenter, this combination creates a powerful two-tier auto scaling architecture. As KEDA scales your pods up or down based on workload metrics, Karpenter automatically provisions or deletes nodes in response to changing resource requirements. This integration delivers optimal performance while controlling costs by making sure your cluster has precisely the right amount of compute resources available at all times. For effective implementation, consider the following key factors:\n- Set appropriate buffer thresholds in KEDA to accommodate Karpenter’s node provisioning time\n- Configure cooldown periods carefully to prevent scaling oscillations\n- Define clear resource requests and limits to help Karpenter make optimal node selections\n- Create specialized NodePools tailored to specific workload characteristics\nThe following is a sample spec of a KEDA ScaledObject\nfile that scales the number of pods based on CloudWatch metrics of Application Load Balancer (ALB) request count:\nClean up\nTo clean up your resources to avoid incurring more charges, delete your SageMaker HyperPod cluster.\nConclusion\nWith the launch of Karpenter node auto scaling on SageMaker HyperPod, ML workloads can automatically adapt to changing workload requirements, optimize resource utilization, and help control costs by scaling precisely when needed. You can also integrate it with event-driven pod auto scalers such as KEDA to scale based on custom metrics.\nTo experience these benefits for your ML workloads, enable Karpenter in your SageMaker HyperPod clusters. For detailed implementation guidance and best practices, refer to Autoscaling on SageMaker HyperPod EKS.\nAbout the authors\nVivek Gangasani is a Worldwide Lead GenAI Specialist Solutions Architect for SageMaker Inference. He drives Go-to-Market (GTM) and Outbound Product strategy for SageMaker Inference. He also helps enterprises and startups deploy, manage, and scale their GenAI models with SageMaker and GPUs. Currently, he is focused on developing strategies and content for optimizing inference performance and GPU efficiency for hosting Large Language Models. In his free time, Vivek enjoys hiking, watching movies, and trying different cuisines.\nAdam Stanley is a Solution Architect for Software, Internet and Model Provider customers at Amazon Web Services (AWS). He supports customers adopting all AWS services, but focuses primarily on Machine Learning training and inference infrastructure. Prior to AWS, Adam went to the University of New South Wales and graduated with degrees in Mathematics and Accounting. You can connect with him on LinkedIn.\nKunal Jha is a Principal Product Manager at AWS, where he focuses on building Amazon SageMaker HyperPod to enable scalable distributed training and fine-tuning of foundation models. In his spare time, Kunal enjoys skiing and exploring the Pacific Northwest. You can connect with him on LinkedIn.\nTy Bergstrom is a Software Engineer at Amazon Web Services. He works on the HyperPod Clusters platform for Amazon SageMaker."}
{"title": "Meet Boti: The AI assistant transforming how the citizens of Buenos Aires access government information with Amazon Bedrock", "url": "https://aws.amazon.com/blogs/machine-learning/meet-boti-the-ai-assistant-transforming-how-the-citizens-of-buenos-aires-access-government-information-with-amazon-bedrock/", "source": "AWS Machine Learning Blog", "published": "2025-08-28T20:25:00+00:00", "text": "Artificial Intelligence\nMeet Boti: The AI assistant transforming how the citizens of Buenos Aires access government information with Amazon Bedrock\nThis post is co-written with Julieta Rappan, Macarena Blasi, and María Candela Blanco from the Government of the City of Buenos Aires.\nThe Government of the City of Buenos Aires continuously works to improve citizen services. In February 2019, it introduced an AI assistant named Boti available through WhatsApp, the most widely used messaging service in Argentina. With Boti, citizens can conveniently and quickly access a wide variety of information about the city, such as renewing a driver’s license, accessing healthcare services, and learning about cultural events. This AI assistant has become a preferred communication channel and facilitates more than 3 million conversations each month.\nAs Boti grows in popularity, the Government of the City of Buenos Aires seeks to provide new conversational experiences that harness the latest developments in generative AI. One challenge that citizens often face is navigating the city’s complex bureaucratic landscape. The City Government’s website includes over 1,300 government procedures, each of which has its own logic, nuances, and exceptions. The City Government recognized that Boti could improve access to this information by directly answering citizens’ questions and connecting them to the right procedure.\nTo pilot this new solution, the Government of the City of Buenos Aires partnered with the AWS Generative AI Innovation Center (GenAIIC). The teams worked together to develop an agentic AI assistant using LangGraph and Amazon Bedrock. The solution includes two main components: an input guardrail system and a government procedures agent. The input guardrail uses a custom LLM classifier to analyze incoming user queries, determining whether to approve or block requests based on their content. Approved requests are handled by the government procedures agent, which retrieves relevant procedural information and generates responses. Since most user queries focus on a single procedure, we developed a novel reasoning retrieval system to improve retrieval accuracy. This system initially retrieves comparative summaries that disambiguate similar procedures and then applies a large language model (LLM) to select the most relevant results. The agent uses this information to craft responses in Boti’s characteristic style, delivering short, helpful, and expressive messages in Argentina’s Rioplatense Spanish dialect. We focused on distinctive linguistic features of this dialect including the voseo (using “vos” instead of “tú”) and periphrastic future (using “ir a” before verbs).\nIn this post, we dive into the implementation of the agentic AI system. We begin with an overview of the solution, explaining its design and main features. Then, we discuss the guardrail and agent subcomponents and assess their performance. Our evaluation shows that the guardrails effectively block harmful content, including offensive language, harmful opinions, prompt injection attempts, and unethical behaviors. The agent achieves up to 98.9% top-1 retrieval accuracy using the reasoning retriever, which marks a 12.5–17.5% improvement over standard retrieval-augmented generation (RAG) methods. Subject matter experts found that Boti’s responses were 98% accurate in voseo usage and 92% accurate in periphrastic future usage. The promising results of this solution establish a new era of citizen-government interaction.\nSolution overview\nThe Government of the City of Buenos Aires and the GenAIIC built an agentic AI assistant using Amazon Bedrock and LangGraph that includes an input guardrail system to enable safe interactions and a government procedures agent to respond to user questions. The workflow is shown in the following diagram.\nThe process begins when a user submits a question. In parallel, the question is passed to the input guardrail system and government procedures agent. The input guardrail system determines whether the question contains harmful content. If triggered, it stops graph execution and redirects the user to ask questions about government procedures. Otherwise, the agent continues to formulate its response. The agent either calls a retrieval tool, which allows it to obtain relevant context and metadata from government procedures stored in Amazon Bedrock Knowledge Bases, or responds to the user. Both the input guardrail and government procedures agent use the Amazon Bedrock Converse API for LLM inference. This API provides access to a wide selection of LLMs, helping us optimize performance and latency across different subtasks.\nInput guardrail system\nInput guardrails help prevent the LLM system from processing harmful content. Although Amazon Bedrock Guardrails offers one implementation approach with filters for specific words, content, or sensitive information, we developed a custom solution. This provided us greater flexibility to optimize performance for Rioplatense Spanish and monitor specific types of content. The following diagram illustrates our approach, in which an LLM classifier assigns a primary category (“approved” or “blocked”) as well as a more detailed subcategory.\nApproved queries are within the scope of the government procedures agent. They consist of on-topic requests, which focus on government procedures, and off-topic requests, which are low-risk conversation questions that the agent responds to directly. Blocked queries contain high-risk content that Boti should avoid, including offensive language, harmful opinions, prompt injection attacks, or unethical behaviors.\nWe evaluated the input guardrail system on a dataset consisting of both normal and harmful user queries. The system successfully blocked 100% of harmful queries, while occasionally flagging normal queries as harmful. This performance balance makes sure that Boti can provide helpful information while maintaining safe and appropriate interactions for users.\nAgent system\nThe government procedures agent is responsible for answering user questions. It determines when to retrieve relevant procedural information using its retrieval tool and generates responses in Boti’s characteristic style. In the following sections, we examine both processes.\nReasoning retriever\nThe agent can use a retrieval tool to provide accurate and up-to-date information about government procedures. Retrieval tools typically employ a RAG framework to perform semantic similarity searches between user queries and a knowledge base containing document chunks stored as embeddings, and then provide the most relevant samples as context to the LLM. Government procedures, however, present challenges to this standard approach. Related procedures, such as renewing and reprinting drivers’ licenses, can be difficult to disambiguate. Additionally, each user question typically requires information from one specific procedure. The mixture of chunks returned from standard RAG approaches increases the likelihood of generating incorrect responses.\nTo better disambiguate government procedures, the Buenos Aires and GenAIIC teams developed a reasoning retrieval method that uses comparative summaries and LLM selection. An overview of this approach is shown in the following diagram.\nA necessary preprocessing step before retrieval is the creation of a government procedures knowledge base. To capture both the key information contained in procedures and how they related to each other, we created comparative summaries. Each summary contains basic information, such as the procedure’s purpose, intended audience, and content, such as costs, steps, and requirements. We clustered the base summaries into small groups, with an average cluster size of 5, and used an LLM to generate descriptions about what made each procedure different from its neighbors. We appended the distinguishing descriptions to the base information to create the final summary. We note that this approach shares similarities to Anthropic’s Contextual Retrieval, which prepends explanatory context to document chunk.\nWith the knowledge base in place, we are able to retrieve relevant government procedures based on the user query. The reasoning retriever completes three steps:\n- Retrieve M Summaries: We retrieve between 1 and M comparative summaries using semantic search.\n- Optional Reasoning: In some cases, the initial retrieval surfaces similar procedures. To make sure that the most relevant procedures are returned to the agent, we apply an optional LLM reasoning step. The condition for this step occurs when the ratio of the first and second retrieval scores falls below a threshold value. An LLM follows a chain-of-thought (CoT) process in which it compares the user query to the retrieved summaries. It discards irrelevant procedures and reorders the remaining ones based on relevance. If the user query is specific enough, this process typically returns one result. By applying this reasoning step selectively, we minimize latency and token usage while maintaining high retrieval accuracy.\n- Retrieve N Full-Text Procedures: After the most relevant procedures are identified, we fetch their complete documents and metadata from an Amazon DynamoDB table. The metadata contains information like the source URL and the sentiment of the procedure. The agent typically receives between 1 and N results, where N ≤ M.\nThe agent receives the retrieved full text procedures in its context window. It follows its own CoT process to determine the relevant content and URL source attributions when generating its answer.\nWe evaluated our reasoning retriever against standard RAG techniques using a synthetic dataset of 1,908 questions derived from known source procedures. The performance was measured by determining whether the correct procedure appeared in the top-k retrieved results for each question. The following plot compares the top-k retrieval accuracy for each approach across different models, arranged in order of ascending performance from left to right. The metrics are proportionally weighted based on each procedure’s webpage visit frequency, making sure that our evaluation reflects real-world usage patterns.\nThe first three approaches represent standard vector-based retrieval methods. The first method, Section Titan, involved chunking procedures by document sections, targeting approximately 250 words per chunk, and then embedding the chunks using Amazon Titan Text Embeddings v2. The second method, Summaries Titan, consisted of embedding the procedure summaries using the same embedding model. By embedding summaries rather than document text, the retrieval accuracy improved by 7.8–15.8%. The third method, Summaries Cohere, involved embedding procedure summaries using Cohere Multilingual v3 on Amazon Bedrock. The Cohere Multilingual embedding model provided a noticeable improvement in retrieval accuracy compared to the Amazon Titan embedding models, with all top-k values above 90%.\nThe next three approaches use the reasoning retriever. We embedded the procedure summaries using the Cohere Multilingual model, retrieved 10 summaries during the initial retrieval step, and optionally applied the LLM-based reasoning step using either Anthropic’s Haiku 3, Claude 3 Sonnet, or Claude 3.5 Sonnet on Amazon Bedrock. All three reasoning retrievers consistently outperform standard RAG techniques, achieving 12.5–17.5% higher top-k accuracies. Anthropic’s Claude 3.5 Sonnet delivered the highest performance with 98.9% top-1 accuracy. These results demonstrate how combining embedding-based retrieval with LLM-powered reasoning can improve RAG performance.\nAnswer generation\nAfter collecting the necessary information, the agent responds using Boti’s distinctive communication style: concise, helpful messages in Rioplatense Spanish. We maintained this voice through prompt engineering that specified the following:\n- Personality – Convey a warm and friendly tone, providing quick solutions to everyday problems\n- Response length – Limit responses to a few sentences\n- Structure – Organize content using lists and highlights key information using bold text\n- Expression – Use emojis to mark important requirements and add visual cues\n- Dialect – Incorporate Rioplatense linguistic features, including voseo, periphrastic future, and regional vocabulary (for example, “acordate,” “entrar,” “acá,” and “allá”).\nGovernment procedures often address sensitive topics, like accidents, health, or security. To facilitate appropriate responses, we incorporated sentiment analysis into our knowledge base as metadata. This allows our system to route to different prompt templates. Sensitive topics are directed to prompts with reduced emoji usage and more empathetic language, whereas neutral topics receive standard templates.\nThe following figure shows a sample response to a question about borrowing library books. It has been translated to English for convenience.\nTo validate our prompt engineering approach, subject matter experts at the Government of the City of Buenos Aires reviewed a sample of Boti’s responses. Their analysis confirmed high fidelity to Rioplatense Spanish, with 98% accuracy in voseo usage and 92% in periphrastic future usage.\nConclusion\nThis post described the agentic AI assistant built by the Government of the City of Buenos Aires and the GenAIIC to respond to citizens’ questions about government procedures. The solution consists of two primary components: an input guardrail system that helps prevent the system from responding to harmful user queries and a government procedures agent that retrieves relevant information and generates responses. The input guardrails effectively block harmful content, including queries with offensive language, harmful opinions, prompt injection, and unethical behaviors. The government procedures agent employs a novel reasoning retrieval method that disambiguates similar government procedures, achieving up to 98.9% top-1 retrieval accuracy and a 12.5–17.5% improvement over standard RAG methods. Through prompt engineering, responses are delivered in Rioplatense Spanish using Boti’s voice. Subject matter experts rated Boti’s linguistic performance highly, with 98% accuracy in voseo usage and 92% in periphrastic future usage.\nAs generative AI advances, we expect to continuously improve our solution. The expanding catalog of LLMs available in Amazon Bedrock makes it possible to experiment with newer, more powerful models. This includes models that process text, as explored in the solution in this post, as well as models that process speech, allowing for direct speech-to-speech interactions. We might also explore the fine-tuning capabilities of Amazon Bedrock to customize models so that they better capture the linguistic features of Rioplatense Spanish. Beyond model improvements, we can iterate on our agent framework. The agent’s tool set can be expanded to support other tasks associated with government procedures like account creation, form completion, and appointment scheduling. As the City Government develops new experiences for citizens, we can consider implementing multi-agent frameworks in which specialist agents, like the government procedures agent, handle specific tasks.\nTo learn more about Boti and AWS’s generative AI capabilities, check out the following resources:\n- Boti: The City Chatbot\n- Government of the City of Buenos Aires: Procedures\n- Amazon Bedrock\n- Amazon Bedrock Knowledge Bases\nAbout the authors\nJulieta Rappan is Director of the Digital Channels Department of the Buenos Aires City Government, where she coordinates the landscape of digital and conversational interfaces. She has extensive experience in the comprehensive management of strategic and technological projects, as well as in leading high-performance teams focused on the development of digital products and services. Her leadership drives the implementation of technological solutions with a focus on scalability, coherence, public value, and innovation—where generative technologies are beginning to play a central role.\nMacarena Blasi is Chief of Staff at the Digital Channels Department of the Buenos Aires City Government, working across the city’s main digital services, including Boti—the WhatsApp-based virtual assistant—and the official Buenos Aires website. She began her journey working in conversational experience design, later serving as product owner and Operations Manager and then as Head of Experience and Content, leading multidisciplinary teams focused on improving the quality, accessibility, and usability of public digital services. Her work is driven by a commitment to building clear, inclusive, and human-centered experiences in the public sector.\nMaría Candela Blanco is Operations Manager for Quality Assurance, Usability, and Continuous Improvement at the Buenos Aires Government, where she leads the content, research, and conversational strategy across the city’s main digital channels, including the Boti AI assistant and the official Buenos Aires website. Outside of tech, Candela studies literature at UNSAM and is deeply passionate about language, storytelling, and the ways they shape our interactions with technology.\nLeandro Micchele is a Software Developer focused on applying AI to real-world use cases, with expertise in AI assistants, voice, and vision solutions. He serves as the technical lead and consultant for the Boti AI assistant at the Buenos Aires Government and works as a Software Developer at Telecom Argentina. Beyond tech, his discipline extends to martial arts: he has over 20 years of experience and currently teaches Aikido.\nHugo Albuquerque is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, Hugo had extensive experience working as a data scientist in the media and entertainment and marketing sectors. In his free time, he enjoys learning other languages like German and practicing social dancing, such as Brazilian Zouk.\nEnrique Balp is a Senior Data Scientist at the AWS Generative AI Innovation Center working on cutting-edge AI solutions. With a background in the physics of complex systems focused on neuroscience, he has applied data science and machine learning across healthcare, energy, and finance for over a decade. He enjoys hikes in nature, meditation retreats, and deep friendships.\nDiego Galaviz is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, he had over 8 years of expertise as a data scientist across diverse sectors, including financial services, energy, big tech, and cybersecurity. He holds a master’s degree in artificial intelligence, which complements his practical industry experience.\nLaura Kulowski is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where she works with customers to build generative AI solutions. Before joining Amazon, Laura completed her PhD at Harvard’s Department of Earth and Planetary Sciences and investigated Jupiter’s deep zonal flows and magnetic field using Juno data.\nRafael Fernandes is the LATAM leader of the AWS Generative AI Innovation Center, whose mission is to accelerate the development and implementation of generative AI in the region. Before joining Amazon, Rafael was a co-founder in the financial services industry space and a data science leader with over 12 years of experience in Europe and LATAM."}
{"title": "Empowering air quality research with secure, ML-driven predictive analytics", "url": "https://aws.amazon.com/blogs/machine-learning/empowering-air-quality-research-with-secure-ml-driven-predictive-analytics/", "source": "AWS Machine Learning Blog", "published": "2025-08-28T20:20:10+00:00", "text": "Artificial Intelligence\nEmpowering air quality research with secure, ML-driven predictive analytics\nAir pollution remains one of Africa’s most pressing environmental health crises, causing widespread illness across the continent. Organizations like sensors.AFRICA have deployed hundreds of air quality sensors to address this challenge, but face a critical data problem: significant gaps in PM2.5 (particulate matter with diameter less than or equal to 2.5 micrometers) measurement records because of power instability and connectivity issues in high-risk regions where physical maintenance is limited. Missing data in PM2.5 datasets reduces statistical power and introduces bias into parameter estimates, leading to unreliable trend detection and flawed conclusions about air quality patterns. These data gaps ultimately compromise evidence-based decision-making for pollution control strategies, health impact assessments, and regulatory compliance.\nIn this post, we demonstrate the time-series forecasting capability of Amazon SageMaker Canvas, a low-code no-code (LCNC) machine learning (ML) platform to predict PM2.5 from incomplete datasets. PM2.5 exposure contributes to millions of premature deaths globally through cardiovascular disease, respiratory illness, and systemic health effects, making accurate air quality forecasting a critical public health tool. A key advantage of the forecasting capability of SageMaker Canvas is its robust handling of incomplete data. Traditional air quality monitoring systems often require complete datasets to function properly, meaning they can’t be relied on when sensors malfunction or require maintenance. In contrast, SageMaker Canvas can generate reliable predictions even when faced with gaps in sensor data. This resilience enables continuous operation of air quality monitoring networks despite inevitable sensor failures or maintenance periods, eliminating costly downtime and data gaps. Environmental agencies and public health officials benefit from uninterrupted access to critical air quality information, enabling timely pollution alerts and more comprehensive long-term analysis of air quality trends. By maintaining operational continuity even with imperfect data inputs, SageMaker Canvas significantly enhances the reliability and practical utility of environmental monitoring systems.\nIn this post, we provide a data imputation solution using Amazon SageMaker AI, AWS Lambda, and AWS Step Functions. This solution is designed for environmental analysts, public health officials, and business intelligence professionals who need reliable PM2.5 data for trend analysis, reporting, and decision-making. We sourced our sample training dataset from openAFRICA. Our solution predicts PM2.5 values using time-series forecasting. The sample training dataset contained over 15 million records from March 2022 to Oct 2022 in various parts of Kenya and Nigeria—data coming from 23 sensor devices from 15 unique locations. The sample code and workflows can be adapted to create prediction models for your PM2.5 datasets. See our solution’s README for detailed instructions.\nSolution overview\nThe solution consists of two main ML components: a training workflow and an inference workflow. These workflows are built using the following services:\n- SageMaker Canvas is used to prepare data and train the prediction model through its no-code interface\n- Batch Transform for inference with Amazon SageMaker AI is used for inference, processing the dataset in bulk to generate predictions\n- Step Functions orchestrates the inferencing process by coordinating the workflow between data retrieval, batch transforming, and database updates, managing workflow state transitions, and making sure that data flows properly through each processing stage\n- Lambda functions perform critical operations at each workflow step: retrieving sensor data from the database in required format, transforming data for model input, sending batches to SageMaker for inferencing, and updating the database with prediction results after processing is complete\nAt a high level, the solution works by taking a set of PM2.5 data with gaps and predicts the missing values within the range of plus or minus 4.875 micrograms per cubic meter of the actual PM2.5 concentration. It does this by first training a model on the data using inputs for the specific schema and a historical set of values from the user to guide the training process, which is completed with SageMaker Canvas. After the model is trained on a representative dataset and schema, SageMaker Canvas exports the model for use with batch processing. The Step Functions orchestration calls a Lambda function every 24 hours that takes a dataset of new sensor data that has gaps and initiates a SageMaker batch transform job to predict the missing values. The batch transform job processes the entire dataset at once, and the Lambda function then updates the existing dataset with the results. The new completed dataset with predicted values can now be distributed to public health decision-makers who need complete datasets to effectively analyze the patterns of PM2.5 data.\nWe dive into each of these steps in later sections of this post.\nSolution walkthrough\nThe following diagram shows our solution architecture:\nLet’s explore the architecture step by step:\n- To systematically collect, identify, and fill PM2.5 data gaps caused by sensor limitations and connectivity issues, Amazon EventBridge Scheduler invokes a Step Functions state machine every 24 hours. Step Functions orchestrates the calling of various Lambda functions to perform different steps without handling the complexities of error handling, retries, and state management, providing a serverless workflow that seamlessly coordinates the PM2.5 data imputation process.\n- The State Machine invokes a Lambda function in your Amazon Virtual Private Cloud (Amazon VPC) that retrieves records containing missing air quality values from the user’s air quality database on Amazon Aurora PostgreSQL-Compatible Edition and stores the records in a CSV file in an Amazon Simple Storage Service (Amazon S3) bucket.\n- The State Machine then runs a Lambda function that retrieves the records from Amazon S3 and initiates the SageMaker batch transform job in your VPC using your SageMaker model created from your SageMaker Canvas predictive model trained on historical PM2.5 data.\n- To streamline the batch transform workflow, this solution uses an event-driven approach with EventBridge and Step Functions. EventBridge captures completion events from SageMaker batch transform jobs, while the task token functionality of Step Functions enables extended waiting periods beyond the time limits of Lambda. After processing completes, SageMaker writes the prediction results directly to an S3 bucket.\n- The final step in the state machine retrieves the predicted values from the S3 bucket and then updates the database in Aurora PostgreSQL-Compatible with the values including a\npredicted\nlabel set totrue\n.\nPrerequisites\nTo implement the PM2.5 data imputation solution, you must have the following:\n- An AWS account with AWS Identity and Access Management (IAM) permissions sufficient to deploy the solution and interact with the database.\n- The following AWS services:\n- Amazon SageMaker AI\n- AWS Lambda\n- AWS Step Functions\n- Amazon S3\n- Aurora PostgreSQL-Compatible\n- Amazon CloudWatch\n- AWS CloudFormation\n- Amazon Virtual Private Cloud (VPC)\n- Amazon EventBridge\n- IAM for authentication to Aurora PostgreSQL-Compatible\n- AWS Systems Manager Parameter Store\n- A local desktop set up with AWS Command Line Interface (AWS CLI) version 2, Python 3.10, AWS Cloud Development Kit (AWS CDK) v2.x, and Git version 2.x.\n- The AWS CLI set up with the necessary credentials in the desired AWS Region.\n- Historical air quality sensor data. Note that our solution requires a fixed schema described in the GitHub repo’s README.\nDeploy the solution\nYou will run the following steps to complete the deployment:\n- Prepare your environment by building Python modules locally for Lambda layers, deploying infrastructure using the AWS CDK, and initializing your Aurora PostgreSQL database with sensor data.\n- Perform steps in the Build your air quality prediction model section to configure a SageMaker Canvas application, followed by training and registering your model in Amazon SageMaker Model Registry.\n- Create SageMaker model using your registered SageMaker Canvas model by updating infrastructure using the AWS CDK.\n- Manage future configuration changes using the AWS CDK.\nStep 1: Deploy AWS infrastructure and upload air quality sensor data\nComplete the following steps to deploy the PM2.5 data imputation solution AWS Infrastructure and upload air quality sensor data to Amazon Aurora RDS:\n- Clone the repository to your local desktop environment using the following command:\n- Change to the project directory:\ncd <BASE_PROJECT_FOLDER>\n- Follow the deployment steps in the README file up to Model Setup for Batch Transform Inference.\nStep 2: Build your air quality prediction model\nAfter you create the SageMaker AI domain and the SageMaker AI user profile as part of the CDK deployment steps, follow these steps to build your air quality prediction model\nConfigure your SageMaker Canvas application\n- On the AWS Management Console, go to the SageMaker AI console and select the domain and the user profile that was created under Admin, Configurations, and Domains.\n- Choose the App Configurations tab, scroll down to the Canvas section, and select Edit.\n- In Canvas storage configuration, select Encryption and select the dropdown for aws/s3.\n- In the ML Ops Configuration, turn on the option to Enable Model Registry registration permissions for this user profile.\n- Optionally, in the Local file upload configuration section in your domain’s Canvas App Configuration, you can turn on Enable local file upload.\n- Choose Submit to save your configuration choices.\n- In your Amazon SageMaker AI home page, go to the Applications and IDEs section and select Canvas.\n- Select the SageMaker AI user profile that was created for you by the CDK deployment and choose Open Canvas.\n- In a new tab, SageMaker Canvas will start creating your application. This takes a few minutes.\nCreate and register your prediction model\nIn this phase, you develop a prediction model using your historical air quality sensor data.\nThe preceding architecture diagram illustrates the end-to-end process for training the SageMaker Canvas prediction model, registering that model and creating a SageMaker model for running inference on newly found PM2.5 data gaps. The training process starts by extracting air quality sensor dataset from the database. The dataset is imported into SageMaker Canvas for predictive analysis. This training dataset is transformed and prepared through data wrangling steps implemented by SageMaker Canvas for building and training ML models.\nPrepare data\nOur solution supports a SageMaker Canvas model trained for a single-target variable prediction based on historical data and performs corresponding data imputation for PM2.5 data gaps. To train your model for predictive analysis, follow the comprehensive End to End Machine Learning workflow in the AWS Canvas Immersion Day workshop, adapting each step to prepare your air quality sensor dataset. Begin with the standard workflow until you reach the data preparation section. Here, you can make several customizations:\n- Filter dataset for single-target value prediction: Your air quality dataset might contain multiple sensor parameters. For single-target value prediction using this solution, filter the dataset to include only\nPM2.5\nmeasurements. - Clean sensor data: Remove records containing sensor fault values. For example, we filtered out values that equal 65535, because 65535 is a common error code for malfunctioning sensors. Adjust this filtering based on the specific error codes your air quality monitoring equipment produces.\nThe following image shows our data wrangling Data Flow implemented using above guidance:\nData Wrangler > Data Flow\n- Review generated insights and remove irrelevant data: Review the SageMaker Canvas generated insights and analyses. Evaluate them based on time-series forecasting and geospatial temporal data for air quality patterns and relationships between other columns of impact. See chosen columns of impact in GitHub for guidance. Analyze your dataset to identify rows and columns that impact the prediction and remove data that can reduce prediction accuracy.\nThe following image shows our data wrangling Analyses obtained with implementing the above guidance:\nData Wrangler > Analyses\nTraining your prediction model\nAfter completing your data preparation, proceed to the Train the Model section of the workshop and continue with these specifications:\n- Select problem type: Select Predictive Analysis as your ML approach. Because our dataset is tabular and contains a timestamp, a target column that has values we’re using to forecast future values, and a device ID column, SageMaker Canvas will choose time series forecasting.\n- Define target column: Set Value as your target column for predicting PM2.5 values.\n- Build configuration: Use the Standard Build option for model training because it generally has a higher accuracy. See What happens when you build a model in How custom models work for more information.\nBy following these steps, you can create a model optimized for PM2.5 dataset predictive analysis, capable of generating valuable insights. Note that SageMaker Canvas supports retraining the ML model for updated PM2.5 datasets.\nEvaluate the model\nAfter training your model, proceed to Evaluate the model and review column impact, root mean square error (RMSE) score and other advanced metrics to understand your model’s performance for generating predictions for PM2.5.\nThe following image shows our model evaluation statistics achieved.\nAdd the model to the registry\nOnce you are satisfied with your model performance, follow the steps in Register a model version to the SageMaker AI model registry. Make sure to change the approval status to Approved\nbefore continuing to run this solution. At the time of this post’s publication, the approval must be updated in Amazon SageMaker Studio.\nLog out of SageMaker Canvas\nAfter completing your work in SageMaker Canvas, you can log out or configure your application to automatically terminate the workspace instance. A workspace instance is dedicated for your use every time you launch a Canvas application, and you are billed for as long as the instance runs. Logging out or terminating the workspace instance stops the workspace instance billing. For more information, see billing and cost in SageMaker Canvas.\nStep 3: Create a SageMaker model using your registered SageMaker Canvas model\nIn the previous steps, you created a SageMaker domain and user profile through CDK deployment (Step 1) and successfully registered your model (Step 2). Now, it’s time to create the SageMaker model in your VPC using the SageMaker Canvas model you registered. Follow Model Setup for Batch Inference and Re-Deploy with Updated Configuration sections in the code README for creating SageMaker model.\nStep 4: Manage future configuration changes\nThe same deployment pattern applies to any future configuration modifications you might require, including:\n- Batch transform instance type optimizations\n- Transform job scheduling changes\nUpdate the relevant parameters in your configuration and run cdk deploy\nto propagate these changes throughout your solution architecture.\nFor a comprehensive list of configurable parameters and their default values, see the configuration file in the repository.\nExecute cdk deploy\nagain to update your infrastructure stack with the your model ID for batch transform operations, replacing the placeholder value initially deployed. This infrastructure-as-code approach helps ensure consistent, version-controlled updates to your data imputation workflow.\nSecurity best practices\nSecurity and compliance is a shared responsibility between AWS and the customer, as outlined in the Shared Responsibility Model. We encourage you to review this model for a comprehensive understanding of the respective responsibilities.\nIn this solution, we enhanced security by implementing encryption at rest for Amazon S3, Aurora PostgreSQL-Compatible database, and the SageMaker Canvas application. We also enabled encryption in transit by requiring SSL/TLS for all connections from the Lambda functions. We implemented secure database access by providing temporary dynamic credentials through IAM authentication for Amazon RDS, eliminating the need for static passwords. Each Lambda function operates with least privilege access, receiving only the minimal permissions required for its specific function. Finally, we deployed the Lambda functions, Aurora PostgreSQL-Compatible instance, and SageMaker Batch Transform jobs in private subnets of the VPC that do not traverse the public internet. This private network architecture is enabled through VPC endpoints for Amazon S3, SageMaker AI, and AWS Secrets Manager.\nResults\nAs shown in the following image, our model, developed using SageMaker Canvas, predicts PM2.5 values with an R-squared of 0.921. Because ML models for PM2.5 prediction frequently achieve R-squared values between 0.80 and 0.98 (see this example from ScienceDirect), our solution is within the range of higher-performing PM2.5 prediction models available today. SageMaker Canvas delivers this performance through its no-code experience, automatically handling model training and optimization without requiring ML expertise from users.\nClean up\nComplete the following steps to clean up your resources:\n- SageMaker Canvas application cleanup:\n- On the go to the SageMaker AI console and select the domain that was created under Admin Configurations, and Domains.\n- Select the user created under User Profiles for that domain.\n- On the User Details page, navigate to Spaces and Apps, and choose Delete to manually delete your SageMaker AI canvas application and clean up resources.\n- SageMaker Domain EFS storage cleanup:\n- Open Amazon EFS and in File systems, delete filesystem tagged as\nManagedByAmazonSageMakerResource\n. - Open VPC and under Security, navigate to Security groups.\n- On Security groups, select security-group-for-inbound-nfs-<your-sagemaker-domain-id> and delete all Inbound rules associated with that group.\n- On Security groups, select security-group-for-outbound-nfs-<your-sagemaker-domain-id> and delete all associated Outbound rules.\n- Finally, delete both the security groups: security-group-for-inbound-nfs-<your-sagemaker-domain-id> and security-group-for-outbound-nfs-<your-sagemaker-domain-id>.\n- Open Amazon EFS and in File systems, delete filesystem tagged as\n- Use the AWS CDK to clean up the remaining AWS resources:\n- After the preceding steps are complete, return to your local desktop environment where the GitHub repo was cloned, and change to the project’s infra directory:\ncd <BASE_PROJECT_FOLDER>/infra\n- Destroy the resources created with AWS CloudFormation using the AWS CDK:\ncdk destroy\n- Monitor the AWS CDK process deleting resources created by the solution. If there are any errors, troubleshoot using the CloudFormation console and then retry deletion.\n- After the preceding steps are complete, return to your local desktop environment where the GitHub repo was cloned, and change to the project’s infra directory:\nConclusion\nThe development of accurate PM2.5 prediction models has traditionally required extensive technical expertise, presenting significant challenges for public health researchers studying air pollution’s impact on disease outcomes. From data preprocessing and feature engineering to model selection and hyperparameter tuning, these technical requirements diverted substantial time and effort away from researchers’ core"}
{"title": "How Amazon Finance built an AI assistant using Amazon Bedrock and Amazon Kendra to support analysts for data discovery and business insights", "url": "https://aws.amazon.com/blogs/machine-learning/how-amazon-finance-built-an-ai-assistant-using-amazon-bedrock-and-amazon-kendra-to-support-analysts-for-data-discovery-and-business-insights/", "source": "AWS Machine Learning Blog", "published": "2025-08-28T20:14:22+00:00", "text": "Artificial Intelligence\nHow Amazon Finance built an AI assistant using Amazon Bedrock and Amazon Kendra to support analysts for data discovery and business insights\nFinance analysts across Amazon Finance face mounting complexity in financial planning and analysis processes. When working with vast datasets spanning multiple systems, data lakes, and business units, analysts encounter several critical challenges. First, they spend significant time manually browsing data catalogs and reconciling data from disparate sources, leaving less time for valuable analysis and insight generation. Second, historical data and previous business decisions often reside in various documents and legacy systems, making it difficult to use past learnings during planning cycles. Third, as business contexts rapidly evolve, analysts need quick access to relevant metrics, planning assumptions, and financial insights to support data-driven decision-making.\nTraditional tools and processes fall short in addressing these challenges. Keyword-based searches often miss contextual relationships in financial data, and rigid query structures limit analysts’ ability to explore data dynamically. Furthermore, the lack of institutional knowledge preservation means valuable insights and decision rationales often remain siloed or get lost over time, leading to redundant analysis and inconsistent planning assumptions across teams. These challenges significantly impact financial planning efficiency, decision-making agility, and the overall quality of business insights. Analysts needed a more intuitive way to access, understand, and use their organization’s collective financial knowledge and data assets.\nThe Amazon Finance technical team develops and manages comprehensive technology solutions that power financial decision-making and operational efficiency while standardizing across Amazon’s global operations. In this post, we explain how the team conceptualized and implemented a solution to these business challenges by harnessing the power of generative AI using Amazon Bedrock and intelligent search with Amazon Kendra.\nSolution overview\nTo address these business challenges, Amazon Finance developed an AI-powered assistant solution that uses generative AI and enterprise search capabilities. This solution helps analysts interact with financial data sources and documentation through natural language queries, minimizing the need for complex manual searches across multiple systems. The assistant accesses a comprehensive knowledge base of financial documents, historical data, and business context, providing relevant and accurate responses while maintaining enterprise security standards. This approach not only streamlines data discovery but also preserves institutional knowledge and enables more consistent decision-making across the organization.\nThe AI assistant’s methodology consists of two key solution components: intelligent retrieval and augmented generation. The retrieval system uses vector stores, which are specialized databases that efficiently store and search high-dimensional representations of text meanings. Unlike traditional databases that rely on keyword matching, vector stores enable semantic search by converting user queries into vector representations and finding similar vectors in the database. Building on this retrieval foundation, the system employs augmented generation to create accurate and contextual responses. This approach enhances traditional language models by incorporating external knowledge sources during response generation, significantly reducing hallucinations and improving factual accuracy. The process follows three steps: retrieving relevant information from knowledge sources using semantic search, conditioning the language model with this context, and generating refined responses that incorporate the retrieved information. By combining these technologies, the assistant delivers responses that are both contextually appropriate and grounded in verified organizational knowledge, making it particularly effective for knowledge-intensive applications like financial operations and planning.\nWe implemented this Retrieval Augmented Generation (RAG) system through a combination of large language models (LLMs) on Amazon Bedrock and intelligent search using Amazon Kendra.\nIn the following sections, we discuss the key architectural components that we used in the solution and describe how the overall solution works.\nAmazon Bedrock\nWe chose Anthropic’s Claude 3 Sonnet, a powerful language model, for its exceptional language generation capabilities and ability to understand and reason complex topics. By integrating Anthropic’s Claude into the RAG module through Amazon Bedrock, the AI assistant can generate contextual and informative responses that seamlessly combine the retrieved knowledge from the vector store with the model’s natural language processing and generation abilities, resulting in a more human-like and engaging conversational experience.\nAmazon Kendra (Enterprise Edition Index)\nAmazon Kendra offers powerful natural language processing for AI assistant applications. It excels at understanding user questions and finding relevant answers through semantic search. The service works smoothly with generative AI models, particularly in RAG solutions. The enterprise security features in Amazon Kendra support data protection and compliance. Its ability to understand user intent and connect directly with Amazon Bedrock makes it ideal for business assistants. This helps create meaningful conversations using business documents and data catalogs.\nWe chose Amazon Kendra Enterprise Edition Index over Amazon OpenSearch Service, primarily due to its sophisticated built-in capabilities and reduced need for manual configuration. Whereas OpenSearch Service requires extensive customization and technical expertise, Amazon Kendra provides out-of-the-box natural language understanding, automatic document processing for over 40 file formats, pre-built enterprise connectors, and intelligent query handling including synonym recognition and refinement suggestions. The service combines keyword, semantic, and vector search approaches automatically, whereas OpenSearch Service requires manual implementation of these features. These features of Amazon Kendra were suitable for our finance domain use case, where accuracy is imperative for usability.\nWe also chose Amazon Kendra Enterprise Edition Index over Amazon Q Business for information retrieval, because it stands out as a more robust and flexible solution. Although both tools aim to streamline access to company information, Amazon Kendra offers superior retrieval accuracy and greater control over search parameters. With Amazon Kendra, you can fine-tune relevance tuning, customize document attributes, and implement custom synonyms to enhance search precision. This level of customization helped us tailor the search experience to our specific needs in the Amazon Finance domain and monitor the search results prior to the augmented generation step within user conversations.\nStreamlit\nWe selected Streamlit, a Python-based framework for creating interactive web applications, for building the AI assistant’s UI due to its rapid development capabilities, seamless integration with Python and the assistant’s backend components, interactive and responsive UI components, potential for data visualization, and straightforward deployment options. With the Streamlit UI, the assistant provides a user-friendly and engaging interface that facilitates natural language interactions while allowing for efficient iteration and deployment of the application.\nPrompt template\nPrompt templates allow for formatting user queries, integrating retrieved knowledge, and providing instructions or constraints for response generation, which are essential for generating contextual and informative responses that combine the language generation abilities of Anthropic’s Claude with the relevant knowledge retrieved from the search powered by Amazon Kendra. The following is an example prompt:\nSolution architecture\nThe following solution architecture diagram depicts how the key architectural components work with each other to power the solution.\nThe workflow consists of the following steps:\n- The user asks the question in a chat box after authentication.\n- The Streamlit application sends the query to an Amazon Kendra retriever for relevant document retrieval.\n- Amazon Kendra sends the relevant paragraph and document references to the RAG solution.\n- The RAG solution uses Anthropic’s Claude in Amazon Bedrock along with the prompt template and relevant paragraph as context.\n- The LLM response is sent back to the Streamlit UI.\n- The response is shown to the user along with the feedback feature and session history.\n- The user feedback on responses is stored separately in Amazon Simple Storage Service (Amazon S3)\n- Amazon Kendra indexes relevant documents stored in S3 buckets for document search and retrieval.\nFrontend architecture\nWe designed the following frontend architecture to allow for rapid modifications and deployment, keeping in mind the scalability and security of the solution.\nThis workflow consists of the following steps:\n- The user navigates to the application URL in their browser.\n- Amazon Route 53 resolves their request to the Amazon CloudFront distribution, which then selects the server closest to the user (to minimize latency).\n- CloudFront runs an AWS Lambda function that makes sure the user has been authenticated. If not, the user is redirected to sign in. After they successfully sign in, they are redirected back to the application website. The flow repeats, and CloudFront triggers the Lambda function again. This time, the user is now able to access the website.\n- Now authenticated, CloudFront returns the assets of the web application.\n- AWS Fargate makes it possible to run containers without having to manage the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances. This allows running containers as a true serverless service. Amazon Elastic Container Service (Amazon ECS) is configured with automatic scaling (target tracking automatic scaling, which scales based on the Application Load Balancer (ALB) requests per target).\nEvaluation of the solution’s performance\nWe implemented a comprehensive evaluation framework to rigorously assess the AI assistant’s performance and make sure it meets the high standards required for financial applications. Our framework was designed to capture both quantitative metrics for measurable performance and qualitative indicators for user experience and response quality. During our benchmarking tests with analysts, we found that this solution dramatically reduced search time by 30% because analysts can now perform natural language search, and it improved the accuracy of search results by 80%.\nQuantitative assessment\nWe focused primarily on precision and recall testing, creating a diverse test set of over 50 business queries that represented typical use cases our analysts encounter. Using human-labeled answers as our ground truth, we evaluated the system’s performance across two main categories: data discovery and knowledge search. In data discovery scenarios, where the system helps analysts locate specific data sources and metrics, we achieved an initial precision rate of 65% and a recall rate of 60% without performing metadata enrichment on the data sources. Although these rates might appear moderate, they represent a significant improvement over the previous manual search process, which had an estimated success rate of only 35% and often required multiple iterations across different systems. The primary reasons for the current rates of the new system were attributed to the lack of rich metadata about data sources and was a good indicator for teams to facilitate better metadata collection of data assets, which is currently underway.\nThe knowledge search capability demonstrated initial rates of 83% precision and 74% recall without performing metadata enrichment on data sources. This marked a substantial improvement over traditional keyword-based search methods, which typically achieved only 45–50% precision in our internal testing. This improvement is particularly meaningful because it translates to analysts finding the right information in their first search attempt roughly 8 out of 10 times, compared to the previous average of 3–4 attempts needed to locate the same information.\nQualitative metrics\nThe qualitative evaluation centered around the concept of faithfulness—a critical metric for financial applications where accuracy and reliability are paramount. We employed an innovative LLM-as-a-judge methodology to evaluate how well the AI assistant’s responses aligned with source documentation and avoided hallucinations or unsupported assertions. The results showed a marked difference between use cases: data discovery achieved a faithfulness score of 70%, and business knowledge search demonstrated an impressive 88% faithfulness. These scores significantly outperform our previous documentation search system, which had no built-in verification mechanism and often led to analysts working with outdated or incorrect information.\nMost importantly, the new system reduced the average time to find relevant information from 45–60 minutes to just 5–10 minutes—an 85% improvement in efficiency. User satisfaction surveys indicate that 92% of analysts prefer the new system over traditional search methods, citing improved accuracy and time savings as key benefits.\nThese evaluation results have not only validated our approach but also highlighted specific areas for future enhancement. We continue to refine our evaluation framework as the system evolves, making sure it maintains high standards of accuracy and reliability while meeting the dynamic needs of our financial analysts. The evaluation framework was instrumental in building confidence within our business user community, providing transparent metrics that demonstrate the system’s capability to handle complex financial queries while maintaining the accuracy standards essential for financial operations.\nUse cases\nOur solution transforms how finance users interact with complex financial and operational data through natural language queries. In this section, we discuss some key examples demonstrating how the system simplifies data discovery.\nSeamless data discovery\nThe solution enables users to find data sources through natural language queries rather than requiring technical knowledge of database structures. It uses a sophisticated combination of vector stores and enterprise search capabilities to match user questions with relevant data sources, though careful attention must be paid to context management and preventing over-reliance on previous interactions. Prior to the AI assistant solution, finance analysts needed deep technical knowledge to navigate complex database structures, often spending hours searching through multiple documentation sources just to locate specific data tables. Understanding system workflows required extensive review of technical documentation or reaching out to subject matter experts, creating bottlenecks and reducing productivity. Even experienced users struggled to piece together complete information about business processes from fragmented sources across different systems. Now, analysts can simply ask questions in natural language, such as “Where can I find productivity metrics?”, “How do I access facility information?”, or “Which dashboard shows operational data?” and receive precise, contextual answers. The solution combines enterprise search capabilities with LLMs to understand user intent and deliver relevant information from both structured and unstructured data sources. Analysts now receive accurate directions to specific consolidated reporting tables, clear explanations of business processes, and relevant technical details when needed. In our benchmark tests, for data discovery tasks alone, the system achieved 70% faithfulness and 65% precision, and document search demonstrated even stronger results with 83% precision and 88% faithfulness, without metadata enrichments.\nAssisting understanding of internal business processes from knowledge documentation\nFinancial analysts previously faced a steep learning curve when working with enterprise planning tools. The complexity of these systems meant that even basic tasks required extensive documentation review or waiting for support from overwhelmed subject matter experts. New team members could take weeks or months to become proficient, while even experienced users struggled to keep up with system updates and changes. This created a persistent bottleneck in financial operations and planning processes. The introduction of the AI-powered assistant has fundamentally changed how analysts learn and interact with these planning tools. Rather than searching through hundreds of pages of technical documentation, analysts can now ask straightforward questions like “How do I forecast depreciation for new assets?”, “How does the quarterly planning process work?” or “What inputs are needed for the quarterly planning cycle?” The system provides clear, contextualized explanations drawn from verified documentation and system specifications. Our benchmark tests revealed that it achieved 83% precision and 88% faithfulness in retrieving and explaining technical and business information. New analysts can become productive in a matter of weeks, experienced users can quickly verify procedures, and subject matter experts can focus on more complex challenges rather than routine questions. This represents a significant advancement in making enterprise systems more accessible and efficient, while maintaining the accuracy and reliability required for financial operations.While the technology continues to evolve, particularly in handling nuanced queries and maintaining comprehensive coverage of system updates, it has already transformed the way teams interact with planning tools independently.\nConclusion\nThe AI-powered assistant solution discussed in this post has demonstrated significant improvements in data discovery and business insights generation, delivering multiple key benefits across Amazon Finance. Analysts can now quickly find relevant information through natural language queries, dramatically reducing search time. The system’s ability to synthesize insights from disparate data sources has notably enhanced data-driven decision-making, and its conversational interface and contextual responses promote self-service data exploration, effectively reducing the burden on centralized data teams.\nThis innovative AI assistant solution showcases the practical power of AWS generative AI in transforming enterprise data discovery and document search. By combining Amazon Kendra Enterprise Edition Index, Amazon Bedrock, and advanced LLMs, the implementation achieves impressive precision rates, proving that sophisticated AI-powered search is both achievable and effective. This success demonstrates how AWS generative AI services can meet current business needs while promoting future innovations in enterprise search. These services provide a strong foundation for organizations looking to enhance data discovery processes using natural language to support intelligent enterprise applications. To learn more about implementing AI-powered search solutions, see Build and scale the next wave of AI innovation on AWS and explore AWS AI use cases.\nAbout the authors\nSaikat Gomes is part of the Customer Solutions team in Amazon Web Services. He is passionate about helping enterprises succeed and realize benefits from cloud adoption. He is a strategic advisor to his customers for large-scale cloud transformations involving people, process, and technology. Prior to joining AWS, he held multiple consulting leadership positions and led large- scale"}
{"title": "Mercury foundation models from Inception Labs are now available in Amazon Bedrock Marketplace and Amazon SageMaker JumpStart", "url": "https://aws.amazon.com/blogs/machine-learning/mercury-foundation-models-from-inception-labs-are-now-available-in-amazon-bedrock-marketplace-and-amazon-sagemaker-jumpstart/", "source": "AWS Machine Learning Blog", "published": "2025-08-27T15:39:06+00:00", "text": "Artificial Intelligence\nMercury foundation models from Inception Labs are now available in Amazon Bedrock Marketplace and Amazon SageMaker JumpStart\nToday, we are excited to announce that Mercury and Mercury Coder foundation models (FMs) from Inception Labs are available through Amazon Bedrock Marketplace and Amazon SageMaker JumpStart. With this launch, you can deploy the Mercury FMs to build, experiment, and responsibly scale your generative AI applications on AWS.\nIn this post, we demonstrate how to get started with Mercury models on Amazon Bedrock Marketplace and SageMaker JumpStart.\nAbout Mercury foundation models\nMercury is the first family of commercial-scale diffusion-based language models, offering groundbreaking advancements in generation speed while maintaining high-quality outputs. Unlike traditional autoregressive models that generate text one token at a time, Mercury models use diffusion to generate multiple tokens in parallel through a coarse-to-fine approach, resulting in dramatically faster inference speeds. Mercury Coder models deliver the following key features:\n- Ultra-fast generation speeds of up to 1,100 tokens per second on NVIDIA H100 GPUs, up to 10 times faster than comparable models\n- High-quality code generation across multiple programming languages, including Python, Java, JavaScript, C++, PHP, Bash, and TypeScript\n- Strong performance on fill-in-the-middle tasks, making them ideal for code completion and editing workflows\n- Transformer-based architecture, providing compatibility with existing optimization techniques and infrastructure\n- Context length support of up to 32,768 tokens out of the box and up to 128,000 tokens with context extension approaches\nAbout Amazon Bedrock Marketplace\nAmazon Bedrock Marketplace plays a pivotal role in democratizing access to advanced AI capabilities through several key advantages:\n- Comprehensive model selection – Amazon Bedrock Marketplace offers an exceptional range of models, from proprietary to publicly available options, so organizations can find the perfect fit for their specific use cases.\n- Unified and secure experience – By providing a single access point for models through the Amazon Bedrock APIs, Amazon Bedrock Marketplace significantly simplifies the integration process. Organizations can use these models securely, and for models that are compatible with the Amazon Bedrock Converse API, you can use the robust toolkit of Amazon Bedrock, including Amazon Bedrock Agents, Amazon Bedrock Knowledge Bases, Amazon Bedrock Guardrails, and Amazon Bedrock Flows.\n- Scalable infrastructure – Amazon Bedrock Marketplace offers configurable scalability through managed endpoints, so organizations can select their desired number of instances, choose appropriate instance types, define custom automatic scaling policies that dynamically adjust to workload demands, and optimize costs while maintaining performance.\nDeploy Mercury and Mercury Coder models in Amazon Bedrock Marketplace\nAmazon Bedrock Marketplace gives you access to over 100 popular, emerging, and specialized foundation models through Amazon Bedrock. To access the Mercury models in Amazon Bedrock, complete the following steps:\n- On the Amazon Bedrock console, in the navigation pane under Foundation models, choose Model catalog.\nYou can also use the Converse API to invoke the model with Amazon Bedrock tooling.\n- On the Model catalog page, filter for Inception as a provider and choose the Mercury model.\nThe Model detail page provides essential information about the model’s capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration.\n- To begin using the Mercury model, choose Subscribe.\n- On the model detail page, choose Deploy.\nYou will be prompted to configure the deployment details for the model. The model ID will be prepopulated.\n- For Endpoint name, enter an endpoint name (between 1–50 alphanumeric characters).\n- For Number of instances, enter a number of instances (between 1–100).\n- For Instance type, choose your instance type. For optimal performance with Nemotron Super, a GPU-based instance type like ml.p5.48xlarge is recommended.\n- Optionally, you can configure advanced security and infrastructure settings, including virtual private cloud (VPC) networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, you might want to review these settings to align with your organization’s security and compliance requirements.\n- Choose Deploy to begin using the model.\nWhen the deployment is complete, you can test its capabilities directly in the Amazon Bedrock playground.This is an excellent way to explore the model’s reasoning and text generation abilities before integrating it into your applications. The playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results. You can use these models with the Amazon Bedrock Converse API.\nSageMaker JumpStart overview\nSageMaker JumpStart is a fully managed service that offers state-of-the-art FMs for various use cases such as content writing, code generation, question answering, copywriting, summarization, classification, and information retrieval. It provides a collection of pre-trained models that you can deploy quickly, accelerating the development and deployment of ML applications. One of the key components of SageMaker JumpStart is model hubs, which offer a vast catalog of pre-trained models, such as Mistral, for a variety of tasks.\nYou can now discover and deploy Mercury and Mercury Coder in Amazon SageMaker Studio or programmatically through the SageMaker Python SDK, and derive model performance and MLOps controls with Amazon SageMaker AI features such as Amazon SageMaker Pipelines, Amazon SageMaker Debugger, or container logs. The model is deployed in a secure AWS environment and in your VPC, helping support data security for enterprise security needs.\nPrerequisites\nTo deploy the Mercury models, make sure you have access to the recommended instance types based on the model size. To verify you have the necessary resources, complete the following steps:\n- On the Service Quotas console, under AWS Services, choose Amazon SageMaker.\n- Check that you have sufficient quota for the required instance type for endpoint deployment.\n- Make sure at least one of these instance types is available in your target AWS Region.\n- If needed, request a quota increase and contact your AWS account team for support.\nMake sure your SageMaker AWS Identity and Access Management (IAM) service role has the necessary permissions to deploy the model, including the following permissions to make AWS Marketplace subscriptions in the AWS account used:\naws-marketplace:ViewSubscriptions\naws-marketplace:Unsubscribe\naws-marketplace:Subscribe\nAlternatively, confirm your AWS account has a subscription to the model. If so, you can skip the following deployment instructions and start with subscribing to the model package.\nSubscribe to the model package\nTo subscribe to the model package, complete the following steps:\n- Open the model package listing page and choose Mercury or Mercury Coder.\n- On the AWS Marketplace listing, choose Continue to subscribe.\n- On the Subscribe to this software page, review and choose Accept Offer if you and your organization agree with the EULA, pricing, and support terms.\n- Choose Continue to proceed with the configuration and then choose a Region where you have the service quota for the desired instance type.\nA product Amazon Resource Name (ARN) will be displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3.\nDeploy Mercury and Mercury Coder models on SageMaker JumpStart\nFor those new to SageMaker JumpStart, you can use SageMaker Studio to access the Mercury and Mercury Coder models on SageMaker JumpStart.\nDeployment starts when you choose the Deploy option. You might be prompted to subscribe to this model through Amazon Bedrock Marketplace. If you are already subscribed, choose Deploy. After deployment is complete, you will see that an endpoint is created. You can test the endpoint by passing a sample inference request payload or by selecting the testing option using the SDK.\nDeploy Mercury using the SageMaker SDK\nIn this section, we walk through deploying the Mercury model through the SageMaker SDK. You can follow a similar process for deploying the Mercury Coder model as well.\nTo deploy the model using the SDK, copy the product ARN from the previous step and specify it in the model_package_arn\nin the following code:\nDeploy the model:\nUse Mercury for code generation\nLet’s try asking the model to generate a simple tic-tac-toe game:\nWe get the following response:\nFrom the preceding response, we can see that the Mercury model generated a complete, functional tic-tac-toe game with minimax AI implementation at 528 tokens per second, delivering working HTML, CSS, and JavaScript in a single response. The code includes proper game logic, an unbeatable AI algorithm, and a clean UI with the specified requirements correctly implemented. This demonstrates strong code generation capabilities with exceptional speed for a diffusion-based model.\nUse Mercury for tool use and function calling\nMercury models support advanced tool use capabilities, enabling them to intelligently determine when and how to call external functions based on user queries. This makes them ideal for building AI agents and assistants that can interact with external systems, APIs, and databases.\nLet’s demonstrate Mercury’s tool use capabilities by creating a travel planning assistant that can check weather and perform calculations:\nExpected response:\nAfter receiving the tool results, you can continue the conversation to get a natural language response:\nExpected response:\nClean up\nTo avoid unwanted charges, complete the steps in this section to clean up your resources.\nDelete the Amazon Bedrock Marketplace deployment\nIf you deployed the model using Amazon Bedrock Marketplace, complete the following steps:\n- On the Amazon Bedrock console, in the navigation pane, under Foundation models, choose Marketplace deployments.\n- Select the endpoint you want to delete, and on the Actions menu, choose Delete.\n- Verify the endpoint details to make sure you’re deleting the correct deployment:\n- Endpoint name\n- Model name\n- Endpoint status\n- Choose Delete to delete the endpoint.\n- In the Delete endpoint confirmation dialog, review the warning message, enter\nconfirm\n, and choose Delete to permanently remove the endpoint.\nDelete the SageMaker JumpStart endpoint\nThe SageMaker JumpStart model you deployed will incur costs if you leave it running. Use the following code to delete the endpoint if you want to stop incurring charges. For more details, see Delete Endpoints and Resources.\nConclusion\nIn this post, we explored how you can access and deploy Mercury models using Amazon Bedrock Marketplace and SageMaker JumpStart. With support for both Mini and Small parameter sizes, you can choose the optimal model size for your specific use case. Visit SageMaker JumpStart in SageMaker Studio or Amazon Bedrock Marketplace to get started. For more information, refer to Use Amazon Bedrock tooling with Amazon SageMaker JumpStart models, Amazon SageMaker JumpStart Foundation Models, Getting started with Amazon SageMaker JumpStart, Amazon Bedrock Marketplace, and SageMaker JumpStart pretrained models.\nThe Mercury family of diffusion-based large language models offers exceptional speed and performance, making it a powerful choice for your generative AI workloads with latency-sensitive requirements.\nAbout the authors\nNiithiyn Vijeaswaran is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is AWS AI accelerators (AWS Neuron). He holds a Bachelor’s degree in Computer Science and Bioinformatics.\nJohn Liu has 15 years of experience as a product executive and 9 years of experience as a portfolio manager. At AWS, John is a Principal Product Manager for Amazon Bedrock. Previously, he was the Head of Product for AWS Web3 / Blockchain. Prior to AWS, John held various product leadership roles at public blockchain protocols, fintech companies and also spent 9 years as a portfolio manager at various hedge funds.\nJonathan Evans is a Worldwide Solutions Architect for Generative AI at AWS, where he helps customers leverage cutting-edge AI technologies with Anthropic’s Claude models on Amazon Bedrock, to solve complex business challenges. With a background in AI/ML engineering and hands-on experience supporting machine learning workflows in the cloud, Jonathan is passionate about making advanced AI accessible and impactful for organizations of all sizes.\nRohit Talluri is a Generative AI GTM Specialist at Amazon Web Services (AWS). He is partnering with top generative AI model builders, strategic customers, key AI/ML partners, and AWS Service Teams to enable the next generation of artificial intelligence, machine learning, and accelerated computing on AWS. He was previously an Enterprise Solutions Architect and the Global Solutions Lead for AWS Mergers & Acquisitions Advisory.\nBreanne Warner is an Enterprise Solutions Architect at Amazon Web Services supporting healthcare and life science (HCLS) customers. She is passionate about supporting customers to use generative AI on AWS and evangelizing model adoption for first- and third-party models. Breanne is also Vice President of the Women at Amazon board with the goal of fostering inclusive and diverse culture at Amazon. Breanne holds a Bachelor’s of Science in Computer Engineering from the University of Illinois Urbana-Champaign."}
{"title": "Learn how Amazon Health Services improved discovery in Amazon search using AWS ML and gen AI", "url": "https://aws.amazon.com/blogs/machine-learning/learn-how-amazon-health-services-improved-discovery-in-amazon-search-using-aws-ml-and-gen-ai/", "source": "AWS Machine Learning Blog", "published": "2025-08-26T18:37:21+00:00", "text": "Artificial Intelligence\nLearn how Amazon Health Services improved discovery in Amazon search using AWS ML and gen AI\nHealthcare discovery on ecommerce domains presents unique challenges that traditional product search wasn’t designed to handle. Unlike searching for books or electronics, healthcare queries involve complex relationships between symptoms, conditions, treatments, and services, requiring sophisticated understanding of medical terminology and customer intent.\nThis challenge became particularly relevant for Amazon as we expanded beyond traditional ecommerce into comprehensive healthcare services. Amazon now offers direct access to prescription medications through Amazon Pharmacy, primary care through One Medical, and specialized care partnerships through Health Benefits Connector. These healthcare offerings represent a significant departure from traditional Amazon.com products, presenting both exciting opportunities and unique technical challenges.\nIn this post, we show you how Amazon Health Services (AHS) solved discoverability challenges on Amazon.com search using AWS services such as Amazon SageMaker, Amazon Bedrock, and Amazon EMR. By combining machine learning (ML), natural language processing, and vector search capabilities, we improved our ability to connect customers with relevant healthcare offerings. This solution is now used daily for health-related search queries, helping customers find everything from prescription medications to primary care services.\nAt AHS, we’re on a mission to transform how people access healthcare. We strive to make healthcare more straightforward for customers to find, choose, afford, and engage with the services, products, and professionals they need to get and stay healthy.\nChallenges\nIntegrating healthcare services into the ecommerce business of Amazon presented two unique opportunities to enhance search for customers on healthcare journeys: understanding health search intent in queries and matching up customer query intent with the most relevant healthcare products and services.\nThe challenge in understanding health search intent lies in the relationships between symptoms (such as back pain or sore throat), conditions (such as a herniated disc or the common cold), treatments (such as physical therapy or medication), and the healthcare services Amazon offers. This requires sophisticated query understanding capabilities that can parse medical terminology and map it to common search terminology that a layperson outside of the medical field might use to search.\nAHS offerings also present unique challenges for search matching. For example, a customer searching for “back pain treatment” might be looking for a variety of solutions, from over-the-counter pain relievers like Tylenol or prescription medications such as cyclobenzaprine (a muscle relaxant), to scheduling a doctor’s appointment or accessing virtual physical therapy. Existing search algorithms optimized for physical products might not match these service-based health offerings, potentially missing relevant results such as One Medical’s primary care services or Hinge Health’s virtual physical therapy program that helps reduce joint and muscle pain through personalized exercises and 1-on-1 support from dedicated therapists. This unique nature of healthcare offerings called for developing specialized approaches to connect customers with relevant services.\nSolution overview\nTo address these challenges, we developed a comprehensive solution that combines ML for query understanding, vector search for product matching, and large language models (LLMs) for relevance optimization. The solution consists of three main components:\n- Query understanding pipeline – Uses ML models to identify and classify health-related searches, distinguishing between specific medication queries and broader health condition searches\n- Product knowledge base – Combines existing product metadata with LLM-enhanced health information to create comprehensive product embeddings for semantic search\n- Relevance optimization – Implements a hybrid approach using both human labeling and LLM-based classification to produce high-quality matches between searches and healthcare offerings\nThe solution is built entirely on AWS services, with Amazon SageMaker powering our ML models, Amazon Bedrock providing LLM capabilities, and Amazon EMR and Amazon Athena handling our data processing needs.\nSolution architecture\nNow let’s examine the technical implementation details of our architecture, exploring how each component was engineered to address the unique challenges of healthcare search on Amazon.com.\nQuery understanding: Identification of health searches\nWe approached the customer search journey by recognizing its two distinct ends of the spectrum. On one end are what we call “spearfishing queries” or lower funnel searches, where customers have a clear product search intent with specific knowledge about attributes. For Amazon Health Services, these typically include searches for specific prescription medications with precise dosages and form factors, such as “atorvastatin 40 mg” or “lisinopril 20 mg.”\nOn the other end are broad, upper funnel queries where customers seek inspiration, information, or recommendations with general product search intent that might encompass multiple product types. Examples include searches like “back pain relief,” “acne,” or “high blood pressure.” Building upon Amazon search capabilities, we developed additional query understanding models to serve the full spectrum of healthcare searches.\nFor identifying spearfishing search intent, we analyzed anonymized customer search engagement data for Amazon products and trained a classification model to understand which search keywords exclusively lead to engagement with Amazon Pharmacy Amazon Standard Identification Numbers (ASINs). This process used PySpark on Amazon EMR and Athena to collect and process Amazon search data at scale. The following diagram shows this architecture.\nFor identifying broad health search intent, we trained a named entity recognition (NER) model to annotate search keywords at a medical terminology level. To build this capability, we used a corpus of health ontology data sources to identify concepts such as health conditions, diseases, treatments, injuries, and medications. For health concepts where we did not have enough alternate terms in our knowledge base, we used LLMs to expand our knowledge base. For example, alternate terms for the condition “acid reflux” might be “heart burn”, “GERD”, “indigestion”, etc. We gated this NER model behind health-relevant product types predicted by Amazon search query-to-product-type models. The following diagram shows the training process for the NER model.\nThe following image is an example of a query identification task in practice. In the example on the left, the pharmacy classifier predicts that “atorvastatin 40 mg” is a query with intent for a prescription drug and triggers a custom search experience geared towards AHS products. In the example on the right, we detect the broad “high blood pressure” symptom but don’t know the customer’s intention. So, we trigger an experience that gives them multiple options to make the search more specific.\nFor those interested in implementing similar medical entity recognition capabilities, Amazon Comprehend Medical offers powerful tools for detecting medical entities in text spans.\nBuilding product knowledge\nWith our ability to identify health-related searches in place, we needed to build comprehensive knowledge bases for our healthcare products and services. We started with our existing offerings and collected all available product knowledge information that best described each product or service.\nTo enhance this foundation, we used a large language model (LLM) with a fine-tuned prompt and few-shot examples to layer in additional relevant health conditions, symptoms, and treatment-related keywords for each product or service. We did this using the Amazon Bedrock batch inference capability. This approach meant that we significantly expanded our product knowledge with medically relevant information.\nThe entire knowledge base was then converted into embeddings using Facebook AI Similarity Search (FAISS), and we created an index file to enable efficient similarity searches. We maintained careful mappings from each embedding back to the original knowledge base items, making sure we could perform accurate reverse lookups when needed.\nThis process used several AWS services, including Amazon Simple Storage Service (Amazon S3) for storage of the knowledge base and the embeddings files. Note that Amazon OpenSearch Service is also a viable option for vector database capabilities. Large-scale knowledge base embedding jobs were executed with scheduled SageMaker Notebook Jobs. Through the combination of these technologies, we built a robust foundation of healthcare product knowledge that could be efficiently searched and matched to customer queries.\nThe following diagram illustrates how we built the product knowledge base using Amazon catalog data, and then used that to prepare a FAISS index file.\nMapping health search intent to the most relevant products and services\nA core component of our solution was implementing the Retrieval Augmented Generation (RAG) design pattern. The first step in this pattern was to identify a set of known keywords and Amazon products, establishing the initial ground truth for our solution.\nWith our product knowledge base built from Amazon catalog metadata and ASIN attributes, we were ready to support new queries from customers. When a customer search query arrived, we converted it to an embedding and used it as a search key for matching against our index. This similarity search used FAISS with matching criteria based on the threshold against the similarity score.\nTo verify the quality of these query-product pairs identified for health search keywords, we needed to maintain the relevance of each pair. To achieve this, we implemented a two-pronged approach to relevance labeling. We used an established scheme to tag each offering as exact, substitute, complement, or irrelevant to the keyword. Referred to as the exact, substitute, complement, irrelevant (ESCI) framework established through academic research. For more information, refer to the ESCI challenge and esci-data GitHub repository.\nFirst, we worked with a human labeling team to establish ground truth on a substantial sample size, creating a reliable benchmark for our system’s performance using this scheme. The labeling team was given guidance based on the ESCI framework and tailored towards AHS products and services.\nSecond, we implemented LLM-based labeling using Amazon Bedrock and batch jobs. After matches were found in the previous step, we retrieved the top products and used them as prompt context for our generative model. We included few-shot examples of ESCI guidance as part of the prompt. This way, we conducted large-scale inference across the top health searches, connecting them to the most relevant offerings using similarity search. We performed this at scale for the query-product pairs identified as relevant to AHS and stored the outputs in Amazon S3.\nThe following diagram shows our query retrieval, re-ranking and ESCI labeling pipeline.\nUsing a mix of high-confidence human and LLM-based labels, we established a true ground truth. Through this process, we successfully identified relevant product offerings for customers using only semantic data from aggregated search keywords and product metadata.\nHow did this help customers?\nWe’re on a mission to make it more straightforward for people to find, choose, afford, and engage with the services, products, and professionals they need to get and stay healthy. Today, customers searching for health solutions on Amazon—whether for acute conditions like acne, strep throat, and fever or chronic conditions such as arthritis, high blood pressure, and diabetes—will begin to see medically vetted and relevant offerings alongside other relevant products and services available on Amazon.com.\nCustomers can now quickly find and choose to meet with doctors, get their prescription medications, and access other healthcare services through a familiar experience. By extending the powerful ecommerce search capabilities of Amazon to address healthcare-specific opportunities, we’ve created additional discovery pathways for relevant health services.\nWe’ve used semantic understanding of health queries and comprehensive product knowledge to create connections that help customers find the right healthcare solutions at the right time.\nAmazon Health Services Offerings\nHere is a little more information about three healthcare services you can use directly through Amazon:\n- Amazon Pharmacy (AP) provides a full-service, online pharmacy experience with transparent medication pricing, convenient home delivery at no additional cost, ongoing delivery updates, 24/7 pharmacist support, and insurance plan acceptance, which supports access and medication adherence. Prime members enjoy special savings with Prime Rx, RxPass, and automatic coupons, making medications more affordable.\n- One Medical Membership and Amazon One Medical Pay Per Visit offer flexible health solutions, from in-office and virtual primary care to condition-based telehealth. Membership offers convenient access to preventive, quality primary care and the option to connect with your care team virtually in the One Medical app. Pay-per-visit is a one-time virtual visit option to find treatment for more than 30 common conditions like acne, pink eye, and sinus infections.\n- Health Benefits Connector matches customers to digital health companies outside of Amazon that are covered by their employer. This program has been expanding over the past year, offering access to specialized care through partners like Hinge Health for musculoskeletal care, Rula and Talkspace for mental health support, and Omada for diabetes treatment.\nKey takeaways\nAs we reflect on our journey to enhance healthcare discovery on Amazon, several key insights stand out that might be valuable for others working on similar challenges:\n- Using domain-specific ontology – We began by developing a deep understanding of customer health searches, specifically identifying what kinds of conditions, symptoms, and treatments customers were seeking. By using established health ontology datasets, we enriched a NER model to detect these entities in search queries, providing a foundation for better matching.\n- Similarity search on product knowledge – We used existing product knowledge along with LLM-augmented real-world knowledge to build a comprehensive corpus of data that could be mapped to our offerings. Through this approach, we created semantic connections between customer queries and relevant healthcare solutions without relying on individual customer data.\n- Generative AI is more than just chatbots – Throughout this project, we relied on various AWS services that proved instrumental to our success. Amazon SageMaker provided the infrastructure for our ML models. However, using Amazon Bedrock batch inference was a key differentiator. It provided us with powerful LLMs for knowledge augmentation and relevance labeling, and services such as Amazon S3 and Amazon EMR supported our data storage and processing needs. Scaling this process manually would have required orders of magnitude more financial budget. Consider generative AI applications at scale beyond merely chat assistants.\nBy combining these approaches, we’ve created a more intuitive and effective way for customers to discover healthcare offerings on Amazon.\nImplementation considerations\nIf you’re looking to implement a similar solution for healthcare or search, consider the following:\n- Security and compliance: Make sure your solution adheres to healthcare data privacy regulations like Health Insurance Portability and Accountability Act (HIPAA). Our approach doesn’t use individual customer data.\n- Cost optimization:\n- Use Amazon EMR on EC2 Spot Instances for batch processing jobs\n- Implement caching for frequently searched queries\n- Choose appropriate instance types for your workload\n- Scalability:\n- Design your vector search infrastructure to handle peak traffic\n- Use auto scaling for your inference endpoints\n- Implement proper monitoring and alerting\n- Maintenance:\n- Regularly update your health ontology datasets\n- Monitor model performance and retrain as needed\n- Keep your product knowledge base current\nConclusion\nIn this post, we demonstrated how Amazon Health Services used AWS ML and generative AI services to solve the unique challenges of healthcare discovery on Amazon.com, illustrating how you can build sophisticated domain-specific search experiences using Amazon SageMaker, Amazon Bedrock, and Amazon EMR. We showed how to create a query understanding pipeline to identify health-related searches, build comprehensive product knowledge bases enhanced with LLM capabilities, and implement semantic matching using vector search and the ESCI relevance framework to connect customers with relevant healthcare offerings.\nThis scalable, AWS based approach demonstrates how ML and generative AI can transform specialized search experiences, advancing our mission to make healthcare more straightforward for customers to find, choose, afford, and engage with. We encourage you to explore how these AWS services can address similar challenges in your own healthcare or specialized search applications. For more information about implementing healthcare solutions on AWS, visit the AWS for Healthcare & Life Sciences page.\nAbout the authors\nK. Faryab Haye is an Applied Scientist II at Amazon Health located in Seattle, WA, where he leads search and query understanding initiatives for healthcare AI. His work spans the complete ML lifecycle from large-scale data processing to deploying production systems that serve millions of customers. Faryab earned his MS in Computer Science with a Machine Learning specialization from the University of Michigan and co-founded the Applied Science Club at Amazon Health. When not building ML systems, he can be found hiking mountains, cycling, skiing, or playing volleyball.\nVineeth Harikumar is a Principal Engineer at Amazon Health Services working on growth and engagement tech initiatives for Amazon One Medical (primary care and telehealth services), Pharmacy prescription delivery, and Health condition programs. Prior to working in healthcare, he worked on building large-scale backend systems in Amazon’s global inventory, supply chain and fulfillment network, Kindle devices, and Digital commerce businesses (such as Prime Video, Music, and eBooks)."}
{"title": "Game On: How Modders Reimagine Classic Games With NVIDIA RTX Remix and Generative AI", "url": "https://blogs.nvidia.com/blog/rtx-ai-garage-rtx-remix-mod-contest-gen-ai/", "source": "NVIDIA Blog", "published": "2025-08-28T13:00:30+00:00", "text": "Last week at Gamescom, NVIDIA announced the winners of the NVIDIA and ModDB RTX Remix Mod Contest, a $50,000 competition celebrating community-made projects that reimagine classic games with modern fidelity.\nThe entries showed how far video game modding has come, with individual modders and small teams pulling off overhauls of similar quality to those created by entire studios.\nAt the heart of these projects was NVIDIA RTX Remix, a platform that lets creators capture assets from classic titles and rebuild them with modern lighting, geometry and materials. Paired with generative AI tools like PBRFusion and ComfyUI, modders can now upscale or generate thousands of textures and automate repetitive tasks so they can focus on their artistry.\nPlus, with NVIDIA RTX GPUs accelerating these AI-driven workflows, ambitious remasters that once took years can now come together in months.\nThere are currently 237 RTX Remix projects in development, building on over 100 finished mods and 2 million downloads across fan favorites like Half-Life 2, Need for Speed: Underground, Portal and Deus Ex.\nTransforming Classics With Generative AI\nThe RTX Remix Mod Contest crowned Merry Pencil Studios’ Painkiller RTX Remix with several awards, but it wasn’t the only mod worth celebrating.\nHere’s a closer look at the winning submission, along with other standout projects that showcase how RTX Remix and AI-powered tools are redefining what’s possible with modding on PCs.\n‘Painkiller’ RTX Remix: Winner in Best Overall, Best Use of RTX and Most Complete Categories\nThe mod team Merry Pencil Studios rebuilt more than 35 levels of the gothic shooter Painkiller using AI-assisted workflows and handcrafted artistry. The team batch-processed thousands of low-resolution textures and generated high-resolution physically based rendering (PBR) materials that automatically got imported into RTX Remix.\nThe team’s AI model of choice was PBRFusion, a model trained by the RTX Remix community that can upscale textures by 4x and generate high-quality normal, roughness and height maps.\nThis workflow provided a consistent foundation for the game’s complex environments, freeing up time for creative polish. From there, the team used tools like Blender and InstaMAT to craft assets like lanterns and other gothic details that define the game’s atmosphere.\n“Generative AI has completely expanded what feels possible in modding. Beyond texture upscaling, we’re now seeing it generate 3D models, refine complex multi-material surfaces and assist with coding tasks like building workflow tools, writing documentation and catching errors.” — Merry Pencil Studios\n“PBRFusion and other AI tools made it possible for a small team to convert an entire game into PBR. It set the baseline look, while we focused our manual efforts on the assets players notice most.” — Merry Pencil Studios\nWith RTX Remix, gothic churches now glow with volumetric light pouring through stained glass, marble statues scatter colored light and combat scenes erupt with particle effects that cast realistic shadows. NVIDIA GeForce RTX GPUs powered the workflow from start to finish, with real-time path tracing and NVIDIA DLSS technology ensuring smooth iteration while editing even on massive scenes.\n“The NVIDIA GeForce RTX 5090 GPU was a dream for our workflow: speed, fluidity, everything felt seamless. DLSS Frame Generation doubled or even tripled frame rates, making the game look incredible on high-refresh displays.” — Merry Pencil Studios\nWhat makes the Painkiller RTX Remix notable is its scope, featuring over 35 remastered levels. This amount of work couldn’t have been completed in such a short time without RTX Remix and the generative AI tools the team used.\nBy combining generative AI automation with careful craftsmanship, Merry Pencil Studios delivered a project that feels both ambitious and polished.\n‘Unreal’ RTX Remix: An Ambitious AI Texture Rebuild\n“I wouldn’t have been able to create PBR textures without AI. I could have maybe created emissive maps and height maps, but I wouldn’t have been able to do the roughness or normal maps myself.” — mstewart401\nUnrealRTX demonstrates the scale of generative AI’s impact in modding. Modder mstewart401 set out to remaster the entire 1998 classic, with 14 levels completed by the contest deadline and more in progress.\nWith RTX Remix’s built-in AI texture tools, plus experimental methods like generating animations from AI video tools and hand-editing light maps, whole environments were reimagined with new detail and atmosphere.\nThe results are striking: glowing crystals pulse with emissive light, alien landscapes shimmer with modern materials and the game’s otherworldly maps feel richer and more alive. By leaning on AI for the bulk of the texture work, mstewart401 could focus on creative polishing — delivering an overhaul that feels ambitious even by professional standards.\n“If someone like me can make a mod like this, anyone can. I only get an hour here and an hour there, but with generative AI and RTX, I’ve been able to push ‘Unreal’ further than I ever thought possible.” — mstewart401\n‘Need for Speed: Underground’ RTX Remix: Blending AI and 3D Artistry\nIn the Need for Speed: Underground RTX Remix, modder Alessandro893 used AI and 3D artistry to remaster every race course in the game with new textures, materials and lighting.\n“In racing games, generative AI opens up new possibilities for creating realistic and immersive environments. In a racing game like ‘Need for Speed: Underground,’ the visual environment is crucial for player immersion, but it also needs to feel responsive and varied.” — Alessandro893\nUsing ComfyUI, Alessandro893 generated more than 500 new textures, then refined them in Adobe Photoshop for consistency and realism. In addition, the modder built over 30 new high-poly car and environment models in Blender, upgrading older assets with smoother, more lifelike detail.\n“Generative AI was mainly used for texture generation. The original look was preserved by using the original textures as input for AI. It’s impossible to create such a large number of textures in such a short period of time alone without AI.” — Alessandro893\nWith RTX GPUs driving AI texture conversion, path-traced reflections and DLSS acceleration, the team could reimagine racing environments with faster iteration and higher fidelity than ever. But as the modder emphasized, AI didn’t replace artistry. It created room for it.\nThe overhaul is most striking on the Chinatown track, which was rebuilt with new buildings, vegetation and fully path-traced lighting that makes neon reflections pop against wet pavement.\nBy leaning on AI to handle the repetitive work of texture generation, the modder could focus on creative refinements — giving Olympic City a modern, cinematic twist while preserving its nostalgic feel.\n‘Portal 2’ RTX Remix: An Innovative AI-Powered Workflow\n“AI opened up new opportunities and drastically accelerated my workflow, allowing me to focus on more ambitious creative tasks.” — Skurtyyskirts\nSkurtyyskirts, the modder behind Portal 2 RTX Remix, used a unique workflow — tapping a large language model to build a custom plug-in called Substance2Remix, bridging Adobe Substance Painter directly to RTX Remix.\nThis flow allowed the modder to pull in an asset, apply AI-assisted materials, hand-paint details and push it straight back into the game, all in one rapid loop. What would normally take days of exporting and importing was done in minutes.\n“Once I saw the potential of Remix’s REST application programming interface, I realized I could create a more integrated workflow between tools like Substance Painter and RTX Remix. I didn’t want to deal with a manual, tedious export-import process for my handmade textures, so I developed a simple plug-and-play plug-in. This completely shifted the role of AI from a simple upscaling tool to a core component of my creative pipeline, enabling me to focus on creating detailed, high-quality textures by hand.” — Skurtyyskirts\nEarly on, the project leaned on AI upscalers like PBRFusion, but over time the workflow evolved into a mix of AI and manual artistry. The result is a sharper, more atmospheric game environment — enhanced further by RTX Remix’s volumetrics and fog systems, which make the decaying test chambers feel more alive.\nBy creating a new pipeline, the project opens the door for other modders to experiment with faster, AI-powered workflows of their own.\nPress Start: Remaster With RTX Remix\nTo get started creating RTX Remix mods, download NVIDIA RTX Remix from the home screen of the NVIDIA App and check out our tutorials and documentation. PBRFusion on Hugging Face also offers a plug-and-play setup with ComfyUI, letting modders batch-process textures into high-quality, PBR maps in just a few clicks.\nCheck out all of the mods submitted to the RTX Remix Modding Contest, as well as 100 more Remix mods, available to download from ModDB. Read the RTX Remix article to learn more about the contest and winners. For a sneak peek at RTX Remix projects under active development, join the community over at the RTX Remix Showcase Discord server — it’s a great place to get a helping hand.\nEach week, the RTX AI Garage blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building AI agents, creative workflows, productivity apps and more on AI PCs and workstations.\nPlug in to NVIDIA AI PC on Facebook, Instagram, TikTok and X — and stay informed by subscribing to the RTX AI PC newsletter. Join NVIDIA’s Discord server to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.\nFollow NVIDIA Workstation on LinkedIn and X.\nSee notice regarding software product information."}
{"title": "Drop Into the Battle: ‘Gears of War: Reloaded’ Launches on GeForce NOW", "url": "https://blogs.nvidia.com/blog/geforce-now-thursday-gears-of-war-reloaded/", "source": "NVIDIA Blog", "published": "2025-08-28T13:00:17+00:00", "text": "Brace yourself, COGs — the Locusts aren’t the only thing rising up. The Coalition’s legendary shooter Gears of War: Reloaded is launching day one on GeForce NOW.\nBut that’s just the start. This GFN Thursday, seven games join the GeForce NOW library, including Ubisoft’s The Rogue Prince of Persia, the electrifying 2D roguelike action-platformer.\nMore Grit, More Gears\nChainsaws — check. Grizzled one-liners — absolutely. Gears of War: Reloaded is back, buffed and primed, remastered from the ground up in Unreal Engine 5. It’s the classic curb-stomping action gamers remember, now with visuals sharp enough to make the Locust run for cover. Form up and get loud.\nDive into battle with Marcus, Dom and the rest of Delta Squad to fight tooth and chainsaw to save humanity from the subterranean Locust Horde. Carve through the epic campaign solo or tag in friends for online co-op mode. The remastered version packs every blast, chainsaw duel and bro fist from the original — plus bonus campaign missions, multiplayer maps and more. Tackle battles with modern controls for franchise newcomers or classic controls for veterans — no grunt left behind.\nStream Gears of War: Reloaded on GeForce NOW and witness Unreal Engine’s best visuals without upgrading hardware. Run multiplayer with the lowest latency with an Ultimate membership, cross-play with the squad and see every crumbling wall and flying chunk — all from the cloud, effortlessly.\nGreatest Leap Yet\nThe Rogue Prince of Persia 1.0 marks the game’s full release after months of early access, bringing refined parkour, polished combat, fresh content and the complete story of the rogue heir racing to reclaim his kingdom. Sprint, vault and wall-run through a reimagined Persia as the prince battles to undo a deadly curse and stop the invading Huns.\nEach run is a new fight for survival, blending fluid platforming with swift, acrobatic combat. Leap over traps, chain stylish moves and wield an ever-expanding arsenal while unlocking medallions, upgrading gear and uncovering the truth behind the prince’s fall — and his shot at redemption.\nOn GeForce NOW, the adventure shines at its best with up to 4K 120 frames-per-second streaming. Land every parkour move with perfect timing thanks to ultralow latency and take the prince’s fight anywhere, instantly, on nearly any device.\nLet’s Play Today\nMake sure to check out Chip ‘n Clawz vs. The Brainioids, a quirky action-strategy hybrid from X-COM creator Julian Gollop, where players control a clever inventor and his robo-cat to fight off an invasion of bizarre Brainioid aliens. Mix third-person action with real-time strategy while building bases, commanding bot armies and squishing rogue brains in solo and co-op modes, all wrapped in a colorful, comic-book world. Couch and online multiplayer, player vs. player battles and a humorous campaign make this a fresh, approachable take on the strategy genre.\nIn addition, members can look for the following:\n- Gears of War: Reloaded (New release on Steam and Xbox, available on PC Game Pass, Aug. 26)\n- Chip ‘n Clawz vs. The Brainioids (New release on Steam, Aug. 26)\n- Make Way (Free, new release on Epic Games Store, Aug. 28)\n- Among Us 3D (Steam)\n- Gatekeeper (Steam)\n- Knightica (Steam)\n- No Sleep for Kaname Date – From AI: THE SOMNIUM FILES (Steam)\nWhat are you planning to play this weekend? Let us know on X or in the comments below.\nThis is a duo appreciation post. 😁\nWho are you locking in with? (tag them) 🔒🎮\n— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) August 27, 2025"}
{"title": "How Do You Teach an AI Model to Reason? With Humans", "url": "https://blogs.nvidia.com/blog/ai-reasoning-cosmos/", "source": "NVIDIA Blog", "published": "2025-08-27T23:13:16+00:00", "text": "AI models are advancing at a rapid rate and scale.\nBut what might they lack that (most) humans don’t? Common sense: an understanding, developed through real-world experiences, that birds can’t fly backwards, mirrors are reflective and ice melts into water.\nWhile such principles seem obvious to humans, they must be taught to AI models tasked with accurately answering complex questions and navigating unpredictable physical environments, such as industrial warehouses or roads.\nNVIDIA is tackling this challenge by developing a set of tests to coach AI models on the limitations of the physical world. In other words, to teach AI common sense.\nThese tests are used to develop reasoning models such as NVIDIA Cosmos Reason, an open reasoning vision language model (VLM) used for physical AI applications that are proficient in generating temporally grounded responses. Cosmos Reason just topped the physical reasoning leaderboard on Hugging Face.\nCosmos Reason is unique compared with previous VLMs as it’s designed to accelerate physical AI development for fields such as robotics, autonomous vehicles and smart spaces. The model can infer and reason through unprecedented scenarios using physical common-sense knowledge.\nFor models to understand complex environments — including industrial spaces and laboratories — they must start small. For example, in the test depicted below, the Cosmos Reason model is tasked with answering a multiple-choice question about the relative motion in the video:\nExample from Cosmos Reason evaluation dataset\nWhat Does Reasoning Look Like for an AI Model?\nTo develop their reasoning capabilities, NVIDIA models are being taught physical common sense about the real world via reinforcement learning.\nFor example, robots don’t intuitively know which way is left, right, up or down. They’re taught these spatial-temporal limitations through training. AI-powered robots used in safety testing, such as vehicle crash testing, must be taught to be aware of how their physical forms interact with their surroundings.\nWithout embedding common sense into the training of these robots, issues can arise in deployment.\n“Without basic knowledge about the physical world, a robot may fall down or accidentally break something, causing danger to the surrounding people and environment,” said Yin Cui, a Cosmos Reason research scientist at NVIDIA.\nDistilling human common sense about the physical world into models is how NVIDIA is bringing about the next generation of AI.\nEnter the NVIDIA data factory team: a group of global analysts who come from various backgrounds — including bioengineering, business and linguistics. They’re working to develop, analyze and compile hundreds of thousands of data units that will be used to train generative AI models on how to reason.\nThe Data Curation Process\nOne of the NVIDIA data factory team’s projects focuses on the development of world foundation models for physical AI applications. These virtual environments create deep learning neural networks that are safer and more effective for training reasoning models, based on simulated domains.\nIt all starts with an NVIDIA annotation group that creates question-and-answer pairs based on video data. These videos are all from the real world and can include any type of footage, whether depicting chickens walking around in their coop or cars driving on a rural road.\nFor example, an annotator might ask about the video below: “The person uses which hand to cut the spaghetti?”\nExample from Cosmos Reason evaluation dataset\nThe annotators then come up with four multiple choice answers labeled A, B, C and D. The model is fed the data and has to reason and choose the correct answer.\n“We’re basically coming up with a test for the model,” said Cui. “All of our questions are multiple choice, like what students would see on a school exam.”\nThese question-and-answer pairs are then quality checked by NVIDIA analysts, such as Michelle Li.\nLi has a background in public health and data analytics, which allows her to look at the broader purpose of the data she analyzes.\n“For physical AI, we have a specific goal of wanting to train models on understanding the physical world, which helps me think about the bigger picture when I’m looking at the Q&A pairs and the types of questions that are being presented,” Li said. “I ask myself, do the Q&A pairs that I’m looking at align with our objectives for the guidelines that we have for the project?”\nAfter this, the data is reviewed by the data factory leads of the project, who make sure it’s up to quality standards and ready to be sent to the Cosmos Reason research team. The scientists then feed the hundred thousands of data units — in this case the Q&A pairs — to the model, training it with reinforcement learning on the bounds and limitations of the physical world.\nWhat Are the Applications of Reasoning AI?\nReasoning models are exceptional because they can make sense of their temporal space as well as predict outcomes. They can analyze a situation, come up with a thought web of probable outcomes and infer the most likely scenario.\nSimply put, reasoning AI demonstrates humanlike thinking. It shows its work, giving the user insight into the logic behind its responses.\nUsers can ask these models to analyze a video such as of two cars driving on a road. When asked a question like, “What would happen if the cars were driving toward each other on the same lane?” the model can reason and determine the most probable outcome of the proposed scenario — for example, a car crash.\n“We’re building a pioneering reasoning model focused on physical AI,” said Tsung-Yi Lin, a principal research scientist on the Cosmos Reason team at NVIDIA.\nThe data factory team’s ability to produce high-quality data will be imperative for driving the development of intelligent autonomous agents and physical AI systems that can safely interact with the real world as NVIDIA reasoning model innovation continues.\nPreview NVDIA Cosmos-Reason1 or download the model on Hugging Face and GitHub."}
{"title": "Take It for a Spin: NVIDIA Rolls Out DRIVE AGX Thor Developer Kit to World’s Automotive Developers", "url": "https://blogs.nvidia.com/blog/drive-agx-developer-kit-general-availability/", "source": "NVIDIA Blog", "published": "2025-08-25T15:00:51+00:00", "text": "As autonomous vehicle systems rapidly grow in complexity, equipped with reasoning vision language action models, generative AI and advanced sensor technologies, developers need tools that are powerful, efficient and built to meet automotive-grade safety requirements.\nThe NVIDIA DRIVE AGX Thor developer kit — now available for preorder today, with delivery in September — provides developers and researchers worldwide an advanced platform to accelerate the design, testing and deployment of AVs and intelligent mobility solutions.\nThe developer kit is built on the NVIDIA Blackwell architecture, next-generation Arm Neoverse V3AE CPUs and the NVIDIA DriveOS 7 software stack. It’s purpose-built for reasoning vision language action models and ideal for automotive development, with sufficient I/O to support surround cameras, radars and lidars, as well as common vehicle interfaces including GbE/10GbE and PCI-Express. DRIVE AGX Thor also meets the automotive industry’s stringent functional safety (ISO 26262) and cybersecurity requirements (ISO 21434).\nThe Growing DRIVE AGX Thor Ecosystem\nThe world’s leading automotive companies are building on NVIDIA DRIVE AGX Thor, including BYD, GAC, IM Motors, Li Auto, Volvo Cars, Xiaomi and Zeekr. Autonomous trucking companies building on NVIDIA DRIVE AGX Thor include Aurora, Gatik, PlusAI and Waabi.\nNVIDIA AV partners DeepRoute.ai, Nuro, WeRide and ZYT are using DRIVE AGX Thor for their AV software platforms. DRIVE AGX Thor production systems are available from Tier 1 suppliers Continental Automotive, Desay SV, Lenovo, Magna and Quanta.\nDRIVE AGX Thor is supported by a growing number of sensor and embedded technology pioneers, including AdaCore, Lauterbach, OMNIVISION, QNX and Vector.\nAV Safety From Cloud to Car\nDesigned for automotive-grade safety and security, DRIVE AGX Thor and DriveOS are key elements of NVIDIA Halos, a comprehensive safety system that brings together NVIDIA’s automotive hardware and software safety technologies with cutting-edge AI research in AV safety.\nHalos offers a holistic approach to automotive safety:\n- At the technology level, it spans platform, algorithmic and ecosystem safety.\n- At the development level, it includes design-, deployment- and validation-time guardrails.\n- At the computational level, it spans AI training to deployment, using three powerful computers — NVIDIA DGX for AI training, NVIDIA Omniverse and NVIDIA Cosmos running on NVIDIA OVX for simulation, and NVIDIA DRIVE AGX for deployment.\nGet Started\nWatch the NVIDIA DRIVE AGX Thor unboxing video and join the NVIDIA DRIVE AGX SDK Developer Program.\nPlus, learn more about NVIDIA Jetson AGX Thor developer kit and NVIDIA Jetson T5000 modules — available today — empowering robotics developers everywhere to build the future of physical AI.\nSee notice regarding software product information."}
{"title": "NVIDIA Jetson Thor Unlocks Real-Time Reasoning for General Robotics and Physical AI", "url": "https://blogs.nvidia.com/blog/jetson-thor-physical-ai-edge/", "source": "NVIDIA Blog", "published": "2025-08-25T15:00:31+00:00", "text": "Robots around the world are about to get a lot smarter as physical AI developers plug in NVIDIA Jetson Thor modules — new robotics computers that can serve as the brains for robotic systems across research and industry.\nRobots demand rich sensor data and low-latency AI processing. Running real-time robotic applications requires significant AI compute and memory to handle concurrent data streams from multiple sensors. Jetson Thor, now in general availability, delivers 7.5x more AI compute, 3.1x more CPU performance and 2x more memory than its predecessor, the NVIDIA Jetson Orin, to make this possible on device.\nThis performance leap will enable roboticists to process high-speed sensor data and perform visual reasoning at the edge — workflows that were previously too slow to run in dynamic real-world environments. This opens new possibilities for multimodal AI applications such as humanoid robotics.\nAgility Robotics, a leader in humanoid robotics, has integrated NVIDIA Jetson into the fifth generation of its robot, Digit — and plans to adopt Jetson Thor as the onboard compute platform for the sixth generation of Digit. This transition will enhance Digit’s real-time perception and decision-making capabilities, supporting increasingly complex AI skills and behaviors. Digit is commercially deployed and performs logistics tasks such as stacking, loading and palletizing in warehouse and manufacturing environments.\n“The powerful edge processing offered by Jetson Thor will take Digit to the next level — enhancing its real-time responsiveness and expanding its abilities to a broader, more complex set of skills,” said Peggy Johnson, CEO of Agility Robotics. “With Jetson Thor, we can deliver the latest physical AI advancements to optimize operations across our customers’ warehouses and factories.”\nBoston Dynamics — which has been building some of the industry’s most advanced robots for over 30 years — is integrating Jetson Thor into its humanoid robot Atlas, enabling Atlas to harness formerly server-level compute, AI workload acceleration, high-bandwidth data processing and significant memory on device.\nBeyond humanoids, Jetson Thor will accelerate various robotic applications — such as surgical assistants, smart tractors, delivery robots, industrial manipulators and visual AI agents — with real-time inference on device for larger, more complex AI models.\nA Giant Leap for Real-Time Robot Reasoning\nJetson Thor is built for generative reasoning models. It enables the next generation of physical AI agents — powered by large transformer models, vision language models and vision language action models — to run in real time at the edge while minimizing cloud dependency.\nOptimized with the Jetson software stack to enable the low latency and high performance required in real-world applications, Jetson Thor supports all popular generative AI frameworks and AI reasoning models with unmatched real-time performance. These include Cosmos Reason, DeepSeek, Llama, Gemini and Qwen models, as well as domain-specific models for robotics like Isaac GR00T N1.5, enabling any developer to easily experiment and run inference locally.\nWith NVIDIA CUDA ecosystem support through its lifecycle, Jetson Thor is expected to deliver even better throughput and faster responses with future software releases.\nJetson Thor modules also run the full NVIDIA AI software stack to accelerate virtually every physical AI workflow with platforms including NVIDIA Isaac for robotics, NVIDIA Metropolis for video analytics AI agents and NVIDIA Holoscan for sensor processing.\nWith these software tools, developers can easily build and deploy applications, such as visual AI agents that can analyze live camera streams to monitor worker safety, humanoid robots capable of manipulation tasks in unstructured environments and smart operating rooms that guide surgeons based on data from multi-camera streams.\nJetson Thor Set to Advance Research Innovation\nResearch labs at Stanford University, Carnegie Mellon University and the University of Zurich are tapping Jetson Thor to push the boundaries of perception, planning and navigation models for a host of potential applications.\nAt Carnegie Mellon’s Robotics Institute, a research team uses NVIDIA Jetson to power autonomous robots that can navigate complex, unstructured environments to conduct medical triage as well as search and rescue.\n“We can only do as much as the compute available allows,” said Sebastian Scherer, an associate research professor at the university and head of the AirLab. “Years ago, there was a big disconnect between computer vision and robotics because computer vision workloads were too slow for real-time decision-making — but now, models and computing have gotten fast enough so robots can handle much more nuanced tasks.”\nScherer anticipates that by upgrading from his team’s existing NVIDIA Jetson AGX Orin systems to Jetson AGX Thor developer kit, they’ll improve the performance of AI models including their award-winning MAC-VO model for robot perception at the edge, boost their sensor-fusion capabilities and be able to experiment with robot fleets.\nWield the Strength of Jetson Thor\nThe Jetson Thor family includes a developer kit and production modules. The developer kit includes a Jetson T5000 module, a reference carrier board with abundant connectivity, an active heatsink with a fan and a power supply.\nThe Jetson ecosystem supports a variety of application requirements, high-speed industrial automation protocols and sensor interfaces, accelerating time to market for enterprise developers. Hardware partners including Advantech, Aetina, ConnectTech, MiiVii and TZTEK are building production-ready Jetson Thor systems with flexible I/O and custom configurations in various form factors.\nSensor and Actuator companies including Analog Devices, Inc. (ADI), e-con Systems, Infineon, Leopard Imaging, RealSense and Sensing are using NVIDIA Holoscan Sensor Bridge — a platform that simplifies sensor fusion and data streaming — to connect sensor data from cameras, radar, lidar and more directly to GPU memory on Jetson Thor with ultralow latency.\nThousands of software companies can now elevate their traditional vision AI and robotics applications with multi-AI agent workflows running on Jetson Thor. Leading adopters include Openzeka, Rebotnix, Solomon and Vaidio.\nMore than 2 million developers use NVIDIA technologies to accelerate robotics workflows. Get started with Jetson Thor by reading the NVIDIA Technical Blog and watching the developer kit walkthrough.\nTo get hands-on experience with Jetson Thor, sign up to participate in upcoming hackathons with Seeed Studio and LeRobot by Hugging Face.\nThe NVIDIA Jetson AGX Thor developer kit is available now starting at $3,499. NVIDIA Jetson T5000 modules are available starting at $2,999 for 1,000 units. Buy now from authorized NVIDIA partners.\nNVIDIA today also announced that the NVIDIA DRIVE AGX Thor developer kit, which provides a platform for developing autonomous vehicles and mobility solutions, is available for preorder. Deliveries are slated to start in September."}
{"title": "Introducing gpt-realtime and Realtime API updates", "url": "https://openai.com/index/introducing-gpt-realtime", "source": "OpenAI News", "published": "2025-08-28T10:00:00+00:00", "text": ""}
{"title": "Supporting nonprofit and community innovation", "url": "https://openai.com/index/supporting-nonprofit-and-community-innovation", "source": "OpenAI News", "published": "2025-08-28T05:00:00+00:00", "text": ""}
{"title": "Collective alignment: public input on our Model Spec", "url": "https://openai.com/index/collective-alignment-aug-2025-updates", "source": "OpenAI News", "published": "2025-08-27T13:00:00+00:00", "text": ""}
{"title": "OpenAI and Anthropic share findings from a joint safety evaluation", "url": "https://openai.com/index/openai-anthropic-safety-evaluation", "source": "OpenAI News", "published": "2025-08-27T10:00:00+00:00", "text": ""}
{"title": "Helping people when they need it most", "url": "https://openai.com/index/helping-people-when-they-need-it-most", "source": "OpenAI News", "published": "2025-08-26T04:00:00+00:00", "text": ""}
{"title": "Announcing the OpenAI Learning Accelerator", "url": "https://openai.com/global-affairs/learning-accelerator", "source": "OpenAI News", "published": "2025-08-25T06:00:00+00:00", "text": ""}
